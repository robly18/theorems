\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{mathtools}

\usepackage[thmmarks, amsmath]{ntheorem}

\usepackage{graphicx}

\usepackage{diffcoeff}
\diffdef{}{op-symbol=\mathrm{d},op-order-sep=0mu}

\usepackage{cancel}
\usepackage{interval}

\usepackage{enumitem}

\setlist[enumerate,1]{label=(\roman*)}

\title{Algebra Homework 6}
\author{Duarte Maia}
%\date{}

\theorembodyfont{\upshape}
\theoremseparator{.}
\newtheorem{theorem}{Theorem}
\newtheorem{prop}{Prop}
\renewtheorem*{prop*}{Prop}
\newtheorem{lemma}{Lemma}

\newtheorem{ex}{Exercise}

\theoremstyle{nonumberplain}
\theoremheaderfont{\itshape}
\theorembodyfont{\upshape}
\theoremseparator{:}
\theoremsymbol{\ensuremath{\fbox{\text{Yay!}}}}
\newtheorem{proof}{Proof}
\newtheorem{sol}{Solution}

\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}

\newcommand{\kk}{\Bbbk}

\newcommand{\PP}{\mathbb{P}}
\newcommand{\FF}{\mathcal{F}}
\newcommand{\DD}{\mathcal{D}}

\newcommand{\I}{\mathrm{i}}
\newcommand{\e}{\mathrm{e}}

\newcommand{\conj}[1]{\overline{#1}}

\DeclareMathOperator{\inte}{int}
\DeclareMathOperator{\codim}{codim}
\newcommand{\grad}{\nabla}

\DeclareMathOperator{\vol}{vol}
\DeclareMathOperator{\Av}{Av}
\DeclareMathOperator{\Lie}{Lie}
\DeclareMathOperator{\trace}{tr}
\newcommand{\transpose}{\top}
\DeclareMathOperator{\spec}{spec}

\DeclareMathOperator{\Ad}{Ad}
\DeclareMathOperator{\ad}{ad}
\DeclareMathOperator{\End}{End}


\DeclareMathOperator{\Aff}{Aff}
\DeclareMathOperator{\Sym}{Sym}
\newcommand{\GL}{\mathrm{GL}}
\newcommand{\SL}{\mathrm{SL}}
\newcommand{\Hp}{\mathrm{H}}

\newcommand{\HH}{\mathcal{H}}

\let\Im\relax
\DeclareMathOperator{\Im}{Im}
\let\Re\relax
\DeclareMathOperator{\Re}{Re}

\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\norm}{\lvert}{\rvert}
\DeclarePairedDelimiter{\Norm}{\lVert}{\rVert}
\DeclarePairedDelimiter{\braket}{\langle}{\rangle}


\begin{document}
\maketitle

\begin{ex}
Prove that for fixed $a, b \in A$ and all large enough integers $n$ one has
\begin{gather}
\exp(\frac1n a) \exp(\frac1n b) = \exp(\frac1n(a + b + \alpha_n)),\\
\exp(\frac1n a) \exp(\frac1n b) \exp(-\frac1n(a+b)) = \exp(\frac1{n^2} (\frac{[a,b]}2 + \beta_n)),
\end{gather}
where $\alpha_n$ and $\beta_n$ converge to zero.
\end{ex}

\begin{sol}
To show that $\alpha_n$ and $\beta_n$ converge, we apply the logarithm. Indeed, for $n$ large enough, $\exp(\frac1n a)$ and $\exp(\frac1n b)$ are close enough to one, and the product is continuous, so $\exp(\frac1n a) \exp(\frac1n b)$ has a well-defined logarithm $\ell_n$, and thus we set $\alpha_n = n \ell_n - a - b$. A similar argument works to define $\beta_n$.

Now, we show that these sequences converge to zero. To do so, we write down an explicit expression for $\alpha_n$ with $n$ large:
\begin{equation}
\alpha_n = n \left( \log(\exp(\frac1n a) \exp(\frac1n b)) - \frac1n a - \frac1n b \right).
\end{equation}

Now, we write this out using the power series expansion for $\log$ and $\exp$, with two caveats. The first is that we keep $a$ and $b$ fixed, with the only variable being $n$. The second is that we don't care about terms of order higher than one (in $\frac1n$), so we disregard them. In other words, all that will be written next will be done modulo $O(n^{-2})$, including inside the $\log$:
\begin{equation}
\begin{aligned}
\frac1n \alpha_n &= \log(\exp(\frac1n a) \exp(\frac1n b)) - \frac1n a - \frac1n b\\
&= \log\left( \left(1 + \frac1n a \right) \left(1 + \frac1n b \right) \right) - \frac1n a - \frac1n b\\
&= \log \left(1 + \frac1n a + \frac1n b \right) - \frac1n a - \frac1n b \\
&= \left( \frac1n a + \frac1n b\right) - \frac1n a - \frac1n b = 0.
\end{aligned}
\end{equation}

Thus, $\frac1n \alpha_n$ is null modulo a second order term, i.e. $\frac1n \alpha_n = O(n^{-2})$ hence $\alpha_n = O(n^{-2})$ and thus $\alpha_n \to 0$.

\medskip

Now, we do the same process for $\beta_n$. Now, we write $n^2 \beta_n$ modulo $O(n^{-3})$:
\begin{equation}
\begin{aligned}
\frac1{n^2} \beta_n &= \log\left( \exp(\frac1n a) \exp(\frac1n b) \exp(-\frac1n(a+b)) \right) - \frac{ab-ba}2\\
&= \log\left( {\scriptstyle (1+\frac1n a + \frac1{2 n^2} a^2) (1+\frac1n b + \frac1{2 n^2} b^2) (1-\frac1n (a+b) + \frac1{2 n^2} (a+b)^2)} \right) - \frac1{n^2}\frac{ab-ba}2\\
&= \log\left( 1 + \frac1{n^2} (ab - a(a+b) - b(a+b)) + \frac1{2n^2}(a^2 + b^2 + (a+b)^2) \right) - \frac1{n^2}\frac{ab-ba}2\\
&= \log\left( 1 + \frac1{n^2} (- a^2 - ba - b^2) + \frac1{2n^2}(2 a^2 + 2 b^2 + ab + ba) \right) - \frac1{n^2}\frac{ab-ba}2\\
&= \log\left( 1 + \frac1{2n^2} (ab - ba) \right) - \frac1{n^2}\frac{ab-ba}2\\
&= \frac1{2n^2} (ab-ba) + \frac1{4n^4}(ab-ba)^2 - \frac1{n^2}\frac{ab-ba}2\\
&= 0.
\end{aligned}
\end{equation}

Thus, $\frac1{n^2} \beta_n = O(n^{-3})$, hence $\beta_n = O(n^{-1})$.
\end{sol}

\begin{ex}
Show that $\log(\exp(a)) = a$ and $\exp(\log(1+a)) = 1+a$ for small enough $a$.
\end{ex}

\begin{sol}
The composites are well-defined for small enough $a$, in the first case by continuity of $\exp$ and in the second you don't even need that. Now, to show that the composites are what you expect them to, we observe the following: composite of power series works formally; i.e. in a Banach algebra, given $f(x)$ and $g(x)$ formal power series with positive radius of convergence and $g(0) = 0$, we have $f(g(x))$ coincides with the formal power series $(f \circ g)(x)$. Now, in the Banach algebra $\R$, we know from calculus that $\log(\exp(a)) = a$ and $\exp(\log(1+a)) = 1+a$. Therefore, by uniqueness of power series expansions, e.g. the formal composite $\log \circ \exp$ is just $x$. Therefore $\log(\exp(a)) = a$ holds in any Banach algebra, and likewise for the other equation.
\end{sol}

\begin{ex}
Prove that continuous group homomorphisms $f \colon (\R, +) \to A^\times$ are of the form $t \mapsto \exp(t a)$ for some $a \in A$.
\end{ex}

\begin{sol}
First, pick $x_0 \in \R$, $x_0 \neq 0$, small enough that $a_0 = \log x_0$ is well-defined. Now, for all $n \in \Z_{> 0}$ we have
\begin{equation}
f(x_0/n)^n = f(x_0) = \exp(a_0),
\end{equation}
hence, taking the logarithm on both sides,
\begin{equation}
\log(f(x_0/n)) = a_0 / n,
\end{equation}
and thus $f(x_0/n) = \exp(a_0 / n)$.

On the other hand, $f(m x_0 / n) = f(x_0 / n)^m = \exp(a_0 / n)^m = \exp(m a_0 / n)$. Therefore, we conclude
\begin{equation}
f(q x_0) = \exp(q a_0),
\end{equation}
for all $q \in Q$, and by continuity this holds for all $q \in \R$. Finally, we have $f(t) = \exp(t a)$ for $a = a_0 / x_0$.
\end{sol}

\begin{ex}
Find the Lie algebras of $O_n$, $U_n$, and $SU_n$ and compute their dimensions.
\end{ex}

\begin{sol}
We begin by claiming that the Lie algebra of $O_n$ is the collection of skew-symmetric $n \times n$ matrices.

First, note that $\exp(a)^\transpose = \exp(a^\transpose)$ (this is seen directly by expanding the power series). Hence, if $a$ is skew-symmetric, then $\exp(t a) \exp(t a)^\transpose = \exp(t a) \exp(t a^\transpose) = \exp(t a) \exp(-t a) = I$. Thus, any skew-symmetric matrix is in the Lie algebra of $O_n$.

Now, suppose that $a$ is in the Lie algebra of $O_n$, so that $\exp(t a) \exp(t a^\transpose) = I$ for all $t \in \R$. If we take $t = \frac1n \to 0$ and apply the first formula in the first exercise, we get
\begin{equation}
\frac1n{a + a^\transpose + \alpha_n} = 0,
\end{equation}
for all values of $n$. Thus, $a + a^\transpose + \alpha_n = 0$, and taking the limit we obtain that $a$ is skew-symmetric.

Finally, to obtain the dimension of this Lie algebra: an arbitrary skew-symmetric matrix has all diagonal elements equal to zero, the elements above the diagonal are arbitrary, and the elements below the diagonal are determined by the ones above. Thus, the dimension of this space is the number of elements above the diagonal, which is $\frac{n (n-1)}2$.

\smallskip

The same argument with transpose replaced by conjugate transpose shows that the Lie algebra of $U_n$ is the $n \times n$ skew-hermitian matrices, i.e. those that satisfy $a + a^* = 0$. By a similar but slightly modified argument, we get that the (real) dimension of this Lie algebra is $n^2$, because the diagonal elements are free as long as they are purely imaginary, and the ones above the diagonal are free and determine those below it.

\smallskip

Finally, we determine the Lie algebra of $SU_n$. We claim that it consists of the skew-hermitian matrices whose trace is null.

To this effect, suppose that $a$ is a skew-hermitian matrix. We already know that $\exp(t a)$ is always unitary, so it suffices to check when its determinant is always one. This is certainly true for $t = 0$, so this is equivalent to verifying whether $\diff{}t \det(\exp(ta)) = 0$. We will do so by using the chain rule. Of course, we already know that $\diff{}t \exp(ta) = a \exp(ta) = \exp(ta) a$, so we focus on computing the derivative of the determinant.

We know that the determinant is smooth (just look at the expression), and computing its partial derivatives at the identity is pretty easy. Indeed, if $e_{ij}$ is the matrix whose entries are all null except the $ij$-th, which is one, then it is trivial that $\det(I + t e_{ij}) = 1 + t \delta_{ij}$ (this is the Kronecker delta), hence $\partial_{ij} \det(I) = \delta_{ij}$. From this it is easy to compute the derivative of the determinant at the identity in the sense of multivariable calculus, and we end up with $(\dl \det)_I(A) = \trace A$.

By the multiplicativity of the determinant we may extend this result to the determinant at any invertible matrix $A \in \GL_n$, as on the one hand since $\det \circ L_A = \det(A) \times \det$ we obtain $(\dl (\det \circ L_A))_I(B) = \det(A) \times \trace(B)$, and on the other using the chain rule we get
\begin{equation}
(\dl (\det \circ L_A))_I(B) = (\dl \det)_A (\dl L_A)_I (B) = (\dl \det)_A (AB).
\end{equation}

Thus, setting $C = AB$ we obtain $(\dl \det)_A(C) = \det(A) \trace(A^{-1} B)$.

So, back to computing the derivative, we obtain
\begin{equation}
\diff{}t \det(\exp(ta)) = (\dl \det)_{\exp(ta)}(\exp(ta) a) = \det(\exp(ta)) \trace(\exp(-ta) \exp(ta) a) = \det(\exp(ta)) \trace(a).
\end{equation}

Thus, to guarantee that $\det(\exp(ta))$ is constant equal to zero it is necessary and sufficient that $\trace(a) = 0$, which proves that the Lie algebra of $SU_n$ consists of the skew-hermitian matrices with null trace.

The condition that the trace is null adds one condition (it determines the last diagonal element from the others), so reduces the dimension by one. Hence, the dimension of the Lie algebra of $SU_n$ is $n^2 - 1$.
\end{sol}

\begin{ex}
Show that the multiplicative subgroup $A^\times$ is an open subset of $A$, and that the product and inverse are continuous in it.
\end{ex}

\begin{sol}
To show openness, we show that if $a \in A$ is invertible then so is anything of the form $a - x$ for $x$ small enough. We do this by writing an explicit expression for the inverse. Let $b(x)$ be defined by
\begin{equation}
b(x) = \sum_{i \geq 0} a^{-1} (x a^{-1})^i.
\end{equation}

Then, this power series converges absolutely for small values of $x$, namely $\norm{x} < \frac1{\norm{a^{-1}}}$, as in this case if we take the norm of the partial sums and apply the multiplicative triangular inequality, we obtain a bound by a geometric sum of ratio $\norm x \norm{a^{-1}}$. Moreover, if we take the product of this power series on the left by $a$ and $x$ we obtain respectively
\begin{equation}
a b(x) = \sum_{i \geq 0} (x a^{-1})^i, \quad x b(x) = \sum_{i \geq 1} (x a^{-1})^i
\end{equation}
both of which are also absolutely convergent and so we can subtract them nicely and the only term which survives is $(x a^{-1})^0 = 1$. Hence, we have an expression for the inverse.

Now, using this same expression we can also show that the inverse is continuous, but first notice that the product is obviously continuous because $\norm{(a_0 + \delta_0)(a_1 + \delta_1) - a_0 a_1} \leq \norm{\delta_0} \norm{a_1} + \norm{a_0} \norm{\delta_1} + \norm{\delta_0} \norm{\delta_1}$, which goes to zero with $\delta_0$ and $\delta_1$ small.

Thus, we may write the expression for the inverse as a composite of a geometric power series
\begin{equation}
J(x) = \sum_{i \geq 0} x^i
\end{equation}
with multiplication by $a^{-1}$ inside and outside $J$. We've seen that multiplication is continuous, but also so is $J$ (near the origin) because it is a power series of positive radius of convergence. Thus taking inverses is locally continuous, which is equivalent to continuous.
\end{sol}

\begin{ex}
\leavevmode
\begin{enumerate}
\item Show that $\Ad$ is continuous. Moreover, that it is differentiable at $1_A$ and $(\dl \Ad)_{1_A} = \ad$.
\item Check that $[\ad a, \ad b] = \ad([a,b])$.
\end{enumerate}
\end{ex}

\begin{sol}
\leavevmode
\begin{enumerate}
\item Since by hypothesis $A$ is finite dimensional, $\End A$ can be identified with a space of square matrices, an in particular with $\R^N$ or $\C^N$ for some $N$. Thus, to verify that $\Ad$ is continuous and differentiable, it suffices to verify that all its coordinates are differentiable, i.e: pick a basis $e_1, \dots, e_n$ for $A$, and let $\omega^1, \dots, \omega^n$ be its dual basis. Then, we intend to show that the map $f_{ij}(a) = \omega^i \Ad(a)(e_j)$ is differentiable.

Now, we can write this down explicitly as $a \mapsto \omega^i(a e_j a^{-1})$. In turn, this is a composition of a projection $\omega^i$ (trivially differentiable) with the expression $a e_j a^{-1}$. If we show that both the inverse and product are differentiable maps, we can show that this expression is differentiable and compute its derivative with the chain rule.

The derivative of the product coincides with the usual Leibniz rule, by the same proof as in usual calculus. The derivative of the inverse can be computed writing down the power series expression for $(a + x)^{-1}$, whose first terms are
\begin{equation}
(a+x)^{-1} = a^{-1} - a^{-1} x a^{-1} + O(x^2).
\end{equation}

Thus, $(\dl (\cdot)^{-1})_a(x) = -a^{-1} x a^{-1} = -\Ad(a)(x)$.

In conclusion, we have shown that $\Ad$ is differentiable, and we will now compute its derivative at $1_A$. To do so, we expand $\Ad(a)$ as a power series expansion in $a$, but work modulo second and higher order terms, and in the end we will obtain a linear approximation of $\Ad$ near the identity, which by definition gives us the derivative.
\begin{equation}
\begin{multlined}
\Ad(1+a)(x) = (1+a)x(1+a)^{-1} \approx (1+a) x (1 - a)x\\
= x + ax - xa - axa \approx x + [a,x] = x + \ad(a)(x).
\end{multlined}
\end{equation}

Thus, the derivative of $\Ad$ at 1 is $\ad$.

\item Okay.
\begin{equation}
\begin{aligned}
[\ad a, \ad b](x) &= (\ad a)(\ad b)(x) - (\ad b)(\ad a)(x)\\
&= (\ad a)(bx - xb) - (\ad b)(ax - xa)\\
&= abx - axb - bxa + xba - bax + bxa + axb - xab\\
&= abx + xba - bax - xab\\
&= (ab-ba)x - x(ab-ba) = [[a,b],x] = \ad([a,b])(x)
\end{aligned}
\end{equation}

\phantom{Bottom text.}
\end{enumerate}
\end{sol}

\begin{ex}
Prove that if $G$ is a connected Lie group TFAE
\begin{enumerate}
\item $G$ is commutative,
\item $\Lie G$ is abelian,
\item $G$ is isomorphic as a topological group to some $\R^n \times (S^1)^m$.
\end{enumerate}
\end{ex}

\begin{sol}
First, we use exercise 1 to prove that (i) implies (ii). Indeed, we know that $\frac{[a,b]}2$ may be computed as the limit as $n \to \infty$ of
\begin{equation}
B_n(a,b) = n^2 \log( \exp(a/n) \exp(b/n) \exp(-(a+b)/n) ).
\end{equation}
Now, since $\exp(a/n)$ and $\exp(b/n)$ commute when $a, b \in \Lie G$, this expression may be interchanged to get
\begin{equation}
B_n(a,b) = n^2 \log( \exp(b/n) \exp(a/n) \exp(-(a+b)/n) ) = B_n(b,a),
\end{equation}
and taking the limit we get that $\frac{[a,b]}2 = \frac{[b,a]}2$, thus by antisymmetry $[a,b] = 0$.

Next, we show that (ii) implies (iii). To do so, we begin with the following lemma: if $G$ is a connected Lie group, any element of $G$ may be written as a product of exponentials of elements of $\Lie G$. Indeed, we know by a theorem in class that $\exp$ surjects onto an open neighborhood of the identity, and we know by an exercise from a few homeworks ago that if $G$ is a connected topological group and $U$ is a neighborhood of its identity, any element of $G$ can be written as a finite product of elements of $U$.

Finally, since $\Lie G$ is abelian, all its elements commute as matrices. Therefore, we can turn products of exponents into exponents of sums, and since $\Lie G$ is a vector space we conclude that $\exp \colon \Lie G \to G$ is surjective in this case, so it suffices to look at its kernel.

Since $\exp$ is a local diffeomorphism, its kernel (which is evidently a subgroup of $\Lie G$) is discrete. Thus, by another homework we did a few weeks ago, there exists a basis of $\Lie G$ in which $H = \ker \exp$ is the integer lattice generated by the first $m$ basis elements, and hence $\Lie G / H \cong \R^{\dim G} / \R^m \cong \R^n \times (S^1)^m$ (these are isomorphisms of Lie groups). Finally, by the isomorphism theorem we have that $f \colon \R^n \times (S^1)^m \to G$ is an isomorphism, where $f$ is the map induced by the exponential. Moreover, since the exponential is a continuous open map, we conclude that so is $f$, and therefore $f$ is an isomorphism which is continuous and open, thereby being a homeomorphism, and so an isomorphism of topological groups. This completes this part of the proof.

\smallskip

Obviously (iii) implies (i).

\smallskip

Since we did the three cyclical implications, the equivalence is complete.
\end{sol}

\end{document}