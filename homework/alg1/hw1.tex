\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{mathtools}

\usepackage[thmmarks, amsmath]{ntheorem}

\usepackage{graphicx}

\usepackage{diffcoeff}
\diffdef{}{op-symbol=\mathrm{d},op-order-sep=0mu}

\usepackage{cancel}

\usepackage{enumitem}

\setlist[enumerate,1]{label=(\roman*)}

\title{Title}
\author{Duarte Maia}
%\date{}

\theorembodyfont{\upshape}
\theoremseparator{.}
\newtheorem{theorem}{Theorem}
\newtheorem{prop}{Prop}
\renewtheorem*{prop*}{Prop}
\newtheorem{lemma}{Lemma}

\newtheorem{ex}{Exercise}

\theoremstyle{nonumberplain}
\theoremheaderfont{\itshape}
\theorembodyfont{\upshape}
\theoremseparator{:}
\theoremsymbol{\ensuremath{\blacksquare}}
\newtheorem{proof}{Proof}
\newtheorem{sol}{Solution}

\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}

\newcommand{\kk}{\Bbbk}

\newcommand{\PP}{\mathbb{P}}
\newcommand{\FF}{\mathcal{F}}

\newcommand{\I}{\mathrm{i}}
\newcommand{\e}{\mathrm{e}}

\newcommand{\conj}[1]{\overline{#1}}

\DeclareMathOperator{\inte}{int}
\DeclareMathOperator{\codim}{codim}
\newcommand{\grad}{\nabla}


\DeclareMathOperator{\spec}{spec}

\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\norm}{\lvert}{\rvert}
\DeclarePairedDelimiter{\Norm}{\lVert}{\rVert}
\DeclarePairedDelimiter{\braket}{\langle}{\rangle}


\begin{document}
\maketitle

\begin{ex}\leavevmode
\begin{enumerate}
\item Prove that $\spec S$ is nonempty.
\item Show that if $S$ forms an algebra then $\mu \in \spec S$ is an algebra homomorphism $S \to \kk$.
\item Let $\mu_1, \dots, \mu_r \in \spec S$ all distinct. Suppose $v_1 + \dots + v_r = 0$ with all $v_i \in V_{\mu_i}$. Then, every $v_i$ is null.
\item Show that if all operators $s \in S$ are diagonalizable then $V = \bigoplus_{\mu \in \spec S} V_\mu$.
\end{enumerate}
\end{ex}

\begin{sol}\leavevmode
\begin{enumerate}
\item{} [Note: It is necessary to add the hypothesis that $V \neq 0$.]

We perform strong induction in $\dim V$. We rephrase the problem in the following (obviously equivalent) manner: We wish to find at least one eigenvector in $V$ common to all $s \in S$. [From it, we construct $\mu(s) = \frac{s(v)}v$, where this fraction has the obvious meaning; then $v \in V_\mu$ and hence $V_\mu \neq 0$.]

Base case ($\dim V = 1$): In this case any nonzero vector is an eigenvector to any operator, so picking any $v \in V \setminus 0$ will do.

Induction step: If every $s \in S$ is a scalar, picking any $v \in \setminus 0$ will do. This includes the case $S = \emptyset$. Otherwise, pick $s_0 \in S$ which is not a scalar. Pick an eigenvalue $\lambda$ of $s_0$, and let $W$ be the corresponding eigenspace (note $W \neq 0$).

We claim that all $s \in S$ restrict to endomorphisms of $W$. This is justified by the following calculation, which uses the fact that all operators in $S$ commute:
\begin{equation}
s_0(s(w)) = s(s_0(w)) = s(\lambda w) = \lambda s(w).
\end{equation}

Let $S'$ be the collection of restrictions of $s \in S$ to $W$. Clearly, $(S', W)$ are in the conditions of the induction hypothesis (here we use that $s_0$ is not a scalar to have $\dim W < \dim V$), and so there exists some $v \in W$ which is an eigenvector of all $s \in S'$. It is evident that this same $v$ is also an eigenvector of all $s \in S$. This completes the proof.

\item Let $v \in V_\mu$ be a nonzero vector. Then, $v$ uniquely determines $\mu$ by the `expression'
\begin{equation}
\mu(s) = \frac{s(v)}v.
\end{equation}

In general, division of one vector by another does not make sense, but since $v$ is a common eigenvector to all $s \in S$ it does make sense (assuming $v\neq0$, which is the case).

Then, we can verify the desired properties (of algebra homomorphism) directly. In the following, $e$ is the identity, $s_1, s_2 \in S$, and $\lambda \in \kk$.

\begin{itemize}
\item $\mu(e) = \frac vv = 1$,
\item $\mu(s_1 + s_2) = \frac{(s_1 + s_2)(v)}v = \frac{s_1(v)}v + \frac{s_2(v)}v = \mu(s_1) + \mu(s_2)$,
\item $\mu(\lambda s_1) = \frac{\lambda s_1(v)}v = \lambda \frac{s_1(v)}v = \lambda \mu(s_1)$,
\item $\mu(s_1 s_2) = \frac{s_1(s_2(v))}v = \frac{s_1\left(\frac{s_2(v)}v v\right)}v = \frac{s_1(\mu(s_2) v)}v = \mu(s_2) \frac{s_2(v)}v = \mu(s_1) \mu(s_2)$.
\end{itemize}

\item We perform induction on $r$. Evidently, the statement is true for $r = 1$, so we do the induction step.

Suppose that the statement is proved for some fixed $r$; we shall prove it for $r+1$. Let $\mu_1, \dots, \mu_{r+1}$ be distinct elements of $\spec S$. Let
\begin{equation}\label{eq:eq1}
v_1 + \dots + v_{r+1} = 0
\end{equation}
with each $v_i \in V_{\mu_i}$.

Since all $\mu_i$ are distinct, then in particular $\mu_1$ and $\mu_{r+1}$ must disagree on at least one point $s \in S$. Keeping this particular $s$ in mind, it is evident that both of the following expressions hold, the first by multiplying \eqref{eq:eq1} by $s(\mu_{r+1})$ and the second by applying $s$ to both sides of $\eqref{eq:eq1}$ and simplifying:
\begin{gather}
s(\mu_{r+1}) v_1 + \dots + s(\mu_{r+1}) v_{r+1} = 0,\\
s(\mu_1) v_1 + \dots + s(\mu_{r+1}) v_{r+1} = 0.
\end{gather}

Subtracting one expression from the other, we obtain
\begin{equation}
(s(\mu_{r+1})-s(\mu_1)) v_1 + \dots + (s(\mu_{r+1})-s(\mu_{r})) v_{r} = 0,
\end{equation}
and so all terms of this sum must be null by the induction hypothesis. In particular, since $s(\mu_{r+1}) \neq s(\mu_1)$, $v_1 = 0$ in \eqref{eq:eq1}. Now, we may apply the induction hypothesis on the collection $\mu_2, \dots, \mu_{r+1}$ using the fact that, by \eqref{eq:eq1}, $v_2 + \dots + v_{r+1} = 0$, yielding that all other $v_i$ are zero.

\item The previous item shows that the spaces $V_\mu$ are independent, so it suffices to show that they span $V$.

In the following we will use the following rephrasing of the statement `the spaces $V_\mu$ span $V$': there exists a spanning set of $V$ whose elements are eigenvectors of all $s \in S$.

We perform strong induction on the dimension of $V$. The case $\dim V = 1$ is evident, as $V_\mu \neq 0$ and hence is the entire space (for any given $\mu \in \spec S$, which exists by (i)). [We could also have started with $\dim V = 0$; in this case $V=0$ is not a problem.]

Now, suppose that the proposition has been shown for all vector spaces with dimension less than the dimension of a given space $V$. As in the proof of (i), if all $s \in S$ are scalars the proposition is evident (if $\mu(s)$ is the scalar corresponding to $s$ for all $s$, $V_\mu = V$). Hence, we may without loss of generality assume that $s_0$ is a fixed nonscalar element of $S$.

Since $s_0$ is diagonalizable, its eigenspaces span $V$. Therefore, it suffices to show that $\bigoplus V_\mu$ spans all eigenspaces of $s_0$. Therefore, pick any eigenspace $W$ of $s_0$; to it we will apply the induction hypothesis.

As in (i), by the commutativity of the operators all $s \in S$ restrict to operators on $W$, hence by the induction hypothesis there exists a spanning set of $W$ composed of eigenvectors common to all $s \in S$ [Technically they are eigenvectors of the restrictions of $s$ to $W$, but this is also an eigenvector of $s$], let us call this set $A_W$.

By taking the union $A = \bigcup A_W$ over all eigenspaces of $s_0$, we obtain a collection of common eigenvectors which span all eigenspaces of $s_0$, and so, by diagonalizability, span all of $V$. This completes the induction step, and hence the proof.
\end{enumerate}
\end{sol}

\begin{ex} \leavevmode
\begin{enumerate}
\item Show that an $O_n$-invariant polynomial is of the form $f(\vec x) = \phi(R^2(\vec x))$.
\item Show that the collection of conjugation-invariant polynomials is freely generated by $s_1, \dots, s_n$.
\end{enumerate}
\end{ex}

\begin{sol} \leavevmode
\begin{enumerate}
\item First, note that swapping the sign of any particular variable $x_i$ is an orthogonal transformation. Therefore, an $O_n$-invariant polynomial $f(\vec x)$ will be invariant under change of sign of any particular variable, and therefore \emph{all monomials have even degrees in every variable}, so that we may write $f$ in the form
\begin{equation}
f(\vec x) = q(x_1^2, \dots, x_n^2)
\end{equation}
for some polynomial $q$.

Moreover, note that $f$ is a radial function, because for any $\vec x$ there is an orthogonal transformation taking it to $(\norm{\vec x},0,\dots,0)$. As a consequence,
\begin{equation}
q(x_1^2, \dots, x_n^2) = q(\norm{\vec x}^2, 0, \dots, 0),
\end{equation} 
and so, if we set $\phi(t) = q(t,0,\dots,0)$, we obtain $f(\vec x) = q(R^2(\vec x))$, as desired.

\item In the following, let $p(x)$ be a conjugate invariant polynomial.

First, we use the fact that any complex matrix is conjugate to an upper triangular matrix. Moreover, by scaling one of the basis vectors we have that the following two matrices are equivalent for all $\lambda \neq 0$:
\begin{equation}\label{eq:ut}
\begin{bmatrix}
a_{11} & a_{12} & a_{13} & \cdots & a_{1n} \\
0 & a_{22} & a_{23} & \cdots & a_{2n} \\
0 & 0 & a_{33} & \cdots & a_{3n} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & a_{nn}
\end{bmatrix}
\sim
\begin{bmatrix}
a_{11} & \lambda a_{12} & \lambda a_{13} & \cdots & \lambda a_{1n} \\
0 & a_{22} & a_{23} & \cdots & a_{2n} \\
0 & 0 & a_{33} & \cdots & a_{3n} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & a_{nn}
\end{bmatrix}
\end{equation}

As a consequence, they take the same value on $p$, and by continuity, taking the limit as $\lambda \to 0$, we obtain
\begin{equation}
p\begin{pmatrix}
a_{11} & a_{12} & a_{13} & \cdots & a_{1n} \\
0 & a_{22} & a_{23} & \cdots & a_{2n} \\
0 & 0 & a_{33} & \cdots & a_{3n} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & a_{nn}
\end{pmatrix}
=
p\begin{pmatrix}
a_{11} & 0 & 0 & \cdots & 0 \\
0 & a_{22} & a_{23} & \cdots & a_{2n} \\
0 & 0 & a_{33} & \cdots & a_{3n} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & a_{nn}
\end{pmatrix}
\end{equation}

This argument can be repeated for the second basis vector and so on, and so we conclude
\begin{equation}
p\begin{pmatrix}
a_{11} & a_{12} & a_{13} & \cdots & a_{1n} \\
0 & a_{22} & a_{23} & \cdots & a_{2n} \\
0 & 0 & a_{33} & \cdots & a_{3n} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & a_{nn}
\end{pmatrix}
=
p\begin{pmatrix}
a_{11} & 0 & 0 & \cdots & 0 \\
0 & a_{22} & 0 & \cdots & 0 \\
0 & 0 & a_{33} & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & a_{nn}
\end{pmatrix}
=: q(a_{11}, \dots, a_{nn}).
\end{equation}

Now note that, using conjugation by permutation matrices, we obtain that $q$ is a symmetric polynomial, and hence, by a classical theorem, can be written in the form
\begin{equation}
q(\vec x) = r(S_1(\vec x), \dots, S_n(\vec x)),
\end{equation}
where the $S_i$ are the elementary symmetric polynomials and $r$ is a unique polynomial depending on $q$ (which in turn depends on $p$).

Now, to wrap up the proof, we show that $p(M) = r(s_1(M), \dots, s_n(M))$. To this effect, note that $M$ is conjugate to an upper triangular matrix as in \eqref{eq:ut}, and hence
\begin{equation}\label{eq:pr}
p(M) = r(S_1(\vec a), \dots, S_n(\vec a)).
\end{equation}
Moreover, notice that $a_{11}, \dots, a_{nn}$, as in \eqref{eq:ut}, are precisely the eigenvalues of $M$ in some order, with multiplicity. Furthermore, since the characteristic polynomial of $M$ is given by $(x-a_{11})\dots(x-a_{nn})$, by definition of the symmetric polynomials we obtain that, for all $i = 1, \dots, n$,
\begin{equation}
s_i(M) = S_i(\vec a).
\end{equation}

This, together with \eqref{eq:pr}, proves that the polynomials $s_i$ generate the algebra of conjugate-invariant polynomials. It remains to check freeness. Suppose that $r$ is a polynomial in $n$ variables such that
\begin{equation}
r(s_1(M), \dots, s_n(M)) = 0 \text{, for all $M$.}
\end{equation}

Then, restricting to the space of diagonal matrices (which is the same as 
$\C^n$) we obtain that
\begin{equation}
r(S_1(\vec a), \dots, S_n(\vec a)) = 0 \text{, for all $\vec a \in \C^n$.}
\end{equation}

However, the classical theory tells us that the elementary symmetric polynomials freely generate the algebra of symmetric polynomials, and so the polynomial $r$ must be null. This completes the proof.
\end{enumerate}
\end{sol}

\begin{ex}\leavevmode
\begin{enumerate}
\item Show that $O_n(\R)$ and $U_n$ are compact but $SL_n(\R)$ and $SL_n(\C)$ are not.

\item Show that $U_n$, $SU_n$ and $SO_n(\R)$ are all path connected.

\item Show that $O_n(\R)$ is not.
\end{enumerate}
\end{ex}

\begin{sol} (In the following, all instances of $O_n$ and $SO_n$ should be read as $O_n(\R)$ and $SO_n(\R)$ respectively.)
\begin{enumerate}
\item As subsets of $\R^{n^2}$ and $\C^{n^2}$, both $O_n$ and $U_n$ are the preimage under $M \mapsto M M^*$ of $\{I\}$. This is a continuous function and a closed set respectively, so $O_n$ and $U_n$ are closed subspaces of the respective ambient space, so it suffices to show that they are bounded. To this effect, note that all entries of an orthogonal/unitary matrix have norm less than or equal to 1, and hence these matrices are contained in the rectangle whose sides are all $[-1,1]$. This shows compactness.

To show that $SL_n(\R)$ and $SL_n(\C)$ are not compact [this requires $n > 1$ because otherwise this statement is false], it suffices to show that they are unbounded subsets of ambient space. To this effect, consider the matrices $M_\lambda$, for $\lambda \in \R$, defined by taking the identity matrix and replacing one of the nondiagonal entries by $\lambda$. All of these have determinant equal to one, but one of the entries is unbounded.

\item We will show that these spaces are path connected directly, by constructing a path from any given matrix $M$ to the identity. In order to avoid rewriting the same thing, in what follows let $\kk$ be $\R$ or $\C$ as appropriate.

To begin, we claim that the following operation (operation $\rho$)on a matrix can be performed continuously without changing orthonormality or the determinant. Given a matrix $M$, whose rows we call $m_1, \dots, m_n$, given two distinct indices $i \neq j$, and given two numbers $\alpha, \beta \in \R$ with $\alpha^2 + \beta^2 = 1$, replace the $i$-th row of $M$ with $\alpha m_i + \beta m_j$ and the $j$-th row with $-\beta m_i + \alpha m_j$.

To prove this claim, begin by noting that this operation does not change orthonormality (verified directly) nor the determinant (checked by multilinearity). Moreover, if we have a path $(\alpha(t), \beta(t))$, doing the operation using these two paths as the parameters will yield a path of matrices. Finally, we show that for any $\alpha, \beta$ in these conditions there exists a path $(\alpha(t),\beta(t))$ which starts at $(1,0)$ and ends at $(\alpha,\beta)$.  To do so, set $\alpha(t) = (1-t) + t \alpha$ and $\beta(t) = \pm\sqrt{1 - \abs{\alpha(t)}^2}$, where the sign $\pm$ is chosen as the sign of $\beta$. This concludes the proof the claim.

Now, in the complex case, we show that the following operation (operation $\mu$) can also be performed: multiply the $i$-th row by some complex number $z$ of unit norm, and the $j$-th column by its conjugate. The argument for preservation of orthonormality and determinant is equally simple, and to show that this operation can be performed continuously simply write $z = \e^{i t}$ and make $t$ vary from $0$ to some target value.

We may now begin constructing our path. If $m_1, \dots, m_n$ are the rows of a matrix $M$ in one of the spaces under consideration, we begin by using operation $\mu$ (if we are in the complex case) to make $m_{11}, m_{21}, \dots, m_{nn}$ all real, by using the $n$-th column as a `dump factor'. Then, (start here in the real case) use operation $\rho$ to repeatedly make $m_{21}, \dots, m_{(n-1)(n-1)}$ null, by `rotating these numbers to $m_{11}$'. Formally, at each step, given $(m_{11},m_{1k}) \in \R^2$, there is a (real) rotation that turns this vector into one of the form $(*,0)$.

At the end of this process, the first column has $m_{11}$ as the only non-null (real) entry. By using a rotation by $\pi$ if necessary, we may assume $m_{11} \geq 0$, and since each column has unit norm we have $m_{11} = 1$. By orthogonality of the columns, the resulting matrix looks like this:
\begin{equation}
M = \begin{bmatrix}
1 & 0 & \cdots & 0\\
0 & m_{22} & \cdots & m_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
0 & m_{n2} & \cdots & m_{nn}
\end{bmatrix}.
\end{equation}

Now, repeat the procedure recursively on the submatrices. It is evident that the operations $\rho$ and $\mu$, when applied to $M$, will not spoil the work that has already been done, so the process continues until the penultimate column, by which point the matrix will have the appearance
\begin{equation}
M = \begin{bmatrix}
1 & 0 & \cdots & 0 & 0\\
0 & 1 & \cdots & 0 & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & \cdots & 1 & 0 \\
0 & 0 & \cdots & 0 & m_{nn}
\end{bmatrix}.
\end{equation}

Now we look at the case we find ourselves in. If we are in $SO_n$ or $SU_n$, the determinant of $M$ is one and hence $m_{nn} = 1$, so we are at the identity, as desired. On the other hand, if we are in $U_n$ we only know that $\abs{m_{nn}} = 1$, but since we are not beholden to keeping the determinant constant, we may continuously vary $m_{nn}$ to 1 in the unit circle. This completes the construction, and hence the proof that $U_n$, $SU_n$ and $SO_n$ are path connected.

\item The image of a connected set under a continuous function is connected. The determinant is continuous. There exist orthogonal matrices with determinant $\pm 1$ (consider the identity with one or zero of its diagonal entries swapped for $-1$). These are the only possible values for determinants of orthogonal matrices. Hence, $\det O_n = \{-1,1\}$, which is disconected. Therefore, $O_n$ cannot be connected to begin with.

Note: The work done in the previous item shows that $O_n$ has exactly two connected components, categorized by their determinant.
\end{enumerate}
\end{sol}

\begin{ex}
Show that $SL_n(\R)$ and $SL_n(\C)$ are path connected.
\end{ex}

\begin{sol}
This solution follows the same approach as the previous, finding a path from any given matrix in the space to the identity.

Let $M$ be a (real or complex) matrix of determinant one, whose rows we call $m_1, \dots, m_n$. Then, the following operation can be done continuously in a way that preserves the determinant: given two distinct indices $i \neq j$, we may replace $m_j$ by $m_j + \alpha m_i$, where $\alpha$ is an arbitrary (real or complex) number. This can be done by simply considering the path of matrices which, at time $t$, looks like $M$ but its $j$-th row has been replaced by $m_j + \alpha t m_i$.

There is a lot that can be done with this row operation. For example, it is possible to swap two rows (though the sign of one of them must be swapped to preserve the determinant), using the following process
\begin{equation}
(m_i, m_j) \to (m_i, m_j - m_i) \to (m_j, m_j - m_i) \to (m_j, -m_i).
\end{equation}

With this and the row operation, it is possible to perform a slightly modified version of Gauss' algorithm, wherein one first ensures that $m_{11} \neq 0$\footnote{Some $m_{1i}$ must be nonzero because the matrix has nonzero determinant; swap the $i$-th row with the first row if needed.}, then subtracts appropriate multiples of it from $m_{12}, \dots, m_{1n}$ in order to make them equal to zero, and proceeds recursively on the lower-right submatrices. Then at the end the process can be done again but from down to up, ensuring that at the end we obtain a diagonal matrix.

If we are in the complex case, we perform operation $\mu$ from the previous exercise in order to make each diagonal element equal to one, by using $m_{nn}$ as a `factor dump', and then at the end the determinant being equal to one forces $m_{nn}$ to be one as well.

In the real case, this is not possible, but we can still apply $\mu$ to multiply one of the diagonal elements by $\lambda$ and another by $\frac1\lambda$. As such, at the end, we obtain a diagonal matrix whose elements are all $\pm 1$. To conclude, use operation $\rho$ from the previous exercise. Using rotations by $\pi$, it is possible to swap the signs of two diagonal elements at a time. Again we use $m_{nn}$ as a sign dump, and at the end the one-ness of the determinant forces $m_{nn}$ to be positive one. This completes the proof.
\end{sol}

\begin{ex}
Prove that any element of $SO_3(\R)$ is rotation about an axis.
\end{ex}

\begin{sol}
Let $M \in SO_3(\R)$. Then, $M$ has three complex eigenvalues, whose product is $1$.

There are two cases:
\begin{itemize}
\item If all eigenvalues of $M$ are real, then they must all be $\pm 1$ by orthogonality. Since their product is $+1$ and $3$ is odd, at least one of them must be $+1$.
\item On the other hand, if $M$ has some nonreal eigenvalue $\lambda$, it must be part of a conjugate pair of eigenvalues. The remaining eigenvalue has to be real, because nonreal roots of real polynomials come in pairs. This eigenvalue must be $\lambda' = \pm 1$, and since the product of eigenvalues is 1 we have
\begin{equation}
\lambda' = \frac1{\lambda \conj \lambda} = \frac1{\abs{\lambda}^2} > 0.
\end{equation}
\end{itemize}

In either case, we conclude that $M$ has 1 as an eigenvalue; let $e_1$ be a corresponding eigenvector and extend it to a positively oriented orthonormal basis $e_1, e_2, e_3$. In this basis, $M$ is written in the form
\begin{equation}
BMB^{-1} =
\begin{bmatrix}
1 & 0 & 0\\
0 & \alpha & \beta\\
0 & \gamma & \delta
\end{bmatrix},
\end{equation}
where the first column is as it is because $e_1$ is an eigenvector and the first row is as it is by orthogonality between the first column and the others.

Now, the $2 \times 2$ matrix $m = \left[\begin{smallmatrix} \alpha & \beta \\ \gamma & \delta \end{smallmatrix}\right]$ is an orthonormal $2 \times 2$ matrix, and therefore must be of the form
\begin{equation}\label{eq:m}
m = 
\begin{bmatrix}
\cos(\alpha) & \cos(\beta) \\
\sin(\alpha) & \sin(\beta)
\end{bmatrix},
\end{equation}
for some real $\alpha$ and $\beta$, with $\beta = \alpha \pm \pi$. The fact that \eqref{eq:m} holds is because its columns are normal vectors and hence must have that form, and $\beta = \alpha \pm \pi$ can be seen by using orthogonality and the trigonometric formula for the cosine of the difference. To conclude, we can actually see that $\beta = \alpha + \pi$ by using the fact that the determinant is one (we're using the formula for the determinant of a block diagonal matrix here) and the formula for the sine of the difference, from which we obtain $\sin(\beta - \alpha) = 1$. The resulting matrix is of the form
\begin{equation}
m = 
\begin{bmatrix}
\cos(\alpha) & -\sin(\alpha) \\
\sin(\alpha) & \cos(\alpha)
\end{bmatrix},
\end{equation}
which is a rotation matrix. Hence, $M$ rotates the plane $\braket{e_2, e_3}$ around the vector $e_1$ by an angle of $\alpha$. This concludes the proof.
\end{sol}

\end{document}