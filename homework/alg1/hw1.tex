\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{mathtools}

\usepackage[thmmarks, amsmath]{ntheorem}

\usepackage{graphicx}

\usepackage{diffcoeff}
\diffdef{}{op-symbol=\mathrm{d},op-order-sep=0mu}

\usepackage{cancel}
\usepackage{interval}

\usepackage{enumitem}

\setlist[enumerate,1]{label=(\roman*)}

\title{Algebra Homework 1}
\author{Duarte Maia}
%\date{}

\theorembodyfont{\upshape}
\theoremseparator{.}
\newtheorem{theorem}{Theorem}
\newtheorem{prop}{Prop}
\renewtheorem*{prop*}{Prop}
\newtheorem{lemma}{Lemma}

\newtheorem{ex}{Exercise}

\theoremstyle{nonumberplain}
\theoremheaderfont{\itshape}
\theorembodyfont{\upshape}
\theoremseparator{:}
\theoremsymbol{\ensuremath{\blacksquare}}
\newtheorem{proof}{Proof}
\newtheorem{sol}{Solution}

\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}

\newcommand{\kk}{\Bbbk}

\newcommand{\PP}{\mathbb{P}}
\newcommand{\FF}{\mathcal{F}}

\newcommand{\I}{\mathrm{i}}
\newcommand{\e}{\mathrm{e}}

\newcommand{\conj}[1]{\overline{#1}}

\DeclareMathOperator{\inte}{int}
\DeclareMathOperator{\codim}{codim}
\newcommand{\grad}{\nabla}


\DeclareMathOperator{\spec}{spec}

\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\norm}{\lvert}{\rvert}
\DeclarePairedDelimiter{\Norm}{\lVert}{\rVert}
\DeclarePairedDelimiter{\braket}{\langle}{\rangle}


\begin{document}
\maketitle

\begin{ex}\leavevmode
\begin{enumerate}
\item Prove that $\spec S$ is nonempty.
\item Show that if $S$ forms an algebra then $\mu \in \spec S$ is an algebra homomorphism $S \to \kk$.
\item Let $\mu_1, \dots, \mu_r \in \spec S$ all distinct. Suppose $v_1 + \dots + v_r = 0$ with all $v_i \in V_{\mu_i}$. Then, every $v_i$ is null.
\item Show that if all operators $s \in S$ are diagonalizable then $V = \bigoplus_{\mu \in \spec S} V_\mu$.
\end{enumerate}
\end{ex}

\begin{sol}\leavevmode
\begin{enumerate}
\item{} [Note: It is necessary to add the hypothesis that $V \neq 0$.]

We perform strong induction in $\dim V$. We rephrase the problem in the following (obviously equivalent) manner: We wish to find at least one eigenvector in $V$ common to all $s \in S$. [From it, we construct $\mu(s) = \frac{s(v)}v$, where this fraction has the obvious meaning; then $v \in V_\mu$ and hence $V_\mu \neq 0$.]

Base case ($\dim V = 1$): In this case any nonzero vector is an eigenvector to any operator, so picking any $v \in V \setminus 0$ will do.

Induction step: If every $s \in S$ is a scalar, picking any $v \in \setminus 0$ will do. This includes the case $S = \emptyset$. Otherwise, pick $s_0 \in S$ which is not a scalar. Pick an eigenvalue $\lambda$ of $s_0$, and let $W$ be the corresponding eigenspace (note $W \neq 0$).

We claim that all $s \in S$ restrict to endomorphisms of $W$. This is justified by the following calculation, which uses the fact that all operators in $S$ commute:
\begin{equation}
s_0(s(w)) = s(s_0(w)) = s(\lambda w) = \lambda s(w).
\end{equation}

Let $S'$ be the collection of restrictions of $s \in S$ to $W$. Clearly, $(S', W)$ are in the conditions of the induction hypothesis (here we use that $s_0$ is not a scalar to have $\dim W < \dim V$), and so there exists some $v \in W$ which is an eigenvector of all $s \in S'$. It is evident that this same $v$ is also an eigenvector of all $s \in S$. This completes the proof.

\item Let $v \in V_\mu$ be a nonzero vector. Then, $v$ uniquely determines $\mu$ by the `expression'
\begin{equation}
\mu(s) = \frac{s(v)}v.
\end{equation}

In general, division of one vector by another does not make sense, but since $v$ is a common eigenvector to all $s \in S$ it does make sense (assuming $v\neq0$, which is the case).

Then, we can verify the desired properties (of algebra homomorphism) directly. In the following, $e$ is the identity, $s_1, s_2 \in S$, and $\lambda \in \kk$.

\begin{itemize}
\item $\mu(e) = \frac vv = 1$,
\item $\mu(s_1 + s_2) = \frac{(s_1 + s_2)(v)}v = \frac{s_1(v)}v + \frac{s_2(v)}v = \mu(s_1) + \mu(s_2)$,
\item $\mu(\lambda s_1) = \frac{\lambda s_1(v)}v = \lambda \frac{s_1(v)}v = \lambda \mu(s_1)$,
\item $\mu(s_1 s_2) = \frac{s_1(s_2(v))}v = \frac{s_1\left(\frac{s_2(v)}v v\right)}v = \frac{s_1(\mu(s_2) v)}v = \mu(s_2) \frac{s_2(v)}v = \mu(s_1) \mu(s_2)$.
\end{itemize}

\item We perform induction on $r$. Evidently, the statement is true for $r = 1$, so we do the induction step.

Suppose that the statement is proved for some fixed $r$; we shall prove it for $r+1$. Let $\mu_1, \dots, \mu_{r+1}$ be distinct elements of $\spec S$. Let
\begin{equation}\label{eq:eq1}
v_1 + \dots + v_{r+1} = 0
\end{equation}
with each $v_i \in V_{\mu_i}$.

Since all $\mu_i$ are distinct, then in particular $\mu_1$ and $\mu_{r+1}$ must disagree on at least one point $s \in S$. Keeping this particular $s$ in mind, it is evident that both of the following expressions hold, the first by multiplying \eqref{eq:eq1} by $s(\mu_{r+1})$ and the second by applying $s$ to both sides of $\eqref{eq:eq1}$ and simplifying:
\begin{gather}
s(\mu_{r+1}) v_1 + \dots + s(\mu_{r+1}) v_{r+1} = 0,\\
s(\mu_1) v_1 + \dots + s(\mu_{r+1}) v_{r+1} = 0.
\end{gather}

Subtracting one expression from the other, we obtain
\begin{equation}
(s(\mu_{r+1})-s(\mu_1)) v_1 + \dots + (s(\mu_{r+1})-s(\mu_{r})) v_{r} = 0,
\end{equation}
and so all terms of this sum must be null by the induction hypothesis. In particular, since $s(\mu_{r+1}) \neq s(\mu_1)$, $v_1 = 0$ in \eqref{eq:eq1}. Now, we may apply the induction hypothesis on the collection $\mu_2, \dots, \mu_{r+1}$ using the fact that, by \eqref{eq:eq1}, $v_2 + \dots + v_{r+1} = 0$, yielding that all other $v_i$ are zero.

\item The previous item shows that the spaces $V_\mu$ are independent, so it suffices to show that they span $V$.

In the following we will use the following rephrasing of the statement `the spaces $V_\mu$ span $V$': there exists a spanning set of $V$ whose elements are eigenvectors of all $s \in S$.

We perform strong induction on the dimension of $V$. The case $\dim V = 1$ is evident, as $V_\mu \neq 0$ and hence is the entire space (for any given $\mu \in \spec S$, which exists by (i)). [We could also have started with $\dim V = 0$; in this case $V=0$ is not a problem.]

Now, suppose that the proposition has been shown for all vector spaces with dimension less than the dimension of a given space $V$. As in the proof of (i), if all $s \in S$ are scalars the proposition is evident (if $\mu(s)$ is the scalar corresponding to $s$ for all $s$, $V_\mu = V$). Hence, we may without loss of generality assume that $s_0$ is a fixed nonscalar element of $S$.

Since $s_0$ is diagonalizable, its eigenspaces span $V$. Therefore, it suffices to show that $\bigoplus V_\mu$ spans all eigenspaces of $s_0$. Therefore, pick any eigenspace $W$ of $s_0$; to it we will apply the induction hypothesis.

As in (i), by the commutativity of the operators all $s \in S$ restrict to operators on $W$, hence by the induction hypothesis there exists a spanning set of $W$ composed of eigenvectors common to all $s \in S$ [Technically they are eigenvectors of the restrictions of $s$ to $W$, but this is also an eigenvector of $s$], let us call this set $A_W$.

By taking the union $A = \bigcup A_W$ over all eigenspaces of $s_0$, we obtain a collection of common eigenvectors which span all eigenspaces of $s_0$, and so, by diagonalizability, span all of $V$. This completes the induction step, and hence the proof.
\end{enumerate}
\end{sol}

\begin{ex} \leavevmode
\begin{enumerate}
\item Show that an $O_n$-invariant polynomial is of the form $f(\vec x) = \phi(R^2(\vec x))$.
\item Show that the collection of conjugation-invariant polynomials is freely generated by $s_1, \dots, s_n$.
\end{enumerate}
\end{ex}

\begin{sol} \leavevmode
\begin{enumerate}
\item First, note that swapping the sign of any particular variable $x_i$ is an orthogonal transformation. Therefore, an $O_n$-invariant polynomial $f(\vec x)$ will be invariant under change of sign of any particular variable, and therefore \emph{all monomials have even degrees in every variable}, so that we may write $f$ in the form
\begin{equation}
f(\vec x) = q(x_1^2, \dots, x_n^2)
\end{equation}
for some polynomial $q$.

Moreover, note that $f$ is a radial function, because for any $\vec x$ there is an orthogonal transformation taking it to $(\norm{\vec x},0,\dots,0)$. As a consequence,
\begin{equation}
q(x_1^2, \dots, x_n^2) = q(\norm{\vec x}^2, 0, \dots, 0),
\end{equation} 
and so, if we set $\phi(t) = q(t,0,\dots,0)$, we obtain $f(\vec x) = q(R^2(\vec x))$, as desired.

\item In the following, let $p(x)$ be a conjugate invariant polynomial.

First, we use the fact that any complex matrix is conjugate to an upper triangular matrix. Moreover, by scaling one of the basis vectors we have that the following two matrices are equivalent for all $\lambda \neq 0$:
\begin{equation}\label{eq:ut}
\begin{bmatrix}
a_{11} & a_{12} & a_{13} & \cdots & a_{1n} \\
0 & a_{22} & a_{23} & \cdots & a_{2n} \\
0 & 0 & a_{33} & \cdots & a_{3n} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & a_{nn}
\end{bmatrix}
\sim
\begin{bmatrix}
a_{11} & \lambda a_{12} & \lambda a_{13} & \cdots & \lambda a_{1n} \\
0 & a_{22} & a_{23} & \cdots & a_{2n} \\
0 & 0 & a_{33} & \cdots & a_{3n} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & a_{nn}
\end{bmatrix}
\end{equation}

As a consequence, they take the same value on $p$, and by continuity, taking the limit as $\lambda \to 0$, we obtain
\begin{equation}
p\begin{pmatrix}
a_{11} & a_{12} & a_{13} & \cdots & a_{1n} \\
0 & a_{22} & a_{23} & \cdots & a_{2n} \\
0 & 0 & a_{33} & \cdots & a_{3n} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & a_{nn}
\end{pmatrix}
=
p\begin{pmatrix}
a_{11} & 0 & 0 & \cdots & 0 \\
0 & a_{22} & a_{23} & \cdots & a_{2n} \\
0 & 0 & a_{33} & \cdots & a_{3n} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & a_{nn}
\end{pmatrix}
\end{equation}

This argument can be repeated for the second basis vector and so on, and so we conclude
\begin{equation}
p\begin{pmatrix}
a_{11} & a_{12} & a_{13} & \cdots & a_{1n} \\
0 & a_{22} & a_{23} & \cdots & a_{2n} \\
0 & 0 & a_{33} & \cdots & a_{3n} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & a_{nn}
\end{pmatrix}
=
p\begin{pmatrix}
a_{11} & 0 & 0 & \cdots & 0 \\
0 & a_{22} & 0 & \cdots & 0 \\
0 & 0 & a_{33} & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & a_{nn}
\end{pmatrix}
=: q(a_{11}, \dots, a_{nn}).
\end{equation}

Now note that, using conjugation by permutation matrices, we obtain that $q$ is a symmetric polynomial, and hence, by a classical theorem, can be written in the form
\begin{equation}
q(\vec x) = r(S_1(\vec x), \dots, S_n(\vec x)),
\end{equation}
where the $S_i$ are the elementary symmetric polynomials and $r$ is a unique polynomial depending on $q$ (which in turn depends on $p$).

Now, to wrap up the proof, we show that $p(M) = r(s_1(M), \dots, s_n(M))$. To this effect, note that $M$ is conjugate to an upper triangular matrix as in \eqref{eq:ut}, and hence
\begin{equation}\label{eq:pr}
p(M) = r(S_1(\vec a), \dots, S_n(\vec a)).
\end{equation}
Moreover, notice that $a_{11}, \dots, a_{nn}$, as in \eqref{eq:ut}, are precisely the eigenvalues of $M$ in some order, with multiplicity. Furthermore, since the characteristic polynomial of $M$ is given by $(x-a_{11})\dots(x-a_{nn})$, by definition of the symmetric polynomials we obtain that, for all $i = 1, \dots, n$,
\begin{equation}
s_i(M) = S_i(\vec a).
\end{equation}

This, together with \eqref{eq:pr}, proves that the polynomials $s_i$ generate the algebra of conjugate-invariant polynomials. It remains to check freeness. Suppose that $r$ is a polynomial in $n$ variables such that
\begin{equation}
r(s_1(M), \dots, s_n(M)) = 0 \text{, for all $M$.}
\end{equation}

Then, restricting to the space of diagonal matrices (which is the same as 
$\C^n$) we obtain that
\begin{equation}
r(S_1(\vec a), \dots, S_n(\vec a)) = 0 \text{, for all $\vec a \in \C^n$.}
\end{equation}

However, the classical theory tells us that the elementary symmetric polynomials freely generate the algebra of symmetric polynomials, and so the polynomial $r$ must be null. This completes the proof.
\end{enumerate}
\end{sol}

\begin{ex}\leavevmode
\begin{enumerate}
\item Show that $O_n(\R)$ and $U_n$ are compact but $SL_n(\R)$ and $SL_n(\C)$ are not.

\item Show that $U_n$, $SU_n$ and $SO_n(\R)$ are all path connected.

\item Show that $O_n(\R)$ is not.
\end{enumerate}
\end{ex}

\begin{sol} (In the following, all instances of $O_n$ and $SO_n$ should be read as $O_n(\R)$ and $SO_n(\R)$ respectively.)
\begin{enumerate}
\item As subsets of $\R^{n^2}$ and $\C^{n^2}$, both $O_n$ and $U_n$ are the preimage under $M \mapsto M M^*$ of $\{I\}$. This is a continuous function and a closed set respectively, so $O_n$ and $U_n$ are closed subspaces of the respective ambient space, so it suffices to show that they are bounded. To this effect, note that all entries of an orthogonal/unitary matrix have norm less than or equal to 1, and hence these matrices are contained in the rectangle whose sides are all $[-1,1]$. This shows compactness.

To show that $SL_n(\R)$ and $SL_n(\C)$ are not compact [this requires $n > 1$ because otherwise this statement is false], it suffices to show that they are unbounded subsets of ambient space. To this effect, consider the matrices $M_\lambda$, for $\lambda \in \R$, defined by taking the identity matrix and replacing one of the nondiagonal entries by $\lambda$. All of these have determinant equal to one, but one of the entries is unbounded.

\item We will show that these spaces are path connected directly, by constructing a path from any given matrix $M$ to the identity. In order to avoid rewriting the same thing, in what follows let $\kk$ be $\R$ or $\C$ as appropriate.

To begin, we claim that the following operation (operation $\rho$)on a matrix can be performed continuously without changing orthonormality or the determinant. Given a matrix $M$, whose rows we call $m_1, \dots, m_n$, given two distinct indices $i \neq j$, and given two numbers $\alpha, \beta \in \R$ with $\alpha^2 + \beta^2 = 1$, replace the $i$-th row of $M$ with $\alpha m_i + \beta m_j$ and the $j$-th row with $-\beta m_i + \alpha m_j$.

To prove this claim, begin by noting that this operation does not change orthonormality (verified directly) nor the determinant (checked by multilinearity). Moreover, if we have a path $(\alpha(t), \beta(t))$, doing the operation using these two paths as the parameters will yield a path of matrices. Finally, we show that for any $\alpha, \beta$ in these conditions there exists a path $(\alpha(t),\beta(t))$ which starts at $(1,0)$ and ends at $(\alpha,\beta)$.  To do so, set $\alpha(t) = (1-t) + t \alpha$ and $\beta(t) = \pm\sqrt{1 - \abs{\alpha(t)}^2}$, where the sign $\pm$ is chosen as the sign of $\beta$. This concludes the proof the claim.

Now, in the complex case, we show that the following operation (operation $\mu$) can also be performed: multiply the $i$-th row by some complex number $z$ of unit norm, and the $j$-th column by its conjugate. The argument for preservation of orthonormality and determinant is equally simple, and to show that this operation can be performed continuously simply write $z = \e^{i t}$ and make $t$ vary from $0$ to some target value.

We may now begin constructing our path. If $m_1, \dots, m_n$ are the rows of a matrix $M$ in one of the spaces under consideration, we begin by using operation $\mu$ (if we are in the complex case) to make $m_{11}, m_{21}, \dots, m_{nn}$ all real, by using the $n$-th column as a `dump factor'. Then, (start here in the real case) use operation $\rho$ to repeatedly make $m_{21}, \dots, m_{(n-1)(n-1)}$ null, by `rotating these numbers to $m_{11}$'. Formally, at each step, given $(m_{11},m_{1k}) \in \R^2$, there is a (real) rotation that turns this vector into one of the form $(*,0)$.

At the end of this process, the first column has $m_{11}$ as the only non-null (real) entry. By using a rotation by $\pi$ if necessary, we may assume $m_{11} \geq 0$, and since each column has unit norm we have $m_{11} = 1$. By orthogonality of the columns, the resulting matrix looks like this:
\begin{equation}
M = \begin{bmatrix}
1 & 0 & \cdots & 0\\
0 & m_{22} & \cdots & m_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
0 & m_{n2} & \cdots & m_{nn}
\end{bmatrix}.
\end{equation}

Now, repeat the procedure recursively on the submatrices. It is evident that the operations $\rho$ and $\mu$, when applied to $M$, will not spoil the work that has already been done, so the process continues until the penultimate column, by which point the matrix will have the appearance
\begin{equation}
M = \begin{bmatrix}
1 & 0 & \cdots & 0 & 0\\
0 & 1 & \cdots & 0 & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & \cdots & 1 & 0 \\
0 & 0 & \cdots & 0 & m_{nn}
\end{bmatrix}.
\end{equation}

Now we look at the case we find ourselves in. If we are in $SO_n$ or $SU_n$, the determinant of $M$ is one and hence $m_{nn} = 1$, so we are at the identity, as desired. On the other hand, if we are in $U_n$ we only know that $\abs{m_{nn}} = 1$, but since we are not beholden to keeping the determinant constant, we may continuously vary $m_{nn}$ to 1 in the unit circle. This completes the construction, and hence the proof that $U_n$, $SU_n$ and $SO_n$ are path connected.

\item The image of a connected set under a continuous function is connected. The determinant is continuous. There exist orthogonal matrices with determinant $\pm 1$ (consider the identity with one or zero of its diagonal entries swapped for $-1$). These are the only possible values for determinants of orthogonal matrices. Hence, $\det O_n = \{-1,1\}$, which is disconected. Therefore, $O_n$ cannot be connected to begin with.

Note: The work done in the previous item shows that $O_n$ has exactly two connected components, categorized by their determinant.
\end{enumerate}
\end{sol}

\begin{ex}
Show that $SL_n(\R)$ and $SL_n(\C)$ are path connected.
\end{ex}

\begin{sol}
This solution follows the same approach as the previous, finding a path from any given matrix in the space to the identity.

Let $M$ be a (real or complex) matrix of determinant one, whose rows we call $m_1, \dots, m_n$. Then, the following operation can be done continuously in a way that preserves the determinant: given two distinct indices $i \neq j$, we may replace $m_j$ by $m_j + \alpha m_i$, where $\alpha$ is an arbitrary (real or complex) number. This can be done by simply considering the path of matrices which, at time $t$, looks like $M$ but its $j$-th row has been replaced by $m_j + \alpha t m_i$.

There is a lot that can be done with this row operation. For example, it is possible to swap two rows (though the sign of one of them must be swapped to preserve the determinant), using the following process
\begin{equation}
(m_i, m_j) \to (m_i, m_j - m_i) \to (m_j, m_j - m_i) \to (m_j, -m_i).
\end{equation}

With this and the row operation, it is possible to perform a slightly modified version of Gauss' algorithm, wherein one first ensures that $m_{11} \neq 0$\footnote{Some $m_{1i}$ must be nonzero because the matrix has nonzero determinant; swap the $i$-th row with the first row if needed.}, then subtracts appropriate multiples of it from $m_{12}, \dots, m_{1n}$ in order to make them equal to zero, and proceeds recursively on the lower-right submatrices. Then at the end the process can be done again but from down to up, ensuring that at the end we obtain a diagonal matrix.

If we are in the complex case, we perform operation $\mu$ from the previous exercise in order to make each diagonal element equal to one, by using $m_{nn}$ as a `factor dump', and then at the end the determinant being equal to one forces $m_{nn}$ to be one as well.

In the real case, this is not possible, but we can still apply $\mu$ to multiply one of the diagonal elements by $\lambda$ and another by $\frac1\lambda$. As such, at the end, we obtain a diagonal matrix whose elements are all $\pm 1$. To conclude, use operation $\rho$ from the previous exercise. Using rotations by $\pi$, it is possible to swap the signs of two diagonal elements at a time. Again we use $m_{nn}$ as a sign dump, and at the end the one-ness of the determinant forces $m_{nn}$ to be positive one. This completes the proof.
\end{sol}

\begin{ex}
Prove that any element of $SO_3(\R)$ is rotation about an axis.
\end{ex}

\begin{sol}
Let $M \in SO_3(\R)$. Then, $M$ has three complex eigenvalues, whose product is $1$.

There are two cases:
\begin{itemize}
\item If all eigenvalues of $M$ are real, then they must all be $\pm 1$ by orthogonality. Since their product is $+1$ and $3$ is odd, at least one of them must be $+1$.
\item On the other hand, if $M$ has some nonreal eigenvalue $\lambda$, it must be part of a conjugate pair of eigenvalues. The remaining eigenvalue has to be real, because nonreal roots of real polynomials come in pairs. This eigenvalue must be $\lambda' = \pm 1$, and since the product of eigenvalues is 1 we have
\begin{equation}
\lambda' = \frac1{\lambda \conj \lambda} = \frac1{\abs{\lambda}^2} > 0.
\end{equation}
\end{itemize}

In either case, we conclude that $M$ has 1 as an eigenvalue; let $e_1$ be a corresponding eigenvector and extend it to a positively oriented orthonormal basis $e_1, e_2, e_3$. In this basis, $M$ is written in the form
\begin{equation}
BMB^{-1} =
\begin{bmatrix}
1 & 0 & 0\\
0 & \alpha & \beta\\
0 & \gamma & \delta
\end{bmatrix},
\end{equation}
where the first column is as it is because $e_1$ is an eigenvector and the first row is as it is by orthogonality between the first column and the others.

Now, the $2 \times 2$ matrix $m = \left[\begin{smallmatrix} \alpha & \beta \\ \gamma & \delta \end{smallmatrix}\right]$ is an orthonormal $2 \times 2$ matrix, and therefore must be of the form
\begin{equation}\label{eq:m}
m = 
\begin{bmatrix}
\cos(\alpha) & \cos(\beta) \\
\sin(\alpha) & \sin(\beta)
\end{bmatrix},
\end{equation}
for some real $\alpha$ and $\beta$, with $\beta = \alpha \pm \pi$. The fact that \eqref{eq:m} holds is because its columns are normal vectors and hence must have that form, and $\beta = \alpha \pm \pi$ can be seen by using orthogonality and the trigonometric formula for the cosine of the difference. To conclude, we can actually see that $\beta = \alpha + \pi$ by using the fact that the determinant is one (we're using the formula for the determinant of a block diagonal matrix here) and the formula for the sine of the difference, from which we obtain $\sin(\beta - \alpha) = 1$. The resulting matrix is of the form
\begin{equation}
m = 
\begin{bmatrix}
\cos(\alpha) & -\sin(\alpha) \\
\sin(\alpha) & \cos(\alpha)
\end{bmatrix},
\end{equation}
which is a rotation matrix. Hence, $M$ rotates the plane $\braket{e_2, e_3}$ around the vector $e_1$ by an angle of $\alpha$. This concludes the proof.
\end{sol}

\begin{ex}
Given $U$ an open neighborhood of the identity $e$ of a connected topological group $G$, show that $\bigcup U^n = G$.
\end{ex}

\begin{sol}
Let $X = \bigcup U^n$. Evidently, $X$ is open because for any $x \in X$, since $x \in U^n$, we have $Ux \subseteq U^{n+1} \subseteq X$; since multiplication by $x$ is a continuous map and $U$ contains the identity, $Ux$ is a neighborhood of $x$.

Now, to show that $X$ is closed, suppose that $g \in G \setminus X$, but moreover that any neighborhood of $g$ intersects $X$. Then, in particular, $U^{-1} g$ intersects $X$ (this is a neighborhood of $g$ by a similar argument, with the added step that the inverse operator preserves open sets because it is a homeomorphism). If $x \in X \cap U^{-1} g$, then there exists some $u \in U$ such that $x = u^{-1} g$, hence $g = u x$. Moreover, since $x \in X$ then $x \in U^n$ for some $n$. Hence, we conclude $g \in U^{n+1} \subseteq X$, which is a contradiction with the assumption that $g \not \in X$.

In conclusion, $X$ is both an open and a closed subset of $G$, and by connectedness, since $X$ is nonempty, $X = G$.
\end{sol}

\begin{ex}\leavevmode
\begin{enumerate}
\item Prove that if $G$ is path connected then any discrete normal subgroup of $G$ is contained within the center of $G$.
\item Prove that the connected component of the identity is a normal subgroup of $G$.
\end{enumerate}
\end{ex}

\begin{sol}\leavevmode
\begin{enumerate}
\item Let $N \subseteq G$ be a discrete normal subgroup, and let $n \in N$.  Given $g \in G$, pick a path $\gamma(t)$ from $e$ to $g$. Then, by normality, the path $\gamma(t) n \gamma(t)^{-1}$ is always contained in $N$, and since $N$ is discrete it must be a constant path. Since it equals $n$ at $t = 0$, we conclude that it equals $n$ for all time $t$, including $t = 1$, whence we conclude $g n g^{-1} = n$ and therefore $gn = ng$ for all $g \in G$. In other words, $n$ is in the center of $G$.
\item Define the connected component of the identity, call it $N$, as the maximal connected set which contains the identity. Then, in order to show that some set is contained within $N$ it suffices to show that it is connected and contains the identity.
\begin{itemize}
\item $N$ is closed under inverses because $N^{-1}$ is connected (inversion is a homeomorphism and hence preserves connectedness) and contains the identity.
\item $N$ is closed under multiplication because, given $n \in N$, $nN$ isconnected (multiplication by $n$ is a homeomorphism) and contains the identity (because $n^{-1} \in N$). Therefore, for any $n, n' \in N$ we have $nn' \in nN \subseteq N$.
\item $N$ is normal because conjugation is a homeomorphism and preserves the identity, so the conjugation of $N$ by any $g \in G$ will be contained in $N$.
\end{itemize}

This concludes the proof.
\end{enumerate}
\end{sol}

\begin{ex}
Let $H$ be a discrete subgroup of $V$, an $\R$-vector space. Show that there exist $e_1, \dots, e_d$, all linearly independent such that $H = \braket{e_1, \dots, e_d}_\Z$. (This is equivalent to the given statement; just extend this linearly independent set to a basis.)
\end{ex}

\begin{sol}
We induct on the dimension of $V$.

For $\dim V = 1$, $V$ is isomorphic to the real line. We identify $V$ with $\R$ in what follows. If $H \neq 0$ (otherwise the statement is trivial), consider $x = \inf_{h \in H \setminus 0} \norm h < \infty$. Note that $x > 0$ by discreteness, which implies that there exists a neighborhood of $0$ without any other elements of $H$. Moreover, note that the
re are no elements of $H$ in $\ointerval 0 x$, as if there were $x$ would not be the minimum. Furthermore, there is at most one element of $H$ in $\ointerval x {2x}$, because otherwise you could subtract one from the other and obtain an element of $H$ which has norm less than $x$, also a contradiction. Finally note that, since there are elements of $H$ arbitrarily close to $x$, $x$ itself must be an element of $H$.

To complete the proof of the $\dim V = 1$ case, we show that $H = \braket{x}_\Z$. Indeed, if $h \in H$ were not an integer multiple of $x$, we could find an appropriate $k \in \Z$ such that $h + k x \in \ointerval 0 x$, which is a contradiction with the definition of $x$.

\smallskip

Now we perform the induction step. In the general case of $n$-dimensional $V$, if $H = 0$ the statement is trivial. Therefore, begin by considering some $h \in H$, $h \neq 0$.

Define $L = \braket{h}_\R$. Then, consider the group $H \cap L$, which is a nontrivial subgroup of the one-dimensional vector space $L$. Apply the case for $\dim V = 1$ to find $h_0 \in L$ such that $H \cap L = \braket{h_0}_\Z$.

Now consider the quotient $V/L$. In a slight abuse of notation, define $H/L$ as the subgroup of $V/L$ given by the image of $H$ under the projection.

If we can show that $H/L$ is a discrete subgroup of $V/L$, the argument proceeds as follows. Apply the induction hypothesis to find some $[e_1], \dots, [e_d] \in H/L$ (with $e_i \in H$) such that the $[e_i]$ are linearly independent and generate $H/L$ over $\Z$. Then, consider the list $h_0, e_1, \dots, e_d$.

This list is linearly independent, because given a null linear combination, applying the projection and linear independence of $[e_i]$ shows that the coefficients of the $e_i$ are null, and the fact that $h_0 \neq 0$ shows that the coefficient of $h_0$ is null.

This list generates $H$ over $\Z$ because, given some $h \in H$, by the definition of the $[e_i]$ we have $[h] = \sum a_i [e_i]$ for some $a_i \in \Z$. Therefore, $h = \left( \sum a_i e_i \right) + \ell$ for some $\ell \in L$, but solving for $\ell$ we find $\ell \in L \cap H$, and hence $\ell = k h_0$ for some $h \in \Z$. Thus, we have just written $H$ as an integral linear combinations of elements in $h_0, e_1, \dots, e_d$.

Now, we prove that $H/L$ is indeed a discrete subgroup of $V/L$.\footnote{It is a known fact that the quotient topology on $V/L$ coincides with the natural topology on this vector space.} To do so, it suffices to show that there exists a neighborhood of $[0]$ which contains no elements of $H/L$, because this neighborhood can be translated by elements of $H/L$ to show that every element of $H/L$ is isolated.

Let $W$ be a complementary subspace of $L$ in $V$. Then, to say that $[0] \in H/L$ is not isolated is to say that there exists a sequence $h_n \in H$ such that $[h_n] \to [0]$ in $V/L$. We write each $h_n$ as $w_n + x_n h_0$, with $w_n \in W$. Now consider the following: since $x_n$ can be replaced by any element of $x_n + \Z$ (this may require modifying the sequence $h_n$), we may without loss of generality suppose that $x_n \in [0,1]$ for all $n$. As a consequence, $x_n$ has a converging subsequence. If we pass to this subsequence in $h_n$, we conclude that $h_n$ converges to some $\ell \in L$.

Now, consider the following. For each $n$, pick some $N(n) > n$ such that $[h_n] \neq [h_{N(n)}]$. This exists because, if it did not, $h_n$ would be eventually constant, but it cannot be because it converges to $[0]$ without ever actually being $[0]$. Now define $h'_n = h_n - h_{N(n)}$. This sequence is never null by definition of $N(n)$, and yet its limit is $\ell - \ell = 0$. Thus, we have a sequence of elements of $H$ which is never null and yet converges to zero, contradicting the hypothesis that $H$ is discrete. This contradiction, which stems from the assumption that $[0] \in H/L$ is not isolated, proves that $H/L$ is a discrete subgroup of $V/L$ and hence the induction step as described above follows through, and we have finished the solution.
\end{sol}

\begin{ex}\leavevmode
\begin{enumerate}
\item Prove that any continuous group homomorphism $f \colon \R \to \R^n$ is of the form $t \mapsto tv$.
\item Prove that any continuous group homomorphism $S^1 \to C^*$ is given by taking an integer power.
\end{enumerate}
\end{ex}

\begin{sol}
\leavevmode
\begin{enumerate}
\item Define $v = f(1)$. Then, if $n$ is a positive integer we see that
\begin{equation}
f(n) = f(1+\dots+1) = f(1)+\dots+f(1) = n f(1) = nv
\end{equation}

This could also have been verified more rigorously by induction. Moreover, the formula $f(n) = nv$ also holds for $n = 0$ trivially, and it also holds for negative $n$ because $f(-n) = -f(n)$.

A similar argument to above will show that, for any real $x$,
\begin{equation}\label{eq:fnx}
f(nx) = n f(x),
\end{equation}
hence, plugging $x = \frac1n$ for $n \neq 0$ we obtain $f(\frac1n) = \frac1n v$, and applying \eqref{eq:fnx} once again we can show that
\begin{equation}
f(\frac pq) = \frac pq v,
\end{equation}
and hence $f(x) = x v$ for all $x \in \Q$.

The argument is completed by using a limiting argument: given $x \in \R$, find a sequence of rationals $x_n$ which converges to $x$, and take the limit on both sides of $f(x_n) = x_n v$ to obtain $f(x) = x v$ for all $x \in \R$.

\item Since $S^1 \cong \R / 2\pi\Z$, a continuous map $f \colon S^1 \to \C^*$ is the same as a $2\pi$-periodic map $\R \to \C^*$.

Consider $f(1)$. Then, by an argument as in the previous item, for $n \in \Z$ we have $f(n) = f(1)^n$. Now, note that $S^1$ is compact, so the image of $f$ must be compact, and hence bounded. Therefore, $f(1)$ must have unit norm (otherwise we could get unbounded images by making $n \to \pm \infty$). The same argument holds if $1$ is replaced by any other real number, so we conclude that $f$ is actually a $2\pi$-periodic map $\R \to S^1$.

Now, we apply covering space theory. Let $p \colon \R \to S^1$ be the universal covering of $S^1$, with $p(x) = \e^{\I x}$. Then, since $\R$ (the domain of $f$) is simply connected, $f$ lifts uniquely to a map $\tilde f \colon \R \to \R$ with $\tilde f(0) = 0$.

We prove that $\tilde f$ is a homomorphism. Indeed, by definition $\tilde f(0) = 0$, so it suffices to show that $\tilde f(x+y) = \tilde f(x) + \tilde f(y)$. To this effect, consider $\gamma$ a path in $\R$ connecting $0$ to $x$ and $\eta$ a path connecting $0$ to $y$.

Define $\zeta(t)$ as follows: for $t \in [0,1]$, let $\zeta(t) = \gamma(t)$, and for $t \in [1,2]$ let $\zeta(t) = x + \eta(t-1)$. By the pasting lemma this is indeed a path, and it connects $0$ to $x+y$. Therefore, $\tilde f(x+y)$ may be found by lifting $\e^{\I \zeta(t)}$ to a path in $\R$ starting at $0$ and looking at its endpoint.

We write this lift explicitly. Write $\tilde \zeta(t) = \tilde \gamma(t)$ for $t \in [0,1]$ (where $\tilde \gamma$ is the lift of $p \circ \gamma$ starting at $0$) and set $\tilde \zeta(t) = \tilde f(x) + \tilde \eta(t-1)$ for $t \in [1,2]$ (where $\tilde \eta$ is analogously defined). This is evidently a continuous path, so it suffices to show that $p(\tilde \zeta(t)) = f(\zeta(t))$. For $t \in [0,1]$ this is true by definition of $\tilde \gamma$. For $t \in [1,2]$,
\begin{multline}
p(\tilde \zeta(t)) = p(\tilde f(x) + \tilde \eta(t-1)) = p(\tilde f(x)) p(\tilde \eta(t-1))\\
= f(x)f(\eta(t-1)) = f(x + \eta(t-1)) = f(\zeta(t)).
\end{multline}

Hence, $\tilde \zeta$ is indeed a lift of $\zeta$, and so
\begin{equation}
\tilde f(x+y) = \tilde zeta(2) = \tilde f(x) + \tilde \eta(1) = \tilde f(x) + \tilde f(y).
\end{equation}

To conclude, we have lifted $f \colon \R \to S^1$ to a continuous homomorphism $\tilde f \colon \R \to \R$. By the previous item, we know $\tilde f$ is of the form $\tilde f(t) = tv$ for some $v \in \R$. Hence, $f(t) = \e^{\I t v}$ for some $v \in \R$. However, we also know that $f$ is $2\pi$-periodic, whence (by using $f(2\pi) = f(0)$) we conclude that $v$ is of the form $2\pi k$ for some $k \in \Z$. As such, $f(t) = (\e^{\I t})^k$, and so if we see the domain of $f$ as $S^1$ instead of $\R$ we get $f(z) = z^k$. This completes the proof.
\end{enumerate}
\end{sol}

\begin{ex}
Give an example of a discrete subgroup of $\R^2$ whose projection on $\R$ is not discrete.
\end{ex}

\begin{sol}
Let $H = \braket{(1,0), (\sqrt2, 1)}_\Z$. This is a discrete subgroup, because by using a change of basis it can be seen as the lattice $\Z \times \Z$, which is discrete.

Its projection on the first coordinate is $\Z[\sqrt2]$, which is known to be dense in $\R$ and therefore is not discrete.
\end{sol}

\end{document}