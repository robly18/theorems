\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{mathtools}

\usepackage[thmmarks, amsmath]{ntheorem}

\usepackage{graphicx}

\usepackage{diffcoeff}
\diffdef{}{op-symbol=\mathrm{d},op-order-sep=0mu}

\usepackage{cancel}
\usepackage{interval}

\usepackage{enumitem}

\setlist[enumerate,1]{label=(\roman*)}

\title{Algebra Homework 3}
\author{Duarte Maia}
%\date{}

\theorembodyfont{\upshape}
\theoremseparator{.}
\newtheorem{theorem}{Theorem}
\newtheorem{prop}{Prop}
\renewtheorem*{prop*}{Prop}
\newtheorem{lemma}{Lemma}

\newtheorem{ex}{Exercise}

\theoremstyle{nonumberplain}
\theoremheaderfont{\itshape}
\theorembodyfont{\upshape}
\theoremseparator{:}
\theoremsymbol{\ensuremath{\blacksquare}}
\newtheorem{proof}{Proof}
\newtheorem{sol}{Solution}

\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}

\newcommand{\kk}{\Bbbk}

\newcommand{\PP}{\mathbb{P}}
\newcommand{\FF}{\mathcal{F}}
\newcommand{\DD}{\mathcal{D}}

\newcommand{\I}{\mathrm{i}}
\newcommand{\e}{\mathrm{e}}

\newcommand{\conj}[1]{\overline{#1}}

\DeclareMathOperator{\inte}{int}
\DeclareMathOperator{\codim}{codim}
\newcommand{\grad}{\nabla}

\DeclareMathOperator{\vol}{vol}
\DeclareMathOperator{\Av}{Av}
\DeclareMathOperator{\trace}{tr}


\DeclareMathOperator{\Aff}{Aff}
\newcommand{\GL}{\mathrm{GL}}
\newcommand{\SL}{\mathrm{SL}}
\newcommand{\Hp}{\mathrm{H}}

\newcommand{\HH}{\mathcal{H}}

\let\Im\relax
\DeclareMathOperator{\Im}{Im}
\let\Re\relax
\DeclareMathOperator{\Re}{Re}

\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\norm}{\lvert}{\rvert}
\DeclarePairedDelimiter{\Norm}{\lVert}{\rVert}
\DeclarePairedDelimiter{\braket}{\langle}{\rangle}


\begin{document}
\maketitle

\begin{ex}
Prove that $\bar M$ is finite dimensional as a $\kk$-vector space iff $M$ is finitely generated as an $A$-module.
\end{ex}

\begin{sol}
($\rightarrow$) Suppose that $\bar M$ is finite dimensional, and let $[m_1], \dots, [m_n]$ be a homogeneous basis for it. We claim that the collection $m_1, \dots m_n$ generates $M$ as an $A$-module.

To this effect, we show by induction that all elements of $M_k$ can be written as an $A$-linear combination of the $m_i$. To prove the base case, let $m \in M_0$. Since $[m] \in \bar M$, we may write it as a $\kk$-linear combination of the $[m_i]$, and hence there exist scalars $k_1, \dots, k_n$ such that
\begin{equation}
m - \sum k_i m_i \in A_{>0} M.
\end{equation}

Now, if we remove the elements from the left-hand side whose degree is not zero, or equivalently take the zeroth homogeneous component of the linear combination\footnote{Here we are using the fact that $A_{>0} M$ is a graded subalgebra of $M$.} we obtain
\begin{equation}
m - \sum_{m_i \in M_0} k_i m_i \in M_0 \cap A_{>0} M,
\end{equation}
and the only element of this intersection is $0$. Thus, we conclude that $m$ is a $\kk$-linear combination of the $m_i$, and hence in the $A$-module generated by them (because $\kk \subseteq A$).

We will now do the induction step. Suppose that we have shown that all $m \in M$ of degree $r$ or less may be written as an $A$-linear combination of the $m_i$. We will show that this also holds at degree $r+1$.

Given $m \in M$ homogeneous of degree $r+1$, begin by writing
\begin{equation}
[m] = \sum k_i [m_i] \in \bar M.
\end{equation}

Then, $\Delta = m - \sum k_i m_i \in A_{>0} M$. As in the previous case, we may take the homogeneous component of degree $r+1$, and thus
\begin{equation}
\Delta \in A_{>0} M \cap M_{r+1}.
\end{equation}

As such, we may write $\Delta$ as a sum of the form $\sum a_i \mu_i$, with each $a_i \in A_{>0}$ and each $\mu_i \in M$, with $\deg \mu_i = (r+1) - \deg a_i \leq r$. Now we apply the induction step and write each $\mu_i$ as an $A$-linear combination of the $m_i$, and the proof is complete, for we may write $m = \sum k_i m_i + \sum a_i \mu_i$.

($\leftarrow$) Let $m_1, \dots, m_n$ be a finite set which generates $M$ as an $A$-module. Then, we will show that $[m_1], \dots, [m_n]$ generates $\bar M$ as a vector space.

To this effect, let $[m] \in \bar M$. Then, by hypothesis, $m = \sum a_i m_i$ for some $a_i \in A$. Write each $a_i$ as $k_i + a'_i$, with $k_i \in \kk$ and $a'_i \in A_{>0}$. Then,
\begin{equation}
m = \sum k_i m_i + \sum a'_i m_i,
\end{equation}
hence it is easy to check that $[m] = \sum k_i [m_i]$. By arbitraryness of $m$, we conclude that the $[m_i]$ generate $\bar M$ as a $\kk$-vector space and the proof is complete.
\end{sol}

\begin{ex}
Prove that $\phi \colon B \otimes H \to A$ induced by multiplication is surjective.
\end{ex}

\begin{sol}
We will prove by strong induction on degree that all $a \in A$ are in the image of $\phi$.

Suppose that we have shown that all the image of $\phi$ covers all elements of $A$ of degree strictly less than some fixed $r$, and let $a \in A$ be homogeneous of degree $r$. Since $A = I \oplus H$, write $a = i + h$, with $p \in I$ and $h \in H$. We remark that both $I$ and $H$ are graded ($H$ by hypothesis, and $I$ can be checked directly), so $p$ and $h$ may be chosen homogeneous of degree $r$.

Note that $B$ is a subalgebra of $A$, hence must contain $1_A$, so we may write $h = \phi(1_A \otimes h)$. Thus, it suffices to show that $p$ is in the image of $\phi$.

Since $p \in I$, we may write
\begin{equation}
p = \sum \alpha_i b_i,
\end{equation}
with each $b_i \in B_{>0}$ and $\deg \alpha_i + \deg b_i = \deg p = r$. Now, since $\deg b_i > 0$ we conclude $\deg \alpha_i < r$, whence the strong induction hypothesis applies. As such, all $\alpha_i$ are in the image of $\phi$. Finally, we conclude that all $b_i \alpha_i$ are in the image of $\phi$ (if $\alpha_i = \phi(t)$ then $b_i \alpha_i = \phi(bt)$)\footnote{This requires some routine checking, which is easily performed by reducing to the case where $t$ is a generator $b \otimes h$ and unfurling the definition of $\phi$.} and hence $p$ is too, which concludes the proof.
\end{sol}

\begin{ex}
Show that $A_{>0}$ is generated as an ideal by homogeneous elements $a_1, \dots, a_k$ iff these elements generate $A$ as an algebra.
\end{ex}

\begin{sol}
($\rightarrow$) Induction. Suppose all things of degree less than $\deg a$ with $a \in A$ are in $\braket{a_1, \dots, a_k}_A$ (the subscript means generated as an algebra). Then, write $a = \sum x_i a_i$, with each $x_i \in A$, which is possible because of the hypothesis.\footnote{Actually, is only possible assuming that $\deg a > 0$. The case where $a \in A_0$ uses $A_0 = \kk$. Indeed, $a = k 1_A$ for some $k \in \kk$, and $1_A$ is in the algebra generated by the $a_i$ by definition of `algebra generated by'.} Then, each $x_i$ has degree less than the degree of $a$ by the same argument used in previous exercises, and thus we apply the induction hypothesis to it to show that each $x_i$ is in the subalgebra generated by the $a_i$. This shows that $a$ itself is too, which completes the proof.

($\leftarrow$) Let $a \in A_{>0}$, homogeneous without loss of generality. Since the $a_i$ generate $A$ as an algebra, we may write $a$ as
\begin{equation}
a = \sum_{\abs{\alpha} = \deg a} k_\alpha a^\alpha,
\end{equation}
where we are using multiindex notation from analysis: $\alpha$ ranges over $k$-uples of nonnegative integers, $\abs{\alpha}$ is their sum, $k_\alpha$ is an $\alpha$-indexed scalar, and $a^\alpha$ is shorthand for $a_1^{\alpha_1} \dots a_k^{\alpha_k}$.

Now, since $\deg a \geq 1$, we may from each $\alpha$ pick some positive entry $\alpha_i$ and thus every term of the form $k_\alpha a^\alpha$ is a multiple of $a_i$, and thus in the ideal $\braket{a_1, \dots, a_k}$. As a consequence, $a$ itself is in this idea, which completes the proof.
\end{sol}

\begin{ex}
Prove that $H_V = H'_V$.
\end{ex}

\begin{sol}
As the problem statement says, it is obvious that $H_V \subseteq H'_V$, so we focus on the other inclusion.

Let $f \in H'_V$. We wish to show, given $v \in V$, that $v(f) = 0$. To do so, we show that all coefficients of $v(f)$ are null. Well, if $\alpha$ is a multiindex, the coefficient of $x^\alpha$ in $v(f)$ is given by $\partial^\alpha v(f)(0)$. Well, $\partial^\alpha v \in I_V$ obviously, so $\partial^\alpha v(f)(0) = 0$ because $f \in H'_V$. This completes the proof.
\end{sol}

\begin{ex}
Construct a $G$-invariant inner product $\beta$ on $P_n$ which makes $P^i_n \perp P^j_n$ and $\HH(G)$ the orthogonal complement of $I = P (P^G)_{>0}$.
\end{ex}

\begin{sol}
We begin by considering the obvious isomorphism
\begin{equation}
\begin{aligned}
\phi \colon P_n &\to \DD_n\\
x^\alpha &\mapsto \partial^\alpha,
\end{aligned}
\end{equation}
where $\DD_n$ is the space of differential operators on $\R^n$ and $\alpha$ is an arbitrary multiindex. We saw in class that this is an isomorphism so I'm not bothering to prove it.

Now, set
\begin{equation}
\beta_0(p,q) = \phi(p)(\conj q)(0).
\end{equation}

It is obvious that $\beta_0$ is $\C$-linear in the first entry and conjugate-linear in the second. To show symmetry, we restrict ourselves to the case where $p = x^\alpha$ and $q = x^\sigma$ for multiindices $\alpha$ and $\sigma$, as these generate the whole space. In this case,
\begin{equation}\label{eq:betabasic}
\beta_0(p,q) = \phi(p)(q)(0) = (\partial^\alpha x^\sigma)(0) = \begin{cases}
\alpha! & \text{if $\alpha = \sigma$,}\\
0 & \text{otherwise.}
\end{cases}
\end{equation}

This is evidently symmetric in $\alpha$ and $\sigma$, which shows that $\beta_0$ is hermitian. Note the multiindex notation: $\alpha!$ means $\alpha_1! \dots \alpha_n!$

To show that the product is positive definite, let $p(x) = \sum_\alpha c_\alpha x^\alpha$ be an arbitrary polynomial. Then, by \eqref{eq:betabasic} we have
\begin{equation}
\beta_0(p,p) = \sum_\alpha \abs{c_\alpha}^2 \alpha!,
\end{equation}
which is obviously non-negative, and strictly positive so long as any coefficient is positive.

Now that we have done this, we may now define
\begin{equation}
\beta(p,q) = \frac1{\vol G} \int_G \beta_0(g^* p, g^* q) \dl2 g.
\end{equation}

All properties of a real inner product (bilinearity, hermitian, positive definiteness) are trivially checked for $\beta$ using those same properties of $\beta_0$ and some properties of the integral.

It is also easy to verify that $\beta$ is $G$-invariant, using the property that $\int_G f(gh) \dl2 g = \int_G f(g) \dl2 g$ for any $h \in G$.

\smallskip

Now, we show that under this inner product we have $\HH(G) = I^\perp$. To do this, we take a detour.

First, a fact. It is obvious that the differential operators evaluated at zero form a subset of $P^*$. We claim that they coincide.

To this effect, it suffices to show that all the elementary elements of the dual are differential operator, so let us show that there exists a diff. operator $\lambda$ which makes $\lambda(x^\alpha) = 1$ for some fixed multiindex $\alpha$, and $\lambda(x^\beta) = 0$ for all other multiindices $\beta$. We can actually build it explicitly: it is $\lambda = \frac1{\alpha!} \partial_\alpha$.

Okay, now. We will show a weaker (in principle) statement than we desire: we will show that $\HH(G) \perp I$ (wrt the inner product $\beta$). To this effect, in what follows, let $h \in \HH(G)$. We want to show that for any $f \in I$ we have $\beta(h,f) = 0$.

Now, let's look at the definition of `$h$ is $G$-harmonic'. This means that, if $V$ is the collection of $G$-invariant differential operators, then for all $v \in V$ we have $v(h) = 0$. But by exercise 4, this is the same as: if $u \in I_V$ we have $u(h)(0) = 0$. In other words, any element of $P^*$ which corresponds to a differential operator in $I_V$ will kill $h$.

Now, we consider the following element of $P^*$:
\begin{equation}
\lambda_f(h) = \beta(h,f).
\end{equation}

All we need to show is that whenever $f \in I$ we have $\lambda_f \in I_V$, and we will be done.

Now, if $f \in I$ it is a sum of the form $f = \sum p_i q_i$, with $p_i$ an arbitrary polynomial and $q_i$ a $G$-invariant polynomial with no constant term. Thus, we have
\begin{equation}
\lambda_f = \sum \lambda_{p_i q_i}.
\end{equation}

Now, let us show that if $q$ is $G$-invariant then $\lambda_{q}$ is $G$-invariant. Let $r$ be an arbitrary polynomial; then
\begin{equation}
\lambda_q(g^* r) = \beta(g^* r, q) = \beta(r, (g^{-1})^* q) = \beta(r,q) = \lambda_q(r).
\end{equation}

Thus, $\lambda_q$ is $G$-invariant.

[I'm tired of writing so I'll just say some words which would help solve the exercise if true. I do write some more math later, do read that one please.]

Hopefully, the fact that $\lambda_q$ is $G$-invariant means that it actually corresponds to a $G$-invariant differential operator, and the fact that it has no constant term means that $\lambda_q(1) = 0$ and hence the diff. operator corresponding to $q$ has no constant term. Moreover, hopefully there is some way of showing that $\lambda_{pq}$ as a diff. operator is a multiple of $\lambda_q$; maybe $\lambda_{pq} = \lambda_p \lambda_q$? Anyway, if this happens, we do have that $f \in I$ implies $f \in I_V$, which proves that $\HH(G) \perp I$.

[Back to actual math]

Now that we've `shown' $\HH(G) \perp I$, we finish the argument by dimensional considerations. Indeed, since the inner product $\beta$ makes different degree polynomials orthogonal (can be seen by integrating \eqref{eq:betabasic}) the orthogonal complement of $I$ can be taken separately in each degree. Then, it is a matter of checking dimensions to ensure that $\dim(\HH(G) \cap P_d) + \dim(I \cap P_d) = \dim P_d$, and then linear algebra guarantees that these two spaces are orthogonal complements.

[Actually I don't really want to calculate the dimensions of these things, so yeah, this is all you get. Thanks for reading.]
\end{sol}

\begin{ex}
Show that
\begin{equation}
\frac1{\vol(G)} \int_G \trace(\rho(g)) \dl2 g = \trace(\Av_\rho) = \dim(V^G).
\end{equation}
\end{ex}

\begin{sol}
Pick a basis $v_1, \dots, v_n$ of $V$, and let $\omega^1, \dots, \omega^n$ be its dual basis. Then, we may write the trace of $\rho(g)$ as
\begin{equation}
\trace(\rho(g)) = \sum \omega^i(\rho(g)(v_i)) = \sum \omega^i(g \cdot v_i).
\end{equation}

Thus, we get
\begin{equation}
\frac1{\vol(G)} \int_G \trace(\rho(g)) \dl2 g = 
\sum_i \frac1{\vol(G)} \int_G \omega^i(g \cdot v_i) \dl2 g =
\sum_i \omega^i(\Av_\rho(v_i)) = \trace(\Av_\rho),
\end{equation}
which proves the first equality. (In the middle step we used the fact that the integral commutes with linear transformations.)

To prove the second equality, we use the well known and easily proved fact from algebra: the trace of a projection operator is equal to the dimension of its image. And since the image of $\Av_\rho$ is $V^G$, the exercise is complete.
\end{sol}

\end{document}