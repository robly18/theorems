\documentclass{article}

\usepackage{amsmath,amssymb,amsfonts}
\usepackage{mathtools}
\usepackage{ntheorem}
\usepackage[utf8]{inputenc}
\usepackage{diffcoeff}
\diffdef{}{op-symbol=\mathrm{d},op-order-sep=0mu}
\usepackage[integrals]{wasysym}

\usepackage{cite}

%\usepackage[portuges]{babel}

\title{Project in Calculus of Variations\\
\large Holonomic and Non-Holonomic Constraints, with Lagrange Multipliers}
\author{Duarte Maia}
\date{May 2021}

\theoremseparator{.}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}


\theoremstyle{plain}
\theorembodyfont{\upshape}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}

\theoremstyle{nonumberplain}
\theorembodyfont{\upshape}
\theoremseparator{.}
\newtheorem{statement}{Problem Statement}
\newtheorem{proof}{Proof}

\newcommand{\R}{\mathbb{R}}

\newcommand{\tr}{\intercal}

\newcommand{\tstart}{\mathrm{start}}
\newcommand{\tend}{\mathrm{end}}

\newcommand{\wtphi}{
  \mspace{2mu}
  \widetilde{\mspace{-2mu}\smash[t]{\phi}}
}

\DeclarePairedDelimiter\abs{\lvert}{\rvert}
\DeclarePairedDelimiter\norm{\lVert}{\rVert}
\DeclarePairedDelimiter\eval{.}{\rvert}
\DeclarePairedDelimiter\bracket{[}{]}

\DeclareMathOperator{\tg}{tg}

\newcommand{\vecb}{{\vec{b}}}

\begin{document}
\maketitle

\section{Introduction}

This work originally started as a project for a class on the Calculus of Variations, at Instituto Superior TÃ©cnico. The topic of the project is classical optimization with holonomic and non-holonomic constraints. However, when doing research on the subject, I was shocked to find very few mathematical references with the Lagrange multiplier theorem for the non-holonomic case, and corresponding proof. Consequently, I have taken it upon myself to create such a reference.

This document is divided into two parts. The first part is a relatively simple exploration of holonomic constraints, that is, optimization on (possibly time-dependent) manifolds. In other words, we require that our paths satisfy equations of the form $\phi_i(t, u(t)) = 0$, $i = 1, \dots, k$. The second part is trickier, in which the restrictions become differential equations of the form $\phi_i(t,u(t), \dot u(t))$, $i = 1, \dots k$. This is where I had a problem finding references.

This work is the culmination of a couple of weeks spent interpreting 16 pages' worth of Bliss' \textit{Lectures on the Calculus of Variations} \cite[pp.~187-202]{bliss}. I don't know whether to blame my unfamiliarity with the notational conventions, the great amount of generality in which the proof is done, or my own lack of experience with the topic, but it was with more effort than I am proud to admit that I could finally understand the proof which I reproduce in section \ref{sec:nonholonomic}, albeit in much less generality (though the essential ideas are there). Perhaps my own proof might serve as a primer for someone else's reading of Bliss' proof.

\section{Notational Conventions}

In this section we lay down the basic notational conventions that will be followed over the rest of this document.

The problem at hand is to find a curve $u \colon [t_\tstart, t_\tend] \to \R^n$ that minimizes the value of a certain integral $I(u) = \int_{t_\tstart}^{t_\tend} f(t, u(t), \dot u(t)) \dl2 t$. In order for this expression to make sense, it is assumed that the curves $u$ being considered are at least $C^1$, and the function $f$ is at least continuous, though the results to be obtained require in fact that $f$ is at least $C^2$.

The function $f$ is considered to be a function of 3 variables: time $t \in [t_\tstart, t_\tend]$, position $u \in \R^n$ and velocity $\xi \in \R^n$. In truth, we could be considering $f$ defined in more general (i.e. smaller) domains, but since our results are local in nature correctness is not harmed by omitting consideration of the domain.

We allow ourselves to consider partial derivatives in vector coordinates, for example $\diffp f \xi (t, u, \xi)$. These derivatives are to be taken in the sense of multivariable calculus; for example, the expression we have just written is understood as a $1 \times n$ row-matrix.

Arguments of functions will often be omitted. For example, if $u$ represents a curve, an expression like $f(t,u,\dot u)$ is understood to mean $f(t,u(t),\dot u(t))$: the evaluations at time $t$ are implied. Somewhat more extreme, if $u$ is understood to be the curve in consideration, we might go as far as to write $f$ without any arguments to mean $f(t,u(t),\dot u(t))$. So, for example, we might write $I(u) = \int_\tstart^\tend f$. This applies especially to expressions involving partial derivatives. For example, the standard Euler-Lagrange equations might be written as
\[\diffp f u - \diff*{\diffp f \xi}t = 0.\]

Note that that the total time derivative is evaluated \emph{after} the implicit insertion of variables, unlike the partial derivatives. These conventions are common in physics books (see, e.g. \cite{goldstein}), so we hope that they are understandable to the reader.


\section{Holonomic Constraints}

\subsection{Introduction}

An optimization problem with holonomic constraints is one where the objective is to minimize $I(u)$ as $u$ ranges over (say, $C^1$) paths satisfying the constraint $\phi(t, u) = 0$ for all $t$, where $\phi$ is a (nice enough) function $[t_\tstart, t_\tend] \times \R^n \to \R^k$. In other words, $u$ must obey a set of $k$ cartesian equations. As in the statement for Lagrange multipliers in $\R^n$, it is necessary to assume that these equations are independent, in the sense that the $k$ rows of $\diffp \phi u$ are linearly independent everywhere in the domain of consideration; in other words, that 0 is a regular value of $\phi(t, u)$ as a function of $u$. Therefore, we will henceforth work under the following hypotheses:

\begin{statement}
In this section, we endeavor to minimize the value of the functional $I(u)$ as $u$ ranges over paths starting at $u_\tstart$ and ending at $u_\tend$, such that the $k$ cartesian equations given by
\[\phi(t, u(t)) = 0\]
are satisfied for all $t \in [t_\tstart, t_\tend]$.

It is assumed that $\phi$ is a $C^2$ function.
\end{statement}

\subsection{Euler-Lagrange in Local Coordinates (Time-Independent Case)}

The most basic way to solve an optimization problem with holonomic constraints is to write it out in local coordinates.

The idea is easiest to state when $\phi \equiv \phi(u)$. In this case, the problem is reduced to optimization on a $C^2$-submanifold of $\R^n$. The usual proof of necessity of the Euler-Lagrange equations can be easily modified to work in coordinate charts. We state the result without proof (really, it's almost word-for-word the proof in euclidean space).

\begin{theorem}\label{elinmanifolds}
Let $M$ be an $m$-dimensional $C^2$ manifold embedded in $n$-dimensional euclidean space. Let $f : [t_\tstart, t_\tend] \times \R^n \times \R^n \to \R$ be a $C^2$ function, and $u$ a $C^2$ path. Let $r : \R^m \to \R^n$ be a $C^2$ chart of $M$. Define $\tilde f : [t_\tstart, t_\tend] \times \R^m \times \R^m$ (i.e. `$f$ in coordinates') as
\[ \tilde f(t, q, v) = f(t, r(q), r'(q)v).\]

Furthermore, define $\tilde u$ (`$u$ in coordinates') as
\[\tilde u(t) = u(r^{-1}(t)), \text{ for applicable $t$.}\]

Then, if $u$ minimizes the value of $I(u) = \int f(t,u,\dot u)$, it must satisfy the Euler-Lagrange equations in every coordinate chart, i.e., for all $C^2$ charts,
\[\diffp{\tilde f} q - \diff*{\diffp{\tilde f}v}t = 0.\]
\end{theorem}

\begin{remark}
Even though the above theorem requires that the verification be made for every coordinate chart, it is enough to do the verification in a chart that covers $u$, as the Euler-Lagrange equations are invariant under change of coordinates. In physics parlance, we say that the Euler-Lagrange equations are invariant under point transformations. The proof is a trivial exercise in application of the chain rule.
\end{remark}

\begin{remark}
Theorem \ref{elinmanifolds} also holds for abstract $C^2$ manifolds, not embedded in $\R^n$, if $f$ is defined on $[t_\tstart, t_\tend] \times TM$, where $TM$ is the tangent bundle of $M$.
\end{remark}

\begin{example}
Suppose that we wish to find length-minimizing curves on the sphere of unit radius. Then, we begin by parametrizing the sphere using polar coordinates:
\[r(\theta, \varphi) = (\cos\theta \cos\varphi, \sin\theta \cos\varphi, \sin\varphi),\]
and now we calculate the function $f(t,u,\xi) = \norm\xi$ in local coordinates:
\begin{align*}
\tilde f &= \norm{r'(\theta, \varphi) (\dot \theta, \dot \varphi)}\\
&= \norm*{\begin{bmatrix}
\sin\theta \cos\varphi & \cos\theta \sin\varphi\\
-\cos\theta \cos\varphi & \sin\theta \sin\varphi\\
0 & -\cos\varphi
\end{bmatrix}
\begin{bmatrix}
\dot\theta\\
\dot\varphi
\end{bmatrix}
}\\
&= \sqrt{\cos^2(\varphi) \, \dot\theta^2 + \dot\varphi^2}.
\end{align*}

The Euler-Lagrange equations then become, in local coordinates:
\begin{gather*}
\diffp{\tilde f} \theta - \diff*{\diffp{\tilde f}{\dot \theta}}t = 0 \Leftrightarrow \diff*{\left( \frac{\cos^2(\varphi) \dot \theta}{f} \right)}t = 0,\\
\diffp{\tilde f} \varphi - \diff*{\diffp{\tilde f}{\dot \varphi}}t = 0
\Leftrightarrow
\frac{\cos\varphi \sin\varphi \,\dot \theta^2}{f} = \diff*{\left( \frac{\dot \varphi}{f} \right)}t.
\end{gather*}

To find a solution to these equations, we begin by pointing out that the length of a curve does not depend on the parametrization, as long as it does not double back. Therefore, we may, for simplicity, find a unit-speed parametrization. This simplification is equivalent to requiring that $f = 1$, so the equations become
\[\dot \theta = \frac C {\cos^2(\varphi)}, \quad \ddot \varphi = \cos\varphi \sin\varphi \, \dot \theta^2,\]
and the second equation can be simplified to $\ddot \varphi = C^2 \tg \varphi$.

Finally, to simplify matters, let us suppose without loss of generality (by rotation) that the starting point of our curve lies in the equator ($\varphi = 0$), and its initial velocity is $\dot\theta = 1$, $\dot \varphi = 0$. Then, the Euler-Lagrange equations become easy to solve, as $\ddot \varphi = 0$ and so $\varphi$ is constant equal to 0, and $\dot \theta = C = 1$, and so our curve is just a constant speed rotation about the equator. This (and invariance by change of coordinates) proves that the (unit speed) solutions of the Euler-Lagrange equations are great circle rotations.
\end{example}

Of course, this does not solve the problem of length-minimizing curves in the circle, as we would need to ensure that length-minimizing curves are $C^2$, and solutions of the Euler-Lagrange equations do not necessarily minimize the value of the functional. However, this example has served the purpose of exemplifying the method.

\subsection{Langrange Multipliers}

Um problema de otimizaÃ§Ã£o com restriÃ§Ãµes holonÃ³micas Ã© um problema no qual se pretende minimizar um funcional da forma
\[I(u) = \int_a^b f(t,u(t),u'(t)) \dl2 t,\]
sujeito a restriÃ§Ãµes pontuais. A forma mais elementar Ã© exigir que o caminho esteja contido numa variedade definida por equaÃ§Ãµes cartesianas, isto Ã©, exigir que, para todo o $x \in [a,b]$,
\[g_1(u(t)) = c_1, \dots, g_k(u(t)) = c_k,\]
onde os $g_i$ sÃ£o funÃ§Ãµes regulares o suficiente e os $c_i$ sÃ£o constantes reais. Por exemplo, se quisermos otimizar um funcional sobre os caminhos contidos na esfera, podemos escrever isto como a condiÃ§Ã£o $\norm{u(t)} = 1$.

\begin{theorem}
Seja $u$ um minimizante do funcional $I$ sob as condiÃ§Ãµes de fronteira usuais ($u(a) = \alpha$, $u(b) = \beta$), juntamente com a restriÃ§Ã£o $g(u(x)) = c$, em que $g \colon \R^d \to \R^k$ e $c \in \R^k$ Ã© um valor regular de $g$. ImpÃµem-se as hipÃ³teses de regularidade: $u$ Ã© $C^2$, $f$ Ã© $C^1$ e $g$ Ã© $C^2$ num aberto que contÃ©m o caminho. EntÃ£o, existe uma funÃ§Ã£o contÃ­nua, $\lambda \colon \left]a,b\right[ \to \R^k$, tal que
\[\diffp f {u_i}(t,u(t),u'(t)) - \diff*{\left(\diffp f {\xi_i}(t,u(t),u'(t))\right)}t = \sum_{j=1}^k \lambda_j(t) \diffp {g_j} {u_i} (u(t)), \quad i = 1, \dots, d.\]
\end{theorem}

\begin{proof}
Vamos demonstrar esta afirmaÃ§Ã£o construindo localmente a funÃ§Ã£o $\lambda$. Para cada $t_0 \in \left]a,b\right[$ mostraremos que existe uma funÃ§Ã£o $\lambda$ definida numa vizinhanÃ§a de $t_0$ que satisfaz as restriÃ§Ãµes acima, e fica a partir disto provada a afirmaÃ§Ã£o, visto que o facto de $c$ ser valor regular assume que $\lambda(t)$ Ã© sempre Ãºnico e portanto a funÃ§Ã£o $\lambda$ ``cola bem'' nas vizinhanÃ§as.

Para este efeito, usemos o teorema da funÃ§Ã£o inversa. Como $g'(u(t_0))$ Ã© uma matriz $k \times d$ de caracterÃ­stica $k$, uma das suas submatrizes $k \times k$ Ã© invertÃ­vel. Trocando Ã­ndices, podemos sem perda de generalidade assumir que Ã© a submatriz ``da esquerda'', isto Ã©, podemos supor que a matriz
$
A = \begin{bmatrix}
g'(u(t_0))\\
\begin{matrix}
0 & I
\end{matrix}
\end{bmatrix}
$
Ã© invertÃ­vel. Isto permite-nos aplicar o teorema da funÃ§Ã£o inversa Ã  seguinte funÃ§Ã£o. Decomponha-se $u = (v, w)$, em que $v$ sÃ£o as primeiras $k$ componentes de $u$ e $w$ as Ãºltimas $d-k$. Defina-se agora $G(v,w) = (g(v,w),w)$. EntÃ£o, claramente temos $G'(u(t_0)) = A$, que Ã© invertÃ­vel, pelo que existem abertos $U$ e $V$ em $\R^d$ tal que:
\begin{itemize}
\item $u(t_0) \in U$,
\item $G(u(t_0)) \in V$, e
\item $G \colon U \to V$ Ã© um difeomorfismo $C^2$.
\end{itemize}

Visto que $G(u(t_0)) \in V$, existe uma bola, digamos de raio $r$, em torno de $G(u(t_0))$ que estÃ¡ contida em $V$. Considere-se um intervalo $\left]t_0-\delta, t_0 + \delta\right[$ tal que $G(u)$ nÃ£o difere de mais de $r/2$ de $G(u(t_0))$. Seja $v$ uma funÃ§Ã£o de teste de suporte contido neste intervalo. EntÃ£o, para $\varepsilon$ pequeno o suficiente temos que $G(u) + \varepsilon v$ estÃ¡ contido em $V$ para $t \in \left]t_0-\delta,t_0+\delta\right[$. Consequentemente, Ã© possÃ­vel definir $u_\varepsilon$ de modo a concordar com $u$ fora deste intervalo, e como $G^{-1}(G(u) + \varepsilon v)$ dentro dele.

Podemos agora fazer o argumento usual para obter as equaÃ§Ãµes de Euler-Lagrange. Como sabemos que $u$ minimiza o funcional de entre os caminhos que satisfazem $g(u) = c$, sabemos que, se $v$ tiver as primeiras $k$ componentes nulas, $I(u_\varepsilon) \geq I(u)$. Assim sendo, definindo $\Phi(\varepsilon) = I(u_\varepsilon)$, obtemos $\Phi'(0) = 0$, e esta derivada pode ser calculada usando a regra de Leibniz, porque, pelas nossas hipÃ³teses de regularidade, a funÃ§Ã£o $f(t, u_\varepsilon(t), u'_\varepsilon(t))$ Ã© $C^1$ em $\varepsilon$ e $t$. A verificaÃ§Ã£o Ã© tediosa, mas resume-se a dividir no caso em que $t$ nÃ£o estÃ¡ no suporte de $v$ (neste caso a afirmaÃ§Ã£o Ã© trivial) e no caso em que $t$ estÃ¡ no suporte de $v$ (neste caso podemos usar a expressÃ£o $u_\varepsilon = G^{-1}(G(u) + \varepsilon v)$ numa vizinhanÃ§a).

Calcule-se entÃ£o $\Phi'(0)$. Primeiro que tudo, aplicamos Leibniz para obter
\[\Phi'(0) = \int_a^b \diffp*{f(t, u_\varepsilon(t), u'_\varepsilon(t))}\varepsilon \dl2 t.\]

De seguida, podemos usar o facto que para $t \not \in \left]t_0-\delta,t_0+\delta\right[$ temos $u_\varepsilon = u$, para reduzir o integral a
\[\int_{t_0 - \delta}^{t_0 + \delta} \diffp*{f(t, u_\varepsilon(t), u'_\varepsilon(t))}\varepsilon \dl2 t,\]
e neste intervalo temos o benefÃ­cio que $u_\varepsilon$ tem uma expressÃ£o simples. Nomeadamente, $u_\varepsilon(t) = G^{-1}(G(u(t)) + \varepsilon v(t))$. A expressÃ£o para $u'_\varepsilon(t)$ Ã© ligeiramente mais complexa.
\[u'_\varepsilon(t) = (G^{-1})'(G(u(t)) + \varepsilon v(t)) [G'(u(t)) u'(t) + \varepsilon v'(t)].\]

Aplicando a derivada da composta, concluimos
\[\Phi'(0) = \int_{t_0 - \delta}^{t_0 + \delta} \diffp fu (t, u(t), u'(t)) \diffp {u_\varepsilon(t)} \varepsilon(0) + \diffp f \xi (t, u(t), u'(t)) \diffp{u_\varepsilon}{\varepsilon, t}(0,t)\dl2 t,\]
e uma aplicaÃ§Ã£o de integraÃ§Ã£o por partes no segundo termo resulta em
\[\Phi'(0) = \int_{t_0 - \delta}^{t_0 + \delta} \left( \diffp fu (t, u(t), u'(t))  - \diff*{\diffp f \xi (t, u(t), u'(t))}t \right) \diffp {u_\varepsilon(t)} \varepsilon(0)\dl2 t.\]

O termo $\diffp {u_\varepsilon(t)} \varepsilon(0)$ toma nesta prova o lugar do que normalmente chamamos $v$, sendo que normalmente aqui aplicarÃ­amos o lema fundamental do cÃ¡lculo de variaÃ§Ãµes para concluir Euler-Lagrange. No entanto, os $u_\varepsilon$ estÃ£o limitados Ã  nossa variedade, pelo que podemos apenas concluir que o termo nos parÃªnteses pertence ao espaÃ§o normal Ã  variedade. Isto faz sentido com o que esperamos, visto que esse espaÃ§o Ã© gerado pelos gradientes dos $g_i$.

Para prosseguir, escreva-se melhor o termo $\diffp {u_\varepsilon(t)} \varepsilon(0)$. Este Ã© dado por
\[\diffp {u_\varepsilon(t)} \varepsilon(0) = (G^{-1})'(G(u(t))) v(t) = (G'(u(t)))^{-1} v(t).\]

Seja $A(t) = G'(u(t))^{-1}$, $a_{ij}(t)$ o elemento $i,j$ de $A(t)$, e defina-se a funÃ§Ã£o $\lambda \colon \left]t-\delta,t+\delta\right[ \to \R^d$ como
\[\lambda_i(t) = \sum_j \left( \diffp f{u_j} (t, u(t), u'(t))  - \diff*{\diffp f {\xi_j} (t, u(t), u'(t))}t \right) a_{ji}(t). \]

EntÃ£o, a equaÃ§Ã£o acima diz-nos que, pelo TFCV, $\lambda_i = 0$ para $i > k$. Para mais, Ã© fÃ¡cil ver por definiÃ§Ã£o que
\[\lambda(t)^\tr = \left( \diffp fu (t, u(t), u'(t))  - \diff*{\diffp f \xi (t, u(t), u'(t))}t \right) G'(u(t))^{-1},\]
pelo que $G'(u(t))^\tr \lambda(t) = \left( \diffp fu (t, u(t), u'(t))  - \diff*{\diffp f \xi (t, u(t), u'(t))}t \right)^\tr$.

Visto que apenas as primeiras $k$ coordenadas de $\lambda$ sÃ£o nÃ£o-nulas, e que as primeiras $k$ coordenadas de $G$ coincidem com $g$, esta expressÃ£o pode ser escrita como (abusando da notaÃ§Ã£o e usando agora $\lambda(t)$ para representar o vetor das primeiras $k$ coordenadas de $\lambda$):
\[
\begin{bmatrix}
\nabla g_1(u(t)) & \dots & \nabla g_k(u(t))
\end{bmatrix}
\lambda(t)
=
\left( \diffp fu (t, u(t), u'(t))  - \diff*{\diffp f \xi (t, u(t), u'(t))}t \right)^\tr.
\]

Consequentemente, a funÃ§Ã£o $\lambda(t)$ (com apenas $k$ componentes) Ã© a funÃ§Ã£o que se procura, e a prova estÃ¡ concluÃ­da.
\end{proof}

\section{Non-Holonomic Constraints}\label{sec:nonholonomic}

\subsection{Introduction}

Definition: a nonholonomic constraint is of the form
\[\phi(t,u(t),\dot u(t)) = 0.\]

We wish to minimize $I(u) = \int f(t,u,\dot u) \,\dl x$, for $u$ satisfying these constraints and with $u(t_\tstart) = u_\tstart$ and $u(t_\tend) = u_\tend$, for $u_\tstart$ and $u_\tend$ fixed elements of $\R^n$.

Note that $\phi$ is given by a number (let's say $k$) of differential equations. (Remember to show examples.) It is natural (and will be necessary) to require that these equations are independent. In other words, if $\phi \equiv \phi(t,u,\xi)$, we require that the matrix $\diffp\phi\xi$ has linearly independent lines, or, equivalently, rank $k$.

\subsection{Valid Displacements}

The local strategies used in the holonomic case fail here, so we will need to use a kind of global version of the inverse function theorem. The proof will be presented in the annex, so until then here is theorem.

\begin{theorem}
Let $u$ be a $C^k$ path $[t_\tstart,t_\tend] \to \R^n$, $\Omega$ an open subset of $X = [t_\tstart,t_\tend] \times \R^n \times \R^n$ such that $(t,u(t),\dot u(t)) \in \Omega$ for all $t$, and $\phi \equiv \phi(t,u,\xi)$ a $C^k$ function $\phi \colon \Omega \to \R^k$.

Suppose that for all $t$ we have that the matrix $\diffp\phi\xi$ has rank $k$. Then there exist open subsets of $X$, say $U$ and $V$, such that $U$ contains all points of the form $(t,u(t),\dot u(t))$ and is contained in $\Omega$, as well as a function $\wtphi \colon U \to \R^n$ whose first $k$ coordinates agree with $\phi$, such that the function given by
\begin{align*}
\Phi \colon U &\to V\\
(t,u,\xi) &\mapsto (t,u,\wtphi(t,u,\xi))
\end{align*}
is a $C^k$ diffeomorphism.
\end{theorem}

Consider a fixed path $u_0$, and apply the theorem to this case. Let the inverse of the above $\Phi$ be $(t,u,\eta) \mapsto (t,u,\psi(t,u,\eta))$.

We can build valid displacements using the following recipe. Let $v \colon [t_\tstart, t_\tend] \to \R^{n-k}$, and consider the ODE given by $\wtphi(x,u,\dot u) = (0, b v)$, where $b$ is a real number, with $u(t_\tstart) = u_\tstart$. More concretely:
\[
\begin{cases}
\dot u(t) = \psi(t, u(t), (0, b v(t))),\\
u(t_\tstart) = u_\tstart.
\end{cases}
\]

This is an ODE with a parameter $b$. Let us call the solution (if it exists) $u_b^v(t)$. Note that the solution exists for $b = 0$: it is simply $u_0$. Classical ODE theorems guarantee that if the solution exists for some set of parameters (in this case, $b = 0$), then it exists for small enough perturbations of the parameters, and is continuous. Moreover, it can be shown to absorb the degrees of regularity of the ODE. In this case, since $\psi$ is $C^k$, so is $u_b^v(t)$ (in the two variables, $b$ and $v$).

In the next section we will require the quantity $\diffp{u_b^v(t)}b$, which exists for $b$ close enough to 0. Later on, we will require a slight variation, in which we allow for several $v$'s and several $b$'s. More concretely, we would solve the ODE given by
\[\wtphi(t,u,\dot u) = (0, b_1 v_1 + \dots + b_p v_p),\]
where the values of $b_i$ will be considered to be part of a single vector $b$ (yes, it has the same name as when it was just a real number), and the vector-functions $v_1$, $v_2$, and so on are considered to be the columns of a single $n \times p$ matrix $V(t)$. In this case, the ODE becomes
\[\wtphi(t,u,\dot u) = (0, V b).\]

The same conclusions apply: the resulting solution $u^V_b(t)$ exists for small enough $b$, and is $C^k$ in the variables $b$ and $t$.

Note that in constructing these families \emph{we have no guarantee that the end-point $u_b^V(t_\tend)$ equals $u_\tend$}! This is part of what makes this study so difficult.

\subsection{First Variation}

Consider a one-parameter family of arcs of the form $u_b^v(t)$, as described above. In what follows we will abbreviate this to $u_b(t)$. We will investigate the behavior of $I(u_b)$ around zero. More precisely, we will look at $\diff{I(u_b)}b[0]$.

Recall that $I(u_b)$ is defined as $\int f(t, u_b(t), \dot u_b(t)) \dl1 t$, and since this is integration over a compact interval of a $C^1$ function in both its variables (assuming $f$ is $C^1$ and $u_b(t)$ is $C^2$ in both variables) Leibniz' rule is applicable and so
\[\diff{I(u_b)}b = \int_\tstart^\tend \diffp*{f(t, u_b, \dot u_b)}b \dl1 t =\int_\tstart^\tend \diffp f u \diffp{u_b}b + \diffp f \xi \diffp{\dot u_b}b \dl1 t.\]

The usual integration by parts of the calculus of variations can now be performed, owing to the fact that, since $u$ is $C^1$ in $b$ and $t$, the order of derivatives is irrelevant, so that $\diffp{\dot u_b}b = \diffp{u_b(t)}{t,b}$. Therefore,
\[\diff I b = \int_\tstart^\tend \left( \diffp f u - \diff*{\diffp f \xi}t \right) \diffp{u_b}b \dl1 t + \bracket*{ \diffp f \xi \diffp{u_b}b }_\tstart^\tend.\]

Finally, note that we are keeping the first end of $u$ fixed, so that $u_b(t_\tstart)$ is constant. Therefore, the bracket term can be simplified, yielding
\[\diff I b = \int_\tstart^\tend \left( \diffp f u - \diff*{\diffp f \xi}t \right) \diffp{u_b}b \dl1 t + \eval*{\diffp f \xi}_\tend \eval*{\diffp{u_b}b}_\tend.\]

Reminder to introduce $\Theta$-notation. With it, this can be written
\[\diff I b = \int_\tstart^\tend \Theta(f) \diffp{u_b}b \dl1 t + \eval*{\diffp f \xi}_\tend \eval*{\diffp{u_b}b}_\tend.\]

This expression can still be simplified, if we recall that $u$ satisfies $\wtphi(t,u(t), \dot u(t)) = (0, b v(t))$. Taking (time-variable) linear combinations of this, we conclude that, if $\mu(t) = (\mu_1(t), \mu_2(t))$ is regular enough,
\[\mu \cdot \wtphi(t, u, \dot u) = b \mu_2 \cdot v,\]
and so, differentiating in $b$, we get
\[\diffp{(\mu \cdot \wtphi)}u \diffp u b + \diffp{(\mu \cdot \wtphi)}\xi \diffp{\dot u}b = \mu_2 \cdot v,\]
which can be rewritten in terms of $\Theta$ as
\[\Theta(\mu \cdot \wtphi) \diffp u b = \mu_2 \cdot v - \diff*{\left(\diffp{(\mu \cdot \wtphi)}\xi \diffp u b \right)}t.\]

Integrating, this yields
\[\int_\tstart^\tend \Theta(\mu \cdot \wtphi) \diffp u b \dl1 t = \int_\tstart^\tend \mu_2 \cdot v \dl2 t - \bracket*{\diffp{(\mu \cdot \wtphi)}\xi \diffp u b}_\tstart^\tend.\]

Using the same trick as before, the bracket can be simplified to get
\[\int_\tstart^\tend \Theta(\mu \cdot \wtphi) \diffp u b \dl1 t = \int_\tstart^\tend \mu_2 \cdot v \dl2 t - \mu(t_\tend) \cdot \eval*{\diffp{\wtphi}\xi}_\tend \eval*{\diffp u b}_\tend.\]

Adding and subtracting this expression to $\diff I b$ deduced above, we get
\begin{multline*}
\diff I b = \int_\tstart^\tend \Theta(f + \mu \cdot \wtphi) \diffp{u_b}b \dl1 t - \int_\tstart^\tend \mu_2 \cdot v \dl2 t\\
+ \left(\eval*{\diffp{f} \xi}_\tend + \mu(t_\tend) \eval*{\diffp\wtphi\xi}_\tend \right) \eval*{\diffp{u_b}b}_\tend.
\end{multline*}

Note that $\mu$ has still been left arbitrary, as long as $\Theta(\mu \cdot \wtphi)$ makes sense. Therefore, we have freedom to manipulate it. This will be useful in the sequence.

\subsection{The Multiplier Rule}

Suppose, now, that $u_0$ is a minimum of the functional $I$ over the class of paths such that $u(t_\tstart) = u_\tstart$, $u(t_\tend) = u_\tend$ and $\phi(t,u,\dot u) = 0$. We might be tempted to apply the above formula for $\diff I b$ and claim that this quantity must be null for all $v$. However, that is neglecting the requirement that $u(t_\tend) = u_\tend$. One could think about restricting themselves to only $v$'s that guarantee this requirement, in some way, but that might not be possible.

To get around this, consider an arbitrary collection $v_1, \dots, v_{n+1}$ of smooth functions $[t_\tstart, t_\tend] \to \R^k$, which we collect into the matrix $V(t)$. We can see $I$ as function in $n+1$-variables, given by the vector $\vecb$. Furthermore, we can see $u(t_\tend)$ as a function of $\vecb$ as well, and so we may consider the $C^1$ function given by
\[\vecb \mapsto (I(u_\vecb^V), u_\vecb(t_\tend)).\]

By the inverse function theorem, if the jacobian of this function at $\vecb = 0$ were invertible, this map would be a diffeomorphism in a neighborhood of the origin. But this contradicts the assumption that $u_0$ is a minimum of $I$, for in this case we could find $\vecb$ such that $u_\vecb(t_\tend) = u_\tend$, and $I(u_\vecb^V)$ is slightly smaller than $I(u_0)$. In other words, under the assumption that $u$ is a minimum of $I$, the following matrix has linearly dependent rows: (evaluated at $\vecb = 0$)
\[
\begin{bmatrix}
\diffp*{I(u_\vecb^V)}{b_1} & \dots & \diffp*{I(u_\vecb^V)}{b_{n+1}}\\
\diffp*{u_\vecb^V(t_\tend)}{b_1} & \dots &\diffp*{u_\vecb^V(t_\tend)}{b_{n+1}}
\end{bmatrix}.
\]

Note that, at $\vecb = 0$, these columns can be simplified:
\[
\begin{bmatrix}
\diff*{I(u_b^{v_1})}{b} & \dots & \diff*{I(u_b^{v_{n+1}})}{b}\\
\diff*{u_b^{v_1}(t_\tend)}{b} & \dots & \diff*{u_b^{v_{n+1}}(t_\tend)}{b}
\end{bmatrix}.
\]

Since this holds for arbitrary collections $v_1, \dots, v_{n+1}$, we can actually conclude a very strong result. Indeed, let $q$ be the largest number of $v_1, \dots, v_q$ you can have such that the following matrix has linearly independent columns
\[
A =
\begin{bmatrix}
\diff*{I(u_b^{v_1})}{b} & \dots & \diff*{I(u_b^{v_{q}})}{b}\\
\diff*{u_b^{v_1}(t_\tend)}{b} & \dots & \diff*{u_b^{v_{q}}(t_\tend)}{b}
\end{bmatrix}.
\]

By the above remark, $q$ must be less than $n+1$. Therefore, for any $v$, the vector $(\diff*{I(u_b^v)}{b}, \diff*{u_b^v(t_\tend)}{b})$ is a linear combination of the columns of $A$.

Since the matrix $A$ is strictly taller than it is wide, there exists a row-matrix, say $E = \begin{bmatrix} \ell_0 & e \end{bmatrix} = \begin{bmatrix} \ell_0 & e_1 & \dots & e_n\end{bmatrix}$ such that $e A = 0$. By the previous paragraph, we conclude that \emph{for this specific collection of scalars}(!), for any $v$ we have
\[\ell_0 \diff*{I(u_b^v)}{b} + e \diff*{u_b^v(t_\tend)}{b} = 0.\]

Now, by the result of the previous subsection (need to modify), we may rewrite the derivative in $I$ as
\begin{multline*}
\ell_0 \diff I b = \int_\tstart^\tend \Theta(\ell_0 f + \mu \cdot \wtphi) \diffp{u_b}b \dl1 t - \int_\tstart^\tend \mu_2 \cdot v \dl2 t\\
+ \left(\ell_0 \eval*{\diffp{f} \xi}_\tend + \mu(t_\tend) \eval*{\diffp\wtphi\xi}_\tend \right) \eval*{\diffp{u_b}b}_\tend,
\end{multline*}
where, recall, $\mu$ has been left as an arbitrary regular function. If we expand the previous equation, this gives the result
\begin{multline*}
\int_\tstart^\tend \Theta(\ell_0 f + \mu \cdot \wtphi) \diffp{u_b}b \dl1 t - \int_\tstart^\tend \mu_2 \cdot v \dl2 t\\
+ \left(\ell_0 \eval*{\diffp{f} \xi}_\tend + \mu(t_\tend) \eval*{\diffp\wtphi\xi}_\tend + e\right) \eval*{\diffp{u_b}b}_\tend = 0, \text{ for all regular $v$.}
\end{multline*}

We may now rejoice in the degrees of freedom we have left, for we can, for instance, pick $\mu(t_\tend)$ in order to make
\[\ell_0 \eval*{\diffp{f} \xi}_\tend + \mu(t_\tend) \eval*{\diffp\wtphi\xi}_\tend + e = 0.\]

Indeed, note that this choice is independent of $v$. Also independent of $v$ would be to ensure that $\Theta(\ell_0 f + \mu \cdot \wtphi)$. This is an ODE, and existence of solution is guaranteed by classical ODE theory. If we expand we get a linear ODE with varying coefficients. Remember to cite Perko or so.

Then, for this particular $\mu$, we reach the startling conclusion
\[\int_\tstart^\tend \mu_2 \cdot v \dl2 t = 0 \text{ for all regular $v$.}\]

We may now apply the FLCV to get that the last $k$ coordinates of $\mu$ were null after all, and so
\[\Theta(\ell_0 f + \mu_1 \cdot \phi) = 0\]

And so we are done, because we get a result that did not require arbitrary choices. Our conclusion is as follows:

\begin{theorem}
If $u_0$ is a sufficiently regular minimum of the optimization problem, then there exist a constant $\ell_0$ and $k$ $C^1$ functions, say $\lambda_1, \dots, \lambda_k$, such that
\begin{equation}\label{multipliers}
\ell_0 \diffp f u + \lambda \cdot \diffp \phi u = \diff*{\left(\ell_0 \diffp f \xi + \lambda \cdot \diffp \phi \xi\right)}t.
\end{equation}
\end{theorem}

\begin{remark}
Note that this is, in principle, a well-determined first-order ODE. We have $n+k$ functions to solve:
\[u_1, \dots, u_n, \lambda_1, \dots, \lambda_k,\]
and $k+n$ equations: $k$ are given by the condition $\phi(t,u,\dot u) = 0$, and $n$ are given by \eqref{multipliers}.
\end{remark}

\subsection{Normality and Abnormality}

The solutions to the Lagrange multiplier equation can be divided into two types: normal solutions, wherein $\ell_0 \neq 0$, and abnormal solutions, where $\ell_0 = 0$.

\bibliographystyle{plain}
\bibliography{bibliography}

\end{document}