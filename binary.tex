\documentclass[11pt]{amsart}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}

\usepackage{showlabels}

\usepackage{lipsum}

\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\HH}{\mathcal{H}}
\newcommand{\BB}{\mathcal{B}}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{prop}{Prop}

\newtheorem{definition}{Definition}

\newtheorem*{remark}{Remark}

\title[\textbf{Optimal bounds for $b$-ary measures}]{\textbf{On optimal bounds for comparing Binary (and $b$-ary) Nest Measures with the Hausdorff Measure on $\R$}}


\author{Duarte Maia}
\address{Department of Mathematics, Instituto Superior Técnico}
\email{duarte.nascimento@tecnico.ulisboa.pt}

\author{Jorge Drumond Silva}
\address{Department of Mathematics, Instituto Superior Técnico}
\email{jsilva@math.ist.utl.pt}

\date{}


\begin{document}


\begin{abstract}
\lipsum[1]
\end{abstract}

\maketitle


\section{Introduction}

\lipsum[1]

\section{Hausdorff and Binary measures: a review}

\subsection{Hausdorff}

Let $X$ be a metric space, with distance $d$. We define the \emph{$s$-dimensional Hausdorff measure on $X$} as follows:

First, define, for $E \subseteq X$, $s \in \R^+_0$ and $\delta > 0$, $\HH_s^\delta(E) := \inf \sum_i \lvert U_i \rvert^s$, where:

\begin{itemize}

\item $\lvert E \rvert$ denotes the \emph{diameter of $E$}, defined by the $\sup$ of distances between elements of $E$;\footnote{We follow the convention that the diameter of the empty set be 0.}

\item The infimum is taken over countable coverings $\{U_i\}_{i \in \N}$ of $E$ such that the diameter of all $U_i$ is less than $\delta$. This mouthful is usually abbreviated to \emph{$\delta$-coverings}.

\end{itemize}

It is easy to see that, as $\delta$ decreases, this quantity increases. Hence, the limit $\lim_{\delta \to 0} \HH_s^\delta(E)$ exists in $\left[ 0, \infty \right]$. It is this quantity that we call the \emph{$s$-dimensional Hausdorff measure of $E$}:

\begin{definition}(Hausdorff Measure)

 \[\HH_s(E) := \lim_{\delta \to 0} \HH_s^\delta(E)\]
\end{definition}

\subsection{Dimension}

Fixed any set $E$ in a metric space $X$, an interesting phenomenon occurs: there exists a dimension $\sigma \in \left[0, \infty\right]$ such that:

\begin{itemize}
\item For $s < \sigma$, $\HH_s(E) = \infty$;

\item For $s > \sigma$, $\HH_s(E) = 0$;
\end{itemize}

To see that this is so, the following lemma is key:

\begin{lemma}\label{helperdimension}
Let $t < s$ be real nonnegative numbers. Then, if $\HH_t(E) < \infty$ we have $\HH_s(E) = 0$.
\end{lemma}

\begin{proof}
Suppose $t < s$ and $\HH_t(E) < \infty$. Put $M = \HH_t(E) + 1$. Then, for any $\delta > 0$ we may find a $\delta$-covering $\{U_i\}$ of $E$ such that $\sum \lvert U_i \rvert^t < M$. Hence, 

\begin{align*}
\sum \lvert U_i \rvert^s &= \sum \lvert U_i \rvert^t \lvert U_i \rvert^{s - t}\\
&\leq \sum \lvert U_i \rvert^t \delta^{s-t}\\
&\leq \delta^{s-t} M
\end{align*}

Therefore, $\HH_s^\delta(E) \leq \delta^{s-t} M$, whence making $\delta \to 0$ we get $\HH_s(E) = 0$, as desired.
\end{proof}

With this in mind, define, for a set $E$, its dimension

\begin{definition} (Hausdorff dimension of a set)

\[\sigma_E := \inf \{\, s \mid \HH_s(E) < \infty \,\}\]
\end{definition}

We can now show:

\begin{lemma}
(Well-definedness of the dimension of a set) Let $E \subseteq X$, where $X$ is a metric space.

\begin{itemize}
\item For $s < \sigma_E$, $\HH_s(E) = \infty$;

\item For $s > \sigma_E$, $\HH_s(E) = 0$;
\end{itemize}

\end{lemma}

\begin{proof}
The first inequality is trivial by definition of $\sigma_E$.

As for the second inequality, notice that if $s > \sigma_E$ then there exists $t < s$ such that $\HH_t(E) < \infty$. By lemma \ref{helperdimension}, we have $\HH_s(E) = 0$, as desired.
\end{proof}

This shows that the dimension of a set is the only dimension for which its Hausdorff measure may be nontrivial, that is, different from $0$ or $\infty$.

\begin{definition}
We will say $E \subseteq X$ is an \emph{$s$-set} if $\HH_s(E) \in \left]0, \infty \right[$.

It is obvious that an $s$-set has dimension $s$.
\end{definition}

\begin{remark}
Not every $s$-dimensional set is an $s$-set.
\end{remark}

\subsection{Net measures}

Hausdorff measure, and by extension Hausdorff dimension, is sometimes rather unwieldy to calculate, seeing as its definition requires one to look at all possible $\delta$-coverings. As such, there is interest in simplifying this process.

We will restrict our attention to $\R^n$, and later on to $\R$, as unfortunately a lot of what follows is not generalizable to other contexts.

First, notice that it is not without precedent for one to restrict their attention to specific coverings. Indeed, the reader may easily show that, if in the definition of $\HH_s^\delta$ one takes the infimum over \emph{open} $\delta$-coverings instead of arbitrary ones, the value of $\HH_s^\delta$ may change, but the limit as $\delta \to 0$ remains the same. Hence, one could consider the Hausdorff measure defined using open $\delta$-coverings with no loss of generality.

The notion of net measure arises from the idea of redefining the Hausdorff measure with a more manageable collection of sets to use for coverings. In the context of $\R^n$, Besicovitch (1952) used the so-called binary intervals, construction which we will make for the case of $n = 1$, but the reader can satisfy any curiosity they might have regarding the general case in \cite{falconer} \cite{rogers}.

\begin{definition}
Define the so-called \emph{binary intervals} in $\R$ as the set of intervals of the form $\left[ k 2^j, (k+1) 2^j \right[$, for $k, j \in \Z$.

The binary measure is defined analogously to the Hausdorff measure, except instead of arbitrary coverings one restricts themselves to $\delta$-coverings by binary intervals. It is denoted by $\BB_s(E) = \lim_{\delta \to 0} \BB_s^\delta(E)$.
\end{definition}

The usefulness of this definition lies in that, even though these measures do \emph{not} coincide in general with the Hausdorff measure, one can find bounds for one given the other.

The following theorem is found in \cite{falconer} \cite{rogers} and is left here without proof, though in the sequence of this document we will prove stronger results for the particular case of $n = 1$.

\begin{theorem}\label{badbound}
Consider the Hausdorff and Binary measures defined in $\R^n$. The following inequalities hold for any set $E$:

\[ \HH_s(E) \leq \BB_s(E) \leq 3^n 2^{n^2} \HH_s(E) \]
\end{theorem}

\begin{remark}
Even though the preceding theorem will not be proven, it should be noticed that one of the inequalities is trivial.

Indeed, for any $\delta$ we have $\HH_s^\delta(E) \leq \BB_s^\delta(E)$, since both are infima taken over coverings of $E$, but the set of coverings `accepted' by the Hausdorff measure is a superset of those considered in the binary measure. Making $\delta \to 0$, we have inequality $\HH_s(E) \leq \BB_s(E)$.
\end{remark}

The preceding theorem has the following consequence: defining the ``binary dimension'' of a set analogously to how we defined the Hausdorff dimension, these two concepts would coincide. To see why, simply notice that the Hausdorff measure of a set is zero iff the binary measure is zero, and same for infinity.

Hence, if we are interested in the Hausdorff dimension of a set, we may begin by investigating its binary measure.

Notice, however, that the provided bound is, in simple terms, \emph{awful}. Indeed, for $n = 1$ the obtained constant amounts to 6, but we will see below that it can be significantly lowered without much effort. (And then lowered more, with significantly more effort.)

Of course, this is of no consequence for dimensional considerations, which is where (as far as the author is aware) their main utility is found. However, the results that follow allow for much tighter bounds, and hopefully better understanding of how these two relate.

\subsection{Hausdorff Coverings}

As mentioned above, it is sometimes convenient, yet entirely admissible, for one to only consider open coverings of sets instead of allowing for the full generality of any possible covering. This is not the only simplification that will be useful for us to do.

First, notice that any monotone operation that can be done on a set without increasing its diameter can be assumed to be done without loss of generality.

To use an example: notice that taking the closure of a set $U$ does not increase its diameter. As such, given any $\delta$-covering $\{U_i\}$ of a set $E$, the collection $\{ \overline{U_i} \}$ is also a $\delta$-covering of $E$, with all sets closed, and with all diameters the same, whence the Hausdorff sum is also the same. This shows, then, that we can assume without loss of generality that our infimum in the definition of $\HH_s^\delta$ can be taken over the closed sets.

A more specific example, yet useful for our purposes, is (in $\R^n$) the operation of taking the convex hull, which, as the reader may notice, satisfies the condition of monotony and not increasing diameters. This shows, then, that we may take infima over convex sets with no loss of generality.

In the particular case of $\R$, what this shows is that we can assume the sets we are using for coverings are intervals. Sometimes it will be useful for us to assume these intervals closed, sometimes it's useful to suppose them open. In any case, the reader should be aware that these hypotheses harm in no way the generality of the results obtained.

As a final remark, it is often useful to exclude, in our coverings, cases where the sets have diameter 0. Indeed, many authors explicitly exclude the case of 0-diameter sets in the definition of $\delta$-coverings. It is, however, a simple exercise that it doesn't matter whether you allow these sets or not.

\section{Better elementary bounds}

We have already shown, as a remark, that, in $\R$, $\HH_s \leq \BB_s$. We will now proceed to establish a bound in the opposite direction, beginning our journey to find optimal bounds.

Everything that follows will be done in the context of $\R$.

\begin{theorem}
\[ \BB_s \leq 3 \HH_s \]
\end{theorem}

\begin{proof}
To show this fact, we will show that for any $E$ and $\delta$ we have the inequality $\BB_s^\delta(E) \leq 3 \HH_s^\delta(E)$. To this effect, we will show that given any $\delta$-covering of $E$ we can construct a binary $\delta$-covering of $E$ such that its `Hausdorff sum' is not much bigger than the original's.

Fix, then, a $\delta$-covering $\{U_i\}$ of $E$, which we suppose without loss of generality to be composed of intervals with diameter strictly greater than zero. Define, for each $i$, $j_i$ to be the greatest integer such that $2^{j_i} < \lvert U_i \rvert$, and hence $2^{j_i} < \delta$ for all $i$.

Notice that at least one interval of the form $\left[ k_i 2^{j_i}, (k_i + 1) 2^{j_i} \right[$ is wholly contained within $U_i$. Furthermore, this interval, united with the intervals of its size immediately to its left and immediately to its right, cover $U_i$ entirely.

Now, consider the covering $\{I_i\}$ obtained from $\{U_i\}$ by replacing each $U_i$ by the three intervals mentioned above. We now have a binary $\delta$-covering of $E$, with Hausdorff sum:

\[\sum \lvert I_i \rvert^s \leq \sum 3 \lvert U_i \rvert^s\]

This inequality because for every $U_i$ we assign its three corresponding intervals, each with diameter less than $\lvert U_i \rvert$. This completes our proof, since:

\begin{align*}
3 \HH_s^\delta &= 3 \times \inf \text{over $U_i$}\\
&\geq \inf \text{over the corresponding $I_i$}\\
&\geq \inf \text{over all binary $\delta$-coverings}\\
&= \BB_s^\delta
\end{align*}

\end{proof}

This bound, while much better than the one mentioned in theorem \ref{badbound}, can still go a long way. The proof, however, touches on what will be our main tool to try to control the bound. The idea is to find a way to replace any $\delta$-covering with a binary $\delta$-covering such that, if the first covering has Hausdorff sum $A$, the latter has Hausdorff sum at most $CA$, where $C$ is the bound we desire to prove. Taking this a step further, we can focus merely on finding a way to, given an interval of length $\ell$, covering it by binary intervals of lengths $m_i$ such that $\sum m_i^s \leq C \ell^s$. The reader might be concerned about making sure that these binary intervals still form a $\delta$-covering, but that is not an issue, as we will see in the following lemma:

\begin{lemma}
Fix a dimension $s \in \R^+$.

Suppose $C \in \R^+$ is such that, for any interval $I$ of length $\ell$, we can find a countable binary covering of $I$ by intervals $\{J_i\}$ of lengths $m_i$ such that

\[\sum m_i^s \leq C \ell^s\]

Then $\BB_s \leq C \HH_s$
\end{lemma}

\begin{proof}
This proof proceeds much like the previous one, with one caveat.

Fixed some $E$, let $I_i$ be any $\delta$-covering of $E$ by intervals. For each $I_i$, let $\{J_{ij}\}_{j \in \N}$ be a collection of binary intervals such that  $\sum_j \lvert J_{ij} \rvert^s \leq C \lvert I_i \rvert^s$.

First, notice that, while it is \emph{not} necessary that the collection $\{J_{ij}\}_{i,j \in \N}$ forms a $\delta$-covering, we can be assured that it forms at most a $C^{1/s}\delta$-covering, as, for any $i, j$ we have

\[ \lvert J_{ij} \rvert^s \leq \sum_j \lvert J_{ij} \rvert^s \leq C \lvert I_i \rvert^s \leq C \delta^s\]

And so $\lvert J_{ij} \rvert \leq C^{1/s} \delta$, as desired.

The argument now goes much like the previous one, except we can only show that $\BB_s^{C^{1/s} \delta} \leq C \HH_s^\delta$. This turns out to be enough, however, as taking $\delta \to 0$ also yields $C^{1/s} \delta \to 0$, and so both sides of this inequality converge to $\BB_s$ and $\HH_s$, respectively.
\end{proof}

\begin{remark}
It should be noted that we had to exclude the case $s = 0$ from consideration in this last proposition, as we will have to in a lot of what follows. This is a nonissue, however, as this particular case is extremely easy to examine. Indeed, as the reader should verify, for this dimension both the Hausdorff and the binary measure turn out to be simply the counting measure on the reals.
\end{remark}

This last lemma will be central in what follows. Indeed, our strategy to find a bound $C$ will be to, given an interval $I$, find a binary covering $\{J_i\}$ of $I$ such that $\sum \lvert J_i \rvert^s \leq C \lvert I \rvert^s$.


\begin{thebibliography}{1}
\bibitem{falconer}
K. J. Falconer, \textit{The Geometry of Fractal Sets}

\bibitem{rogers}
C. A. Rogers, \textit{Hausdorff Measures}

\end{thebibliography}


\end{document}
