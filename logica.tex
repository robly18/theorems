\documentclass{report}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{comment}
\usepackage[utf8]{inputenc}
\usepackage[portuguese]{babel}
\usepackage{hyperref}
\usepackage{indentfirst}
\usepackage{changepage}

%para desenhar árvores sintáticas
\usepackage{tikz}
\usepackage{tikz-qtree}
\tikzset{level distance=2em}

%para escrever pseudocódigo
\usepackage{listings}
\lstset{mathescape=true, basicstyle=\ttfamily}

\usepackage{graphicx}

\usepackage{cite}
\usepackage{makeidx}
\makeindex
\usepackage[nottoc]{tocbibind}

%para ter [[ ]]
\usepackage{stmaryrd}
\newcommand{\bbracket}[1]{\left\llbracket #1 \right\rrbracket}


\title{Lógica}
\author{}
\date{}

\newtheorem{prop}{Proposição}
\newtheorem*{prop*}{Proposição}
\newtheorem{teorema}{Teorema}
\newtheorem{lema}{Lema}

\theoremstyle{definition}
\newtheorem{definicao}{Definição}
\newtheorem*{definicao*}{Definição}

\newtheorem{ideia}{Ideia}
\newtheorem{obstaculo}{Obstáculo}

\theoremstyle{remark}
\newtheorem{obs}{Obs}

\addto\captionsportuguese{
	\renewcommand{\proofname}{Dem}
}


\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}

\renewcommand{\bf}[1]{\mathbf{#1}}

\newcommand{\F}{\mathrm{F}}
\newcommand{\T}{\mathrm{T}}

\newcommand{\lt}{\mathsf{T}}
\newcommand{\lf}{\mathsf{F}}

\DeclareMathOperator{\var}{Var}
\DeclareMathOperator{\fv}{fv}
\DeclareMathOperator{\ar}{Ar}


\DeclareMathOperator{\pnot}{\texttt{not}}
\newcommand{\pand}{\mathbin{\texttt{and}}}
\newcommand{\por}{\mathbin{\texttt{or}}}
\newcommand{\imply}{\mathbin{\Rightarrow}}
\newcommand{\implied}{\mathbin{\Leftarrow}}
\newcommand{\eqv}{\mathbin{\Leftrightarrow}}

\begin{document}
	\maketitle
	\newpage
	
	\tableofcontents
	\newpage
	
	\chapter{Lógica Proposicional}
	
	A lógica proposicional é um sistema que nos permite expressar raciocínios sobre afirmações e relações entre elas. Mais concretamente, é um sistema no qual as variáveis representam afirmações, que tomam o valor `verdadeiro' e `falso', juntamente com um conjunto de operações lógicas com as quais o leitor já estará familiar, por exemplo o `ou' e o `não'.
	
	Não é preciso procurar muito para descobrir que este sistema tem interesse e aplicações práticas. Qualquer linguagem de programação em uso regular terá este sistema contido no seu funcionamento. Tome-se o exemplo da linguagem Python. Nesta linguagem, as variáveis podem ser do tipo \texttt{bool}. Uma variável deste tipo toma um de dois valores: \texttt{True} e \texttt{False}. Existem também os operadores \texttt{and}, \texttt{or} e \texttt{not}, que recebem valores booleanos e retornam valores booleanos. O uso de parênteses permite-nos agrupar expressões, de modo a formar expressões mais complexas. Por exemplo, \texttt{(a and b) or not (b or not c)} é uma expressão válida em Python (assumindo que as variáveis \texttt{a}, \texttt{b} e \texttt{c} estão definidas e são do tipo \texttt{bool}.)
	
	Formalizaremos e estudaremos este sistema, usando-o como `caixa de areia' para nos preparar para a lógica de primeira ordem, que apesar de semelhante, é significativamente mais complexa.
	
	\section{Noções básicas (Preliminar)}
	
	Esta subsecção tem o sufixo `Preliminar' porque não é final. Isto é, o propósito desta subsecção é apenas dar intuição para o significado das coisas, antes de avançar para as definições secas e rigorosas.
	
	\bigskip
	
	Começamos por introduzir a noção de fórmula. Para os nossos propósitos, uma fórmula é uma expressão (finita) composta por variáveis, operadores lógicos, e parênteses. Por exemplo, a expressão $(a \land b) \lor \neg (b \lor \neg c)$ é um exemplo de uma fórmula. Fórmulas são normalmente denotadas por letras gregas minúsculas, e.g. `A fórmula $\varphi$'.
	
	As variáveis (neste caso, $a$, $b$ e $c$) podem tomar os valores de verdade `verdadeiro' e `falso', que aqui denotamos por $\lt$ e $\lf$.
	
	Existem diversos operadores lógicos conhecidos, mas no que se segue usaremos os operadores existentes no Python: $\pnot$, $\pand$ e $\por$. O leitor já deverá estar familiar com o comportamento destes, mas para evitar qualquer confusão, apresentamos as respetivas tabelas de verdade.
	
	\[\label{tabela:operadores}
	\begin{array}{|c|c||c|c|c|}
	\hline
	a & b & \pnot a & a \pand b & a \por b\\
	\hline
	\lt & \lt & \lf & \lt & \lt\\
	\lf & \lt & \lt & \lf & \lt\\
	\lt & \lf &     & \lf & \lt\\
	\lf & \lf &     & \lf & \lf\\
	\hline
	\end{array}
	\]
	
	Dada uma fórmula $\varphi$, podemos tentar `interpretá-la'. Isto é, se atribuirmos valores de verdade às variáveis podemos `substituir esses valores na fórmula' e avaliá-la.
	
	A título de exemplo, consideremos a fórmula $\varphi$ descrita acima.
	\[\varphi : (a \land b) \lor \neg (b \lor \neg c).\]
	
	Suponha-se que estabelecemos $a = \lt$ e $b = c = \lf$. Então, substituindo na fórmula (e escrevendo os operadores \textit{a la} Python), ficamos com
	\begin{gather*}
	(\lt \pand \lf) \por \pnot (\lf \por \pnot \lf)\\
	\lf \por \pnot (\lf \por \lt)\\
	\lf \por \pnot \lt\\
	\lf \por \lf\\
	\lf.
	\end{gather*}
	
	Podemos tornar o processo mais rigoroso descrevendo-o da seguinte forma: definimos uma \emph{valoração} como sendo uma função $\rho : \{\text{Variáveis}\} \to \{\lt, \lf\}$. Qualquer valoração pode ser estendida de forma natural para uma função
	\[{\overline\rho : \{\text{Fórmulas}\} \to \{\lt, \lf\}},\]
	da forma que descrevemos: substitui-se cada variável $x$ pelo valor de $\rho(x)$ e `faz-se as contas'.
	
	Existe uma classe de fórmulas que tem particular interesse, que são as chamadas tautologias.
	
	Uma fórmula $\varphi$ diz-se uma tautologia se para qualquer valoração $\rho$ temos $\overline\rho(\varphi) = \lt$. Isto é, $\varphi$ é sempre verdadeira.
	
	Parte do objetivo deste capítulo é caracterizar as tautologias, de modo a podermos indentificá-las em tempo finito. Claro que, no caso de lógica proposicional, isto já está feito: dada uma fórmula $\varphi$, para determinar se esta é uma tautologia, basta listar todas as valorações possíveis nas variáveis de $\varphi$ (há aqui um detalhe escondido, mas já voltamos a ele) e fazer as contas para todas elas. Se todas elas derem $\lt$, então $\varphi$ é uma tautologia. Caso contrário, não é!
	
	A razão pela qual procuramos outra forma de caracterizar estas fórmulas é porque eventualmente atacaremos o problema mais geral, onde as variáveis, em vez de serem só $\lt$ e $\lf$, podem ser elementos de um universo qualquer arbitrário: naturais, reais, grupos... Assim sendo, deixa de ser possível testar todos os casos possíveis. Como tal, passa a ser necessário um método `sintático', isto é, que manipule as afirmações sem as tentar interpretar. É daí que nasce a formalização de `demonstração', com fim a esclarecer a relação entre o que é verdade e o que é demonstrável.
	
	Voltando a lógica proposicional, consideremos o problema de `testar todos os casos possíveis' para valores das variáveis numa fórmula. Ao fazermos isto estamos a assumir implicitamente que existe um número finito de casos, mas repare-se que, da forma que o definimos, existe um número potencialmente infinito de valorações! De facto, existe um número infinito de variáveis (por exemplo, $x_1$, $x_2$, \dots) pelo que haverá também um número (não-contável!) infinito de valorações. Assim sendo, é impossível testá-las todas.
	
	Claro que, na prática, isso não é um problema. Fixa uma fórmula $\varphi$, só existe um número finito de variáveis a considerar, visto que apenas um número finito de variáveis consta em $\varphi$, e o valor dado a variáveis que não estas é irrelevante. Estamos habituados a tomar estes princípios como garantidos, mas aproveitá-los-emos como veículo para introduzir a noção de indução na estrutura.
	
	A observação essencial é que podemos decompor uma fórmula arbitrária em fórmulas mais pequenas. Por exemplo, a fórmula $\varphi : (a \land b) \lor \neg (b \lor \neg c)$ pode ser decomposta como $\varphi_1 \lor \varphi_2$, onde $\varphi_1$ e $\varphi_2$ são fórmulas mais pequenas do que a inicial. Isto permite-nos usar o princípio de indução (forte) no tamanho de uma fórmula, em que o passo de indução corresponde a separar uma fórmula como $\varphi_1 \lor \varphi_2$, $\varphi_1 \land \varphi_2$ ou $\neg \varphi_1$. Este processo pára nas fórmulas que não podem ser simplificadas mais: chamamos a estas de fórmulas atómicas, e são aquelas compostas por apenas uma variável. Por exemplo, $x$, ou $a$.
	
	Exemplificaremos o princípio, começando por definir indutivamente a noção de `variáveis em fórmula', e demonstrando, com base nesta definição, que, se $\rho$ é uma valoração e $\varphi$ é uma fórmula, $\overline\rho(\varphi)$ depende apenas do valor de $\rho$ nas variáveis em $\varphi$.
	
	\begin{definicao*}
	Seja $\varphi$ uma fórmula. Definimos $\var \varphi$ indutivamente da seguinte forma:
	\begin{itemize}
	\item Se $\varphi$ é da forma `$x$', então $\var \varphi = \{x\}$.
	
	\item Se $\varphi$ é da forma $\neg \varphi_1$, então $\var \varphi = \var \varphi_1$.
	
	\item Se $\varphi$ é da forma $\varphi_1 \lor \varphi_2$ ou $\varphi_1 \land \varphi_2$, então $\var \varphi = \var \varphi_1 \cup \var \varphi_2$.
	\end{itemize}
	\end{definicao*}
	
	\begin{prop*}
	Seja $\varphi$ uma fórmula, $\rho$ uma valoração. Então, $\overline \rho(\varphi)$ depende apenas de $\rho|_{\var \varphi}$. Por outras palavras, se $\rho$ e $\rho'$ são duas valorações tal que para todo $x \in \var \varphi$ se tem $\rho(x) = \rho'(x)$, então $\overline \rho(\varphi) = \overline \rho'(\varphi)$.
	\end{prop*}
	
	\begin{proof}
	Como dito antes, usaremos esta demonstração para exemplificar o conceito de indução em estrutura.
	
	O caso base são as fórmulas atómicas, $\varphi : x$. Como $\overline\rho(\varphi) = \rho(x)$ neste caso, de facto $\overline\rho(\varphi)$ depende apenas de $\rho(x)$, isto é, o valor que $\rho$ toma nos elementos de $\var \varphi = \{x\}$. A demonstração do caso base está terminada.
	
	Fazemos agora o passo de indução. Seja $\varphi$ uma fórmula não-atómica, e suponha-se que o enunciado é verdadeiro para todas as fórmulas mais pequenas do que $\varphi$. Então, partimos do princípio que $\varphi$ é da forma $\neg \varphi_1$, $\varphi_1 \lor \varphi_2$ ou $\varphi_1 \land \varphi_2$. Em qualquer um destes casos, as fórmulas nas quais decompomos $\varphi$ são estritamente mais pequenas do que $\varphi$, pelo que podemos usar nelas a hipótese de indução.
	
	Usemos o caso $\varphi_1 \lor \varphi_2$ como exemplo, sabendo que os outros dois casos são idênticos.
	
	Primeiro que tudo, repare-se que
	\[\overline\rho(\varphi_1 \lor \varphi_2) = \overline\rho(\varphi_1) \por \overline\rho(\varphi_2),\text{ pois}\]
	\begin{align*}
	\overline\rho(\varphi_1 \lor \varphi_2) &= \text{[$\varphi_1$ com cada variável substituída pelo seu valor]}\\
	&\quad\por \text{[$\varphi_2$ com cada variável substituída pelo seu valor]}\\
	&= \overline\rho(\varphi_1) \por \overline\rho(\varphi_2).
	\end{align*}
	
	Agora repare-se que, por hipótese de indução, $\overline\rho(\varphi_1)$ depende apenas de $\rho$ aplicado a $\var \varphi_1$. Da mesma forma, $\overline\rho(\varphi_2)$ depende apenas de $\rho$ aplicado a $\var \varphi_2$. Logo, $\overline\rho(\varphi) = \overline\rho(\varphi_1) \por \overline\rho(\varphi_2)$ depende apenas de $\rho$ aplicado a $\var \varphi_1 \cup \var \varphi_2 = \var \varphi$, o que conclui a demonstração.
	\end{proof}
	
	Voltemos ao problema de identificar tautologias. Existe um conceito mais geral, que é o conceito de `consequência semântica'. A ideia é que às vezes é possível afirmar, \emph{sob certas condições}, que uma fórmula é sempre verdadeira.
	
	Por exemplo, considere-se $\varphi : (a \land c) \lor b \lor (a \land \neg c)$. Esta fórmula não é uma tautologia: por exemplo, a valoração que leva tudo em $\lf$ faz com que esta fórmula fique falsa. No entanto, se uma valoração $\rho$ satisfaz $\overline\rho(a \lor b) = \lt$, então é fácil verificar que de certeza que $\overline\rho(\varphi) = \lt$. Dito de outra forma: sabendo que $\rho$ dá o valor $\lt$ a $a \lor b$, concluímos que $\rho$ dá o valor $\lt$ a $\varphi$. Às vezes, por abuso de linguagem, omitimos as referências a valorações e dizemos apenas `se $a \lor b$ então $\varphi$'.
	
	Antes de generalizarmos este conceito, introduzimos alguma linguagem para nos ajudar a expressar.
	
	\begin{definicao*}
	Seja $\gamma$ uma fórmula, $\rho$ uma valoração. Dizemos que $\rho$ satisfaz $\gamma$, denotado $\rho \Vdash \gamma$, se $\overline\rho(\gamma) = \lt$.
	
	Se em vez de uma fórmula tivermos um conjunto de fórmulas $\Gamma$ (normalmente usamos letras gregas maiúsculas para conjuntos de fórmulas) dizemos que $\rho$ satisfaz $\Gamma$ se $\rho$ satisfizer todas as fórmulas de $\Gamma$. Isto é, simbolicamente,
	\[\rho \Vdash \Gamma \text{ se para todo $\gamma \in \Gamma$ temos } \rho \Vdash \gamma.\]
	\end{definicao*}
	
	\begin{definicao*}
	Seja $\varphi$ uma fórmula e $\Gamma$ um conjunto de fórmulas. Então, dizemos que \emph{$\varphi$ é consequência semântica de $\Gamma$}, representado simbolicamente como
	\[\Gamma \vDash \varphi\]
	se para todas as valorações $\rho$ tal que $\rho \Vdash \Gamma$ temos $\rho \Vdash \varphi$.
	
	Por outras palavras, `se todas as afirmações em $\Gamma$ são verdade, concluímos que $\varphi$ é verdade'.
	
	Normalmente, quando $\Gamma$ é um conjunto finito, abreviamos a notação, omitindo chavetas. Por exemplo, se $\Gamma = \{\gamma_1, \gamma_2, \gamma_3\}$, escreveríamos $\gamma_1, \gamma_2, \gamma_3 \vDash \varphi$.\label{convencao:consequencia}
	
	Isto sugere uma notação para tautologia. De facto, $\varphi$ é uma tautologia sse $\emptyset \vDash \varphi$ (verifique). Escrevendo $\emptyset = \{\}$ e usando a convenção de omitir chavetas, chegamos à notação para tautologias: $\vDash \varphi$.
	
	Outra convenção que seguiremos será abreviar uniões usando vírgulas. Por exemplo, escrever $\Gamma, \alpha, \beta \vDash \varphi$ em vez de $\Gamma \cup \{\alpha, \beta\} \vDash \varphi$.
	\end{definicao*}
	
	A noção de consequência semântica é útil porque, juntamente com uma regra básica que iremos discutir agora, nos permite deduzir verdades novas a partir de verdades conhecidas.
	
	Suponhamos que temos uma afirmação da forma $a \lor b$. Isto significa que pelo menos um dos termos é verdadeiro. Assim sendo, se nos for dito que uma destas duas afirmações é falsa, concluímos que a outra é necessariamente verdadeira, isto é, $a$ falso implica $b$ verdadeiro. Por outras palavras, $a \lor b, \neg a \vDash b$.
	
	Substituindo $a$ por $\neg a$ e partindo do princípio que $\neg \neg a$ é a mesma coisa que $a$, chegamos à conclusão
	\[\neg a \lor b, a \vDash b.\]
	
	Assim sendo, a afirmação $\neg a \lor b$ representa, de alguma forma, `$a$ implica $b$'. Como tal, definimos o símbolo `$a \imply b$' como sinónimo de $\neg a \lor b$, e concluímos a chamada regra de \textit{modus ponens}:
	\[a \imply b, a \vDash b.\]
	
	Esta regra será a fundação do nosso cálculo dedutivo. De facto, veremos que existe um conjunto razoavelmente pequeno de tautologias $T$ tal que qualquer outra tautologia pode ser obtida a partir de aplicação repetida de \textit{modus ponens} a tautologias em $T$. A implicação tem um papel tão central, de facto, que quando definirmos fórmulas rigorosamente usaremos como base os operadores `não' e `implica'.
	
	\smallskip
	
	Terminamos esta secção preliminar com uma introdução ao conceito de metateorema.
	
	A noção de implicação e consequência semântica estão intrinsecamente ligadas. De facto, mostraremos que, em certo sentido, `$a \imply b$ sse $a \vDash b$'. Isto diz-se um metateorema porque relaciona duas `camadas' de verdade: relaciona uma verdade `dentro do sistema' ($a \imply b$) com uma verdade `sobre o sistema' ($a \vDash b$).
	
	\begin{prop*}
	(Metateorema da dedução) Seja $\Gamma$ um conjunto de fórmulas, $a$ e $b$ proposições. Então,
	\[\Gamma, a \vDash b \text{ sse } \Gamma \vDash a \imply b.\]
	\end{prop*}
	
	\begin{proof}\label{dem:mtd}
	($\rightarrow$) Suponha-se que $\Gamma, a \vDash b$. Desejamos mostrar $\Gamma \vDash a \imply b$, isto é, $\Gamma \vDash \neg a \lor b$. Assim sendo, vamos supor que $\rho$ é uma valoração tal que $\rho \Vdash \Gamma$ e mostraremos que $\rho \Vdash \neg a \lor b$.
	
	Existem dois casos: ou $\rho(a) = \lt$ ou $\rho(a) = \lf$.
	
	Se $\rho(a) = \lf$, então $\overline\rho(\neg a \lor b) = [(\pnot \lf) \por \rho(b)] = [\lt \por \rho(b)] = \lt$.
	
	Se $\rho(a) = \lt$, então temos que $\rho \Vdash \Gamma \cup \{a\}$, donde, como $\Gamma, a \vDash b$ por hipótese, concluímos que $\rho(b) = \lt$. Temos então que $\overline\rho(\neg a \lor b) = (\pnot \lt) \por \lt = \lt$, como desejado.
	
	($\leftarrow$) Suponha-se que $\Gamma \vDash a \imply b$. Seja $\rho$ uma valoração tal que $\rho \Vdash \Gamma \cup \{a\}$. Então, em particular, $\rho \Vdash \Gamma$, donde $\rho \Vdash a \imply b$. Sabemos também que $\rho \Vdash a$, pelo que concluímos $\rho \Vdash \{a \imply b, a\}$. Pela regra de \textit{modus ponens}, como $a \imply b, a \vDash b$, concluímos que $\rho \Vdash b$.
	
	Como partimos do princípio que $\rho$ era uma valoração arbitrária tal que $\rho \Vdash \Gamma \cup \{a\}$ e concluímos $\rho \Vdash b$, temos que $\Gamma \cup \{a\} \vDash b$.
	\end{proof}
	
	Este metateorema marca a primeira ocasião em que a nossa linguagem diz algo sobre si mesma. Frases auto-referenciais terão um papel central quando falarmos dos teoremas de incompletude de Gödel, mas isso terá que esperar até ao terceiro capítulo.
	
	\section{Semântica}
	
	O que se segue assume parcialmente que o leitor já leu a secção anterior. Apesar de não haver precedência explícita ou implícita, algumas das ideias essenciais já foram explicadas, e como tal não serão detalhadas novamente. Por outras palavras, o leitor poderá ler esta secção sem ter lido a anterior, mas se der por si perdido nas definições sem entender o seu significado poderá querer voltar atrás e ler a secção introdutória.
	
	\medskip
	
	Começamos por definir o conceito de fórmula proposicional.
	
	Fixe-se, primeiro, um conjunto, que usaremos em tudo o que se segue, chamado o conjunto das variáveis. Isto é apenas um conjunto infinito contável\footnote{Algo estranhamente, a cardinalidade exata deste conjunto é relevante. Bastantes argumentos que faremos de futuro necessitam explicitamente da contabilidade de $X$!} de símbolos $X$. Normalmente usamos variáveis como $x$, $y$, $p$, $q$, e permitimos a modificação de símbolos como a adição de apóstrofos ou asteriscos. Os símbolos $c$, $c'$, $c^*$ são considerados distintos. Usaremos a convenção que variáveis serão representadas por letras romanas minúsculas.
	
	\begin{obs}
	Há a necessidade de distinguir uma variável de uma `meta-variável'. Isto é: se falamos na variável $x$, poderá ser ambíguo se nos referimos ao elemento $x \in X$ ou se a letra $x$ é uma incógnita que pode significar uma variável arbitrária.
	
	Para evitar esta ambiguidade, representamos meta-variáveis a negrito: $\bf{x}$. Ou seja: $x$ é o elemento de $X$, enquanto que $\bf x$ é uma incógnita que pode ser substituída por qualquer variável: $x$, $y$, $z$, \dots
	\end{obs}
	
	Há quem defina, agora, fórmulas como sequências de símbolos. Isto parece ser uma definição intuitiva, visto que é assim que representamos fórmulas: sequências de caracteres. No entanto, visto que no futuro teremos que escrever programas que lêm e interpretam estas fórmulas, é mais conveniente definirmos fórmulas pelas respetivas árvores semânticas.\label{intro_syntatic_trees}
	
	Para esclarecer o que se entende por isto, considere-se a expressão $a \lor b$. Isto consiste de um operador (o operador `ou') aplicado a duas variáveis. Podemos representar isto como uma árvore na seguinte forma:
	
	\begin{center}
	\Tree [.\texttt{or} $a$ $b$ ]
	\end{center}
	
	Podemos, no entanto, considerar expressões mais complexas. Por exemplo, considere-se a expressão $(a \land b) \lor \neg (b \lor \neg c)$. Interpretada como uma árvore, esta expressão fica
	
	\begin{center}
	\Tree [.\texttt{or} [.\texttt{and} $a$ $b$ ] [.\texttt{not} [.\texttt{or} $b$ [.\texttt{not} $c$ ] ] ] ]
	\end{center}
	
	Para os nossos propósitos, é mais fácil manipular árvores do que sequências de caracteres. Assim sendo, é com base nesta perspetiva que definimos o conjunto das fórmulas proposicionais.
	
	\begin{definicao}
	Definimos o conjunto das fórmulas proposicionais $\F_p$ (sobre o conjunto $X$, que deixamos implícito) indutivamente.
	
	Qualquer variável $\bf x$ é uma fórmula.
	
	Se $\alpha$ e $\beta$ são fórmulas, ambos os seguintes são fórmulas:
	
	\begin{center}
	\Tree [.\texttt{not} $\alpha$ ]
	\hspace{3em}
	\Tree [.\texttt{implies} $\alpha$ $\beta$ ]
	\end{center}
	
	Esta forma de representar fórmulas não é particularmente eficiente tipograficamente, pelo que, no que se segue, continuaremos a representá-las com a notação linear $\neg \alpha$ e $\alpha \imply \beta$.
	
	Ao darmos nomes a fórmulas, associaremos o nome ao conteúdo usando dois pontos. Por exemplo, para associar o nome $\varphi$ à fórmula $a \lor b$, escreveremos $\varphi : a \lor b$.
	\end{definicao}
	
	\begin{definicao}
	Definimos uma valoração $\rho$ como uma função $\rho : X \to \{\lt,\lf\}$.
	
	Dada uma valoração $\rho$, definimos uma função $\overline\rho : \F_p \to \{\lt, \lf\}$ indutivamente:
	
	\begin{itemize}
	\item Se $\varphi : \bf x$, definimos $\overline\rho(\varphi) = \rho(\bf x)$.
	
	\item Se $\varphi : \neg \alpha$, definimos $\overline\rho(\varphi) = \pnot \overline\rho(\alpha)$.\footnote{Se o leitor não perceber o uso dos operadores $\pnot$ e $\por$, ver página \pageref{tabela:operadores}.}
	
	\item Se $\varphi : \alpha \imply \beta$, definimos $\overline\rho(\varphi) = (\pnot \overline\rho(\alpha)) \por \overline\rho(\beta)$.
	\end{itemize}
	
	Dada uma valoração $\rho$, dizemos que $\rho \Vdash \varphi$ ($\rho$ satisfaz $\varphi$) se $\overline\rho(\varphi) = \lt$. Se em vez de uma fórmula $\varphi$ tivermos um conjunto de fórmulas $\Gamma$, dizemos que $\rho \Vdash \Gamma$ se $\rho \Vdash \gamma$ para todo $\gamma \in \Gamma$.
	\end{definicao}
	
	\begin{definicao}\label{def:prop:consequenciasemantica}
	Seja $\varphi \in \F_p$, $\Gamma \subseteq \F_p$. Dizemos que $\Gamma \vDash \varphi$ (pronunciado `$\varphi$ é consequência semântica de $\Gamma$') se todas as valorações que satisfazem $\Gamma$ também satisfazem $\varphi$. Por outras palavras, se para todas as valorações $\rho$ tal que $\rho \Vdash \Gamma$ temos $\rho \Vdash \varphi$.
	
	Para abreviar a notação, seguimos algumas convenções, detalhadas na página \pageref{convencao:consequencia}, relacionadas com omissão de chavetas em expressões do género $\{a, b\} \vDash c$.
	
	Dizemos que $\varphi$ é uma tautologia se $\emptyset \vDash \varphi$. Isto pode ser representado como $\vDash \varphi$.
	\end{definicao}
	
	Começamos por apresentar algumas regras de dedução que nos ajudarão a mostrar que certas coisas são consequência semântica de outras.
	
	\begin{prop}
	A relação $\vDash$ é transitiva. Isto é:
	
	Sejam $\Gamma, \Phi \subseteq \F_p$ e $\psi \in \F_p$. Suponha-se que $\Gamma \vDash \varphi$ para todo $\varphi \in \Phi$ (Podemos representar isto como $\Gamma \vDash \Phi$) e que $\Phi \vDash \psi$. Então $\Gamma \vDash \psi$.
	\end{prop}
	
	\begin{proof} Suponha-se $\Gamma \vDash \Phi$ e $\Phi \vDash \psi$. Então, para mostrar $\Gamma \vDash \psi$ começamos por supor que $\rho$ é uma valoração tal que $\rho \Vdash \Gamma$. Então, por definição de $\Gamma \vDash \Phi$, temos que $\Gamma \vDash \varphi$ para todo $\varphi \in \Phi$. Como $\rho \Vdash \Gamma$, obtemos que $\rho \Vdash \varphi$ para todo $\varphi \in \Phi$, ou seja, que $\rho \Vdash \Phi$. Finalmente, por definição de $\Phi \vDash \psi$, concluímos que $\rho \Vdash \psi$, terminando a demonstração.
	\end{proof}
	
	\begin{prop}\label{prop:mp} (\textit{Modus ponens}) 
	
	Sejam $\alpha, \beta \in \F_p$. Então, $(\alpha \imply \beta), \alpha \vDash \beta$.
	\end{prop}
	
	\begin{proof}
	Suponha-se que $\rho \Vdash \alpha \imply \beta$ e $\rho \Vdash \alpha$. Então, $(\pnot\overline\rho(\alpha))\por\overline\rho(\beta) = \lt$ e $\overline\rho(\alpha) = \lt$. Substituindo $\overline\rho(\alpha)$ por $\lt$ na primeira afirmação, ficamos com $(\pnot\lt)\por\overline\rho(\beta) = \lt$, isto é, $\lf \por \overline\rho(\beta) = \lt$. Para isto acontecer, é necessário que $\overline\rho(\beta) = \lt$, ou seja, $\rho \Vdash \beta$.
	\end{proof}
	
	\begin{prop}\label{prop:mtd}
	(Metateorema da dedução)
	
	Sejam $\alpha, \beta \in \F_p$, $\Gamma \subseteq \F_p$. Então
	\[\Gamma, \alpha \vDash \beta \text{ sse } \Gamma \vDash \alpha \imply \beta.\]
	\end{prop}
	
	\begin{proof}
	Uma possível demonstração é semelhante à demonstração feita na página \pageref{dem:mtd}, com um pequeno detalhe. Essa demonstração foi feita para variáveis $a, b$ em vez de fórmulas $\alpha, \beta$. No entanto, substituindo $\rho(a)$ por $\overline\rho(\alpha)$ e $\rho(b)$ por $\overline\rho(\beta)$, a demonstração funciona sem modificações. Apresentamos, no entanto, uma demonstração ligeiramente diferente de ($\leftarrow$), que exemplifica as proposições acima.
	
	Suponha-se que $\Gamma \vDash \alpha \imply \beta$. É trivial reparar que $\Gamma, \alpha \vDash \Gamma$ (verifique). Por transitividade,
	\begin{equation}\label{eq:mtd:1}
	\Gamma, \alpha \vDash \alpha \imply \beta.
	\end{equation}
	
	Sabemos também que
	\begin{equation}\label{eq:mtd:2}
	\Gamma, \alpha \vDash \alpha.
	\end{equation}
	
	Juntando as afirmações \eqref{eq:mtd:1} e \eqref{eq:mtd:2}, obtemos que $\Gamma, \alpha \vDash \{\alpha \imply \beta, \alpha\}$. A regra de \textit{modus ponens} diz-nos que $\{\alpha \imply \beta, \alpha\} \vDash \beta$, pelo que, por transitividade, $\Gamma, \alpha \vDash \beta$, como desejado.
	\end{proof}
	
	O leitor poderá ter reparado que os símbolos $a \land b$ e $a \lor b$ não foram definidos, visto que na nossa definição de fórmula englobámos apenas as operações $\neg a$ e $a \imply b$. Visto que o uso dos operadores $\land$ e $\lor$ são convenientes, definimo-los por abreviatura. Ou seja, escrevemo-los em termos de `não's e `implica's, e sempre que escrevemos `ou' ou `e' substituímos mentalmente pela abreviatura estabelecida.
	
	Recordamos o leitor que $\overline\rho(\alpha \imply \beta) = (\pnot \overline\rho(\alpha)) \por \overline\rho(\beta)$, pelo que desejamos escrever os operadores $\por$ e $\pand$ como uma expressão semelhante a esta forma.
	
	Afirmamos que
	\begin{gather*}
	a \por b = (\pnot (\pnot a)) \por b\\
	a \pand b = \pnot ((\pnot a) \por (\pnot b)).
	\end{gather*}
	
	Pelo que estabelecemos as seguintes abreviaturas:
	
	\begin{itemize}
	\item Quando escrevemos $\alpha \lor \beta$, substituímos mentalmente por $\neg \alpha \imply \beta$.
	
	\item Quando escrevemos $\alpha \land \beta$, substituímos mentalmente por $\neg (\alpha \imply \neg \beta)$.
	
	\item Estabelecemos também a abreviatura $a \eqv b$ como $(a \imply b) \land (b \imply a)$ (que por sua vez é abreviatura de outra expressão, mas achamos por bem não expandir por completo).
	\end{itemize}
	
	Considere-se a noção de equivalência. Estamos habituados a que afirmações equivalentes sejam `a mesma coisa', na medida em que podemos substituir uma afirmação pela outra. Por exemplo, suponha-se que provamos que $\vDash \neg \neg \alpha \eqv \alpha$. (Faremos os detalhes mais tarde.) Agora, suponhamos que se deseja provar uma afirmação na qual aparece $\neg \neg \alpha$, por exemplo, $\beta \imply \neg \neg \alpha$. Seria agradável reduzir isto a mostrar que $\beta \imply \alpha$. Felizmente, verifica-se que este tipo de atalhos é admissível. É nisto que consiste o \emph{metateorema da substituição de equivalentes}.
	
	Antes de estudarmos a relação de equivalência, no entanto, vale a pena investigar o símbolo $\land$, visto que este consta na definição de $\eqv$.
	
	\begin{prop}\label{prop:conj:prop:sem}
	Sejam $\alpha, \beta \in \F_p$. Então, $\alpha, \beta \vDash \alpha \land \beta$ e $\alpha \land \beta \vDash \{\alpha, \beta\}$.
	\end{prop}
	
	\begin{proof}
	Para mostrar isto, basta mostrar que, se $\rho$ é uma valoração arbitrária, $\rho \Vdash \alpha \land \beta$ sse $\rho \Vdash \{\alpha, \beta\}$.
	
	De facto, $\rho \Vdash \alpha \land \beta$ sse $\overline\rho(\neg(\alpha \imply \neg \beta)) = \lt$ sse $\pnot ((\pnot \overline\rho(\alpha)) \por (\pnot \overline\rho(\beta))) = \lt$. É fácil verificar que isto acontece sse $\overline\rho(\alpha) = \overline\rho(\beta) = \lt$, ou seja, $\rho \Vdash \{\alpha, \beta\}$. Isto conclui a demonstração.
	\end{proof}
	
	Com a informação que já temos até agora, podemos começar a fazer demonstrações sem pensar tanto sobre valorações e casos possíveis. Exemplificamos demonstrando que $\alpha \eqv \neg \neg \alpha$.
	
	\begin{prop}\label{alphaeqvnegnegalpha}
	Para qualquer $\alpha, \beta \in \F_p$, $\vDash \alpha \eqv \neg \neg \alpha$.
	\end{prop}
	
	\begin{proof}
	Primeiro que tudo, é preciso observar a `meta-equivalência' trivial:
	\[\pnot \pnot x = x.\]
	
	Usando isto, concluímos que $\alpha \vDash \neg \neg \alpha$ e $\neg \neg \alpha \vDash \alpha$. Aplicando o metateorema da dedução a cada um destes, obtemos $\vDash \{\alpha \imply \neg \neg \alpha, \neg \neg \alpha \imply \alpha\}$. Usando a proposição \ref{prop:conj:prop:sem}  temos que $\{\alpha \imply \neg \neg \alpha, \neg \neg \alpha \imply \alpha\} \vDash (\alpha \eqv \neg \neg \alpha)$ (recorde-se da definição de equivalência por abreviatura), e por transitividade, como $\emptyset \vDash \{\alpha \imply \neg \neg \alpha, \neg \neg \alpha \imply \alpha\} \vDash (\alpha \eqv \neg \neg \alpha)$, temos $\emptyset \vDash \alpha \eqv \neg \neg \alpha$.
	\end{proof}
	
	Repare-se que a maior parte da demonstração que acabámos de fazer consistiu em passar de `$\alpha \vDash \beta$ e $\beta \vDash \alpha$' para `$\vDash \alpha \eqv \beta$'. Agora que sabemos que este tipo de raciocínios é válido, abreviaremos as nossas demonstrações na medida em que deixamos esta passagem para o leitor. Por exemplo:
	
	\begin{prop}
	Para qualquer $\alpha, \beta \in \F_p$, $\vDash (\alpha \land \beta) \eqv (\beta \land \alpha)$.
	\end{prop}
	
	\begin{proof}
	Sabemos que $\alpha \land \beta \vDash \{\alpha, \beta\}$. Como $\{\alpha, \beta\} = \{\beta, \alpha\}$, obtemos que $\{\alpha, \beta\} \vDash \beta \land \alpha$. Por transitividade, conclui-se $(\alpha \land \beta) \vDash (\beta \land \alpha)$. Reproduzindo a demonstração com o papel de $\alpha$ e $\beta$ trocado, damos a prova por concluída.
	\end{proof}
	
	Visto que já temos ferramentas para mostrar equivalências, passamos agora à proposição que nos permite fazer uso de equivalências. Antes de o podermos fazer, no entanto, precisamos de formalizar o que significa `substituir'.
	
	A ideia essencial é que pretendemos, numa fórmula $\varphi$, substituir certas instâncias de uma expressão $\alpha$ por uma outra expressão (presumivelmente equivalente) $\beta$. Por exemplo, passar de $\varphi : a \land \neg \neg (b \imply c)$ para $\psi : a \land (b \imply c)$, e depois possivelmente para $\theta : (b \imply c) \land a$. Para podermos manipular o conceito de substituição e demonstrar coisas sobre ele, precisamos de o definir de forma rigorosa. Como o leitor poderá estar à espera, fá-lo-emos de forma indutiva.
	
	\begin{definicao}
	Sejam $\varphi, \psi, \alpha, \beta \in \F_p$. Dizemos que \emph{$\psi$ é obtido de $\varphi$ por substituição de $\alpha$ por $\beta$} se:
	
	\begin{itemize}
	\item Caso base: Se $\varphi$ e $\psi$ são a mesma fórmula.
	
	\item Substituição: Se $\varphi : \alpha$ e $\psi : \beta$.
	
	\item Passo de indução (negação): Se todos as seguintes afirmações se verificarem:
	\begin{itemize}
	\item $\varphi$ é da forma $\neg \varphi_1$
	
	\item $\psi$ é da forma $\neg \psi_1$
	
	\item $\psi_1$ é obtido de $\varphi_1$ por substituição de $\alpha$ por $\beta$.
	\end{itemize}
	
	\item Passo de indução (implicação): Se todas as seguintes afirmações se verificarem:
	\begin{itemize}
	\item $\varphi$ é da forma $\varphi_1 \imply \varphi_2$
	
	\item $\psi$ é da forma $\psi_1 \imply \psi_2$
	
	\item $\psi_1$ é obtido de $\varphi_1$ por substituição de $\alpha$ por $\beta$
	
	\item $\psi_2$ é obtido de $\varphi_2$ por substituição de $\alpha$ por $\beta$.
	\end{itemize}
	\end{itemize}
	\end{definicao}
	
	Note-se que esta definição é particularmente complexa, e aplicá-la na prática é chato. Claro que isto corresponde simplesmente à noção intuitiva de `pegar nalguns $\alpha$, apagá-los, e escrever $\beta$ no seu lugar', pelo que, ao afirmar que duas fórmulas são obtidas uma da outra por substituição, não nos daremos ao trabalho de fazer uma inteira justificação com base nestes casos todos. No entanto, se o leitor estiver confuso com a definição é recomendado tentar aplicá-la a alguns exemplos de substituição de fórmulas.
	
	Podemos finalmente passar à demonstração de:
	
	\begin{prop}
	(Metateorema de substituição de equivalentes) Sejam $\varphi, \psi, \alpha$ e $\beta$ fórmulas em $\F_p$ e $\Gamma \subseteq \F_p$. Suponha-se que $\Gamma \vDash \alpha \eqv \beta$ e que $\psi$ é obtido de $\varphi$ por substituição de $\alpha$ por $\beta$. Então,
	\[\Gamma \vDash \varphi \eqv \psi.\]
	\end{prop}
	
	\begin{proof}
	Como esperado, fazemos uma prova por indução usando a definição de $\psi$ ser obtido de $\varphi$ por substituição.
	
	\smallskip
	\textbf{Casos base:}
	
	Se $\varphi$ e $\psi$ são a mesma fórmula, então basta reparar que $\varphi \eqv \varphi$ é uma tautologia (deixamos a demonstração disto para o leitor).
	
	Se $\varphi : \alpha$ e $\psi : \beta$, a afirmação $\Gamma \vDash \varphi \eqv \psi$ é dada como hipótese.
	
	\smallskip
	\textbf{Passo de indução:}
	
	Se $\varphi : \neg \varphi_1$ e $\psi : \neg \psi_1$, onde $\psi_1$ é obtido de $\varphi_1$ por substituição, usamos a hipótese de indução para afirmar que $\Gamma : \varphi_1 \eqv \psi_1$. Basta agora mostrar que $\varphi_1 \eqv \psi_1 \vDash (\neg \varphi_1) \eqv (\neg \psi_1)$, e a conclusão decorre por transitividade. Deixamos a demonstração deste facto auxiliar como exercício para o leitor, de modo a não quebrar o raciocínio.
	
	Se $\varphi : \varphi_1 \imply \varphi_2$ e $\psi : \psi_1 \imply \psi_2$, onde $\psi_1$ e $\psi_2$ são obtidos por substituição a partir de $\varphi_1$ e $\varphi_2$, respetivamente, podemos aplicar a hipótese de indução para afirmar que $\Gamma \vDash \varphi_1 \eqv \psi_1$ e $\Gamma \vDash \varphi_2 \eqv \psi_2$. Por transitividade, é suficiente mostrar que $\{\varphi_1 \eqv \psi_1, \varphi_2 \eqv \psi_2\} \vDash (\varphi_1 \imply \varphi_2) \eqv (\psi_1 \imply \psi_2)$.
	
	Faremos a demonstração deste último facto aplicando repetidamente o metateorema da dedução. Mostraremos que
	\begin{equation}\label{eq:mse:1}
	\varphi_1 \eqv \psi_1, \varphi_2 \eqv \psi_2, \varphi_1 \imply \varphi_2, \psi_1 \vDash \psi_2.
	\end{equation}
	
	Ora, repare-se que $\varphi_1 \eqv \psi_1 \vDash \psi_1 \imply \varphi_1$, como se pode mostrar recorrendo à definição por abreviatura de $\eqv$ e a propriedades essenciais de $\land$ (proposição \ref{prop:conj:prop:sem}). De igual modo, $\varphi_2 \eqv \psi_2 \vDash \varphi_2 \imply \psi_1$. Assim sendo,
	\[\varphi_1 \eqv \psi_1, \varphi_2 \eqv \psi_2, \varphi_1 \imply \varphi_2, \psi_1 \vDash \psi_1, \psi_1 \imply \varphi_1, \varphi_1 \imply \varphi_2, \varphi_2 \imply \psi_2.\]
	
	Aplicando repetidamente modus ponens, chegamos à conclusão que desta última sequência de termos se conclui $\psi_2$, pelo que, por transitividade, a equação \eqref{eq:mse:1} é verdadeira. Aplicando agora o metateorema da dedução duas vezes, conclui-se
	\[
	\varphi_1 \eqv \psi_1, \varphi_2 \eqv \psi_2  \vDash (\varphi_1 \imply \varphi_2) \imply (\psi_1 \imply \psi_2).
	\]
	
	Repetindo o argumento com o papel de $\varphi$ e $\psi$ trocados, obtemos a implicação inversa, pelo que aplicando a proposição \ref{prop:conj:prop:sem} concluímos, finalmente,
	\[
	\varphi_1 \eqv \psi_1, \varphi_2 \eqv \psi_2  \vDash (\varphi_1 \imply \varphi_2) \eqv (\psi_1 \imply \psi_2).
	\]
	\end{proof}
	
	Mostramos agora um conjunto de aplicações do metateorema de substituição de equivalentes que nos será útil no futuro, quando precisarmos do operador $\lor$.
	
	Começamos por realçar duas propriedade com a qual o leitor já estará familiar: o princípio do contrarrecíproco e as leis de deMorgan.
	
	\begin{prop}
	(Contrarrecíproco)
	
	Se $\alpha$ e $\beta$ são fórmulas, temos a seguinte equivalência:
	
	\begin{equation}
	\vDash (\alpha \imply \beta) \eqv (\neg \beta \imply \neg \alpha)
	\end{equation}
	\end{prop}
	
	\begin{proof}
	Faremos esta demonstração `à bruta', usando o nosso conhecimento dos operadores $\pnot$ e $\por$.
	
	Sabemos que, se $\rho$ é uma valoração,
	\begin{align*}
	\rho \Vdash (\alpha \imply \beta) &\text{ sse } (\pnot \overline\rho(\alpha)) \por \overline\rho(\beta) = \lt\\
	&\text{ sse } (\pnot \pnot \overline\rho(\beta)) \por (\pnot \overline\rho(\alpha)) = \lt\\
	&\text{ sse } (\pnot \overline\rho(\neg \beta)) \por \overline\rho(\neg \alpha) = \lt\\
	&\text{ sse } \rho \Vdash (\neg \beta \imply \neg \alpha).
	\end{align*}
	
	Isto mostra que $\alpha \imply \beta \vDash \neg \beta \imply \neg \alpha$ e vice-versa, o que, através de duas aplicações do metateorema da dedução e uso de propriedades do operador $\land$, nos permite chegar à conclusão que
	
	\begin{equation*}
	\vDash (\alpha \imply \beta) \eqv (\neg \beta \imply \neg \alpha),
	\end{equation*}
	como desejado.
	\end{proof}
	
	\begin{prop}\label{demorgan}
	(Leis de deMorgan)
	
	Se $\alpha$ e $\beta$ são fórmulas, temos as seguintes equivalências:
	
	\begin{gather}
	\vDash \neg (\alpha \land \beta) \eqv (\neg \alpha \lor \neg \beta)\label{demorgan1}\\
	\vDash \neg (\alpha \lor \beta) \eqv (\neg \alpha \land \neg \beta)\label{demorgan2}
	\end{gather}
	\end{prop}
	
	\begin{proof}
	Recordemo-nos das definições por abreviatura de $\lor$ e $\land$.
	
	\begin{gather}
	\alpha \land \beta \text{ é o mesmo que } \neg (\alpha \imply \neg \beta)\\
	\alpha \lor \beta \text{ é o mesmo que } (\neg \alpha \imply \beta)
	\end{gather}
	
	Substituindo nas expressões \eqref{demorgan1} e \eqref{demorgan2}, fica claro que a demonstração é mera aplicação repetida do metateorema da substituição de equivalentes usando a equivalência $\alpha \eqv \neg \neg \alpha$.
	\end{proof}
	
	As leis de deMorgan encontram alguma utilidade em provar coisas sobre o operador $\lor$, visto que neste momento só temos informação sobre $\land$.
	
	\begin{lema}\label{lemaou}
	Se $\alpha, \beta$ e $\varphi$ são fórmulas, a seguinte fórmula é uma tautologia:
	\begin{equation}\label{eq:lemaou}
	\vDash (\alpha \imply \varphi) \imply ((\beta \imply \varphi) \imply ((\alpha \lor \beta) \imply \varphi))
	\end{equation}
	\end{lema}
	
	\begin{proof}
	Usando o princípio do contrarrecíproco e o metateorema da substituição de equivalentes, conclui-se que para provar \eqref{eq:lemaou} é necessário e suficiente mostrar
	\begin{equation}
	\vDash (\neg \varphi \imply \neg \alpha) \imply ((\neg \varphi \imply \neg \beta) \imply (\neg \varphi \imply \neg (\alpha \lor \beta))),
	\end{equation}
	e uma aplicação das leis de deMorgan diz-nos que isto é equivalente a mostrar
	\begin{equation}
	\vDash (\neg \varphi \imply \neg \alpha) \imply ((\neg \varphi \imply \neg \beta) \imply (\neg \varphi \imply (\neg \alpha \land \neg \beta))).
	\end{equation}
	
	O problema está agora reduzido a aplicação generosa do metateorema da dedução, \textit{modus ponens} e propriedades elementares (proposição \ref{prop:conj:prop:sem}) do operador $\land$.
	\end{proof}
	
	\section{Sintática}
	
	Como explicado na secção preliminar, o nosso objetivo final será arranjar uma forma sistemática de descobrir que afirmações são tautologias. Um pouco mais geralmente, dado um conjunto de afirmações $\Gamma$ (`hipóteses'), queremos arranjar um método para decidir, dada uma fórmula $\varphi$, se $\Gamma \vDash \varphi$. Para mais, queremos fazer isto sem recorrer a valorações, visto que, quando estivermos a trabalhar num contexto mais geral do que o cálculo proposicional, não teremos a vantagem de universos finitos.
	
	Para o fazer, formalizamos a noção de demonstração, baseando-nos na nossa experiência a escrever provas.
	
	Ao redigirmos provas geralmente escrevemos coisas linearmente (isto é, uma prova é uma sequência de afirmações), em que novas afirmações são justificadas com base em conhecimento prévio. Algumas afirmações são hipóteses (1) do teorema que estamos a tentar provar, algumas delas são afirmações que são simplesmente logicamente válidas (2), e algumas são consequências de afirmações anteriores (3). Para exemplificar, considere-se a seguinte demonstração em linguagem natural. Os passos estão anotados consoante a qual destes tipos de afirmação correspondem.
	
	\medskip
	
	\textbf{Proposição:} Sejam $a$ e $b$ valores de verdade tal que $a \por b = \lt$ e $b = \lf$. Então, $a = \lt$.
	
	\begin{adjustwidth}{1cm}{}
	1: Hipótese: $a \por b = \lt$ (1)\\
	2: Axioma: Se $a \por b = \lt$ então $a = \lt$ ou $b = \lt$ (2)\\
	3: Como consequência das linhas 1 e 2: $a = \lt$ ou $b = \lt$. (3)\\
	4: Hipótese: $b = \lf$ (1)\\
	5: Axioma: Se sabemos $a = \lt$ ou $b = \lt$, mas $b \neq \lt$, então $a = \lt$ (2)\\
	6: Como consequência das linhas 3 e 4: $a = \lt$. (3)
	\end{adjustwidth}
	
	\medskip
	
	Claro que isto pressupõe uma noção de `axiomas', ou pelo menos, afirmações tomadas como verdadeiras sem justificação. Uma escolha óbvia para o nosso caso seria tomar como axiomas todas as tautologias, mas é precisamente o problema de encontrar as tautologias que estamos a tentar resolver! No entanto, o seu uso é instrutivo visto que, apesar de tudo, o problema de encontrar consequências semânticas não fica trivializado. Isto é, a seguinte questão ainda requer algum trabalho:
	
	\begin{center}
	Sejam $\Gamma \subseteq \F_p$ e $\varphi \in \F_p$. Será possível construir uma demonstração cuja conclusão seja $\varphi$, usando como axiomas as tautologias e como hipóteses as fórmulas em $\Gamma$?
	\end{center}
	
	Em breve formalizaremos a noção de demonstração, mas a ideia principal é que criaremos um sistema em que algumas afirmações podem ser demonstradas e outras não. O objetivo é que as afirmações que podem ser demonstradas são exatamente aquelas que são verdadeiras. Ou seja, se demonstramos uma afirmação, ela tem que ser verdadeira (chamamos a isto correção), e se uma afirmação é verdadeira terá que existir uma prova dela (chamamos a isto completude). Em lógica, é normalmente bastante fácil assegurar correção de sistemas de demonstração --- basta certificarmo-nos que todos os passos individuais estão corretos ---, mas provar que qualquer afirmação verdadeira pode ser demonstrada é uma história completamente diferente!
	
	De agora em diante, tomaremos como conjunto de axiomas (chamamos a este conjunto $A$) as tautologias. De futuro, veremos que este conjunto pode ser drasticamente reduzido.
	
	\textbf{Se o leitor deseja ver os axiomas finais,} poderá avançar para a página \pageref{prop:axiomasfinais}. Não há perda de continuidade em ler esta secção mesmo sabendo os axiomas, tendo conhecimento que em qualquer momento que invocamos uma tautologia como axioma, podemos substituir essa invocação por uma demonstração (com base nos axiomas finais) da fórmula em questão.
	
	\begin{definicao}
	Seja $\Gamma \subseteq \F_p$. Uma demonstração com base em $\Gamma$, ou demonstração que tem $\Gamma$ como hipóteses, é uma sequência finita
	\[(\varphi_1, j_1), (\varphi_2, j_2), \dots, (\varphi_n, j_n)\]
	onde cada $\varphi_i$ é uma fórmula proposicional, e o respetivo $j_i$ (que simboliza a justificação de $\varphi_i$) satisfaz as seguintes regras:
	
	\begin{itemize}
	\item Cada $j_i$ é um símbolo da forma \texttt{Ax}, \texttt{Hip} ou $\texttt{MP}_{ab}$, com $a,b \in \N$
	
	\item Se $j_i = \texttt{Ax}$ é necessário que $\varphi_i \in A$, onde $A$ é o conjunto de axiomas. Recordamos que, neste momento, $A$ é para nós o conjunto das tautologias.
	
	\item Se $j_i = \texttt{Hip}$ é necessário que $\varphi_i \in \Gamma$
	
	\item Se $j_i = \texttt{MP}_{ab}$, então $a, b < i$ e $\varphi_a$ é a fórmula $\varphi_b \imply \varphi_i$.
	
	Por outras palavras, se $\varphi_a$ é da forma $\alpha \imply \beta$ e $\varphi_b$ é a fórmula $\alpha$, então podemos concluir a fórmula $\beta$, com a justificação $\texttt{MP}_{ab}$ (\textit{Modus ponens}, com hipóteses $a$ e $b$).
	\end{itemize}
	\end{definicao}
	
	\begin{definicao}
	Dizemos que $\varphi \in \F_p$ é um teorema de $\Gamma \subseteq \F_p$, denotado
	\[\Gamma \vdash \varphi\]
	se existe uma demonstração com base em $\Gamma$ cuja última afirmação seja $\varphi$.
	\end{definicao}
	
	Antes de começarmos a mostrar propriedades deste sistema, vamos ver algumas demonstrações para entender como é que o sistema funciona.
	
	\begin{prop*}
	Se $\alpha \in \F_p$ então $\vdash \alpha \eqv \neg \neg \alpha$.
	\end{prop*}
	\begin{proof}
	Recordamos primeiro o leitor da convenção de omissão de chavetas, segundo a qual a afirmação $\vdash \alpha \eqv \neg \neg \alpha$ é abreviatura de $\{\} \vdash \alpha \eqv \neg \neg \alpha$, ou seja, `$\alpha \eqv \neg \neg \alpha$ pode ser provado sem hipóteses'.
	
	Pela proposição \ref{alphaeqvnegnegalpha}, a afirmação $\alpha \eqv \neg \neg \alpha$ é uma tautologia. Assim sendo, temos uma demonstração particularmente concisa:
	\[(\alpha \eqv \neg \neg \alpha, \texttt{Ax}).\]
	\end{proof}
	
	\begin{prop*}
	Se $\alpha, \beta, \gamma \in \F_p$ então $\alpha \imply (\beta \imply \gamma), \alpha \land \beta \vdash \gamma$.
	\end{prop*}
	\begin{proof}
	Apresentamos a seguinte demonstração, que não é a mais direta. De facto, como veremos em breve, qualquer demonstração com um número finito $n$ de hipóteses pode ser feito de forma extremamente expedita em apenas $2n+1$ passos. No entanto, as tautologias necessárias para o fazer podem ser complicadas, pelo que apresentamos uma prova que usa apenas tautologias evidentes.
	
	\begin{align*}
	&\alpha \imply (\beta \imply \gamma) &\texttt{Hip}\\
	&\alpha \land \beta &\texttt{Hip}\\
	&\alpha \land \beta \imply \alpha &\texttt{Ax}&&\text{Aplicar MTD à prop \ref{prop:conj:prop:sem}}\\
	&\alpha &\texttt{MP}_{3,2}\\
	&\beta \imply \gamma &\texttt{MP}_{1, 4}\\
	&\alpha \land \beta \imply \beta &\texttt{Ax}&&\text{Aplicar MTD à prop \ref{prop:conj:prop:sem}}\\
	&\beta &\texttt{MP}_{6,2}\\
	&\gamma &\texttt{MP}_{5,7}
	\end{align*}
	\end{proof}
	
	\bigskip
	
	Como mencionado anteriormente, há duas coisas que queremos que a nossa relação $\vdash$ satisfaça. Queremos que se $\Gamma \vdash \varphi$ então $\Gamma \vDash \varphi$ (correção), e queremos que se $\Gamma \vDash \varphi$ então $\Gamma \vdash \varphi$ (completude). A primeira propriedade é a mais fácil de verificar, e como tal, será a primeira que vamos provar.
	
	\begin{prop}\label{prop:correcao} (Correção do cálculo proposicional)
	Seja $\Gamma \subseteq \F_p$, $\varphi \in \F_p$. Suponha-se que $\Gamma \vdash \varphi$. Então, $\Gamma \vDash \varphi$.
	\end{prop}
	
	\begin{proof}
	Se $\Gamma \vdash \varphi$, existe uma demonstração de $\varphi$ com base em $\Gamma$. Suponha-se que esta é dada por $(\varphi_1, j_1), \dots, (\varphi_n, j_n)$. Mostraremos por indução que todos os $\varphi_i$ são consequência semântica de $\Gamma$, e portanto, em particular, $\Gamma \vDash \varphi_n$, ou seja, $\Gamma \vDash \varphi$.
	
	Faremos esta prova usando o princípio de indução forte:
	
	\begin{center}
	Seja $S$ um subconjunto dos naturais. Suponha-se que se prova a seguinte afirmação para todo $n \in \N$:
	
	Se o conjunto $\N \cap \left[0,n\right[$ está contido em $S$, então $n \in S$.
	
	O princípio de indução forte diz-nos que, neste caso, $S = \N$.\footnote{Para justificar este princípio: suponha-se que $S \neq \N$. Então, $\N \setminus S$ é não-vazio. Qualquer subconjunto não-vazio dos naturais tem mínimo; seja, então, $n$ o mínimo de $\N \setminus S$. Por definição de mínimo, temos que $\N \cap \left[0, n\right[ \subseteq S$, e, por hipótese, isto implica que $n \in S$. Contradição, pelo que $\N \setminus S$ tem que ser vazio e portanto $S = \N$.} Note-se que não é necessário provar caso base.
	\end{center}
	
	Seja $i$ um índice tal que para todo $i' < i$ se tem $\Gamma \vDash \varphi_{i'}$. Então, mostramos que $\Gamma \vDash \varphi_i$. A prova consiste em examinar a justificação de $\varphi_i$, isto é, $j_i$.
	
	\begin{itemize}
	\item Se $j_i = \texttt{Hip}$, então $\varphi_i \in \Gamma$, donde $\Gamma \vDash \varphi_i$ trivialmente.
	
	\item Se $j_i = \texttt{Ax}$, então $\varphi_i \in A$, donde $\varphi_i$ é uma tautologia. Isto claramente implica $\Gamma\vDash\varphi_i$.
	
	\item Se $j_i = \texttt{MP}_{ab}$, basta usar a transitividade da consequência semântica. Por hipótese, $\Gamma \vDash \{\varphi_a, \varphi_b\}$. Por definição de demonstração, $\varphi_a$ é a fórmula $\varphi_b \imply \varphi_i$. Pela regra de \textit{modus ponens}, $\{\varphi_b \imply \varphi_i, \varphi_b\} \vDash \varphi_i$, pelo que, por transitividade, $\Gamma \vDash \varphi_i$, como desejado.
	\end{itemize}
	\end{proof}
	
	Iniciamos agora a demonstração da completude do cálculo. Fazemos primeiro notar que, sob certas condições, esta é uma trivialidade:
	
	\begin{prop}
	Sejam $\Gamma \subseteq \F_p$ e $\varphi \in \F_p$ tal que $\Gamma \vDash \varphi$. Suponha-se, em adição, que $\Gamma = \{\gamma_1, \dots, \gamma_n\}$ é finito. Então, $\Gamma \vdash \varphi$.
	\end{prop}
	
	\begin{proof}
	Por aplicação repetida do metateorema de dedução, sabendo que $\Gamma \vDash \varphi$ obtemos que $\vDash \gamma_1 \imply (\gamma_2 \imply \dots (\gamma_n \imply \varphi) \dots )$. Assim sendo, esta última fórmula é uma tautologia, e então um axioma. Como tal, considere-se a seguinte demonstração, com base em $\Gamma$:
	\begin{align*}
	&\gamma_1 \imply (\gamma_2 \imply (\gamma_3 \imply \dots (\gamma_n \imply \varphi) \dots )) &&\texttt{Ax}\\
	&\gamma_1 &&\texttt{Hip}\\
	&\gamma_2 \imply (\gamma_3 \imply \dots (\gamma_n \imply \varphi) \dots ) &&\texttt{MP}_{12}\\
	&\gamma_2 &&\texttt{Hip}\\
	&\gamma_3 \imply ( \dots (\gamma_n \imply \varphi) \dots)  &&\texttt{MP}_{34}\\
	&\vdots&&\vdots\\
	&\gamma_n \imply \varphi &&\texttt{MP}_{\dots}\\
	&\gamma_n &&\texttt{Hip}\\
	&\varphi &&\texttt{MP}_{\dots}
	\end{align*}
	
	Isto é uma demonstração com todas as suas hipóteses em $\Gamma$, cuja conclusão é $\varphi$, pelo que $\Gamma \vdash \varphi$ como desejado.
	\end{proof}
	
	Isto mostra que o maior obstáculo para a completude é a possibilidade de haver infinitas hipóteses.
	
	Para demonstrar a completude no caso geral, uma demonstração direta não é um bom plano de ataque. De facto, para mostrar que $\Gamma \vDash \varphi$ implica $\Gamma \vdash \varphi$ seria necessário partir de uma hipótese sobre o conjunto das valorações, e, a partir disso, construir uma demonstração. O problema é que as valorações não nos dizem grande coisa sobre como poderíamos construir a demonstração.
	
	Recorremos então ao princípio do contrarrecíproco. Isto é, provaremos a afirmação equivalente: $\Gamma \nvdash \varphi$ implica $\Gamma \nvDash \varphi$. Partimos do princípio que não existe uma demonstração de $\varphi$ a partir de $\Gamma$ e pretendemos construir uma valoração $\rho$ tal que $\rho \Vdash \Gamma$ mas $\rho \nVdash \varphi$. Felizmente, a construção de valorações é um processo razoavelmente simples, visto que é como resolver equações. Cada $\gamma \in \Gamma$ dá-nos uma `equação que $\rho$ tem que satisfazer', e juntando a estas a condição que $\rho$ satisfaça $\neg \varphi$, ficamos com um `sistema com infinitas incógnitas' ($\rho(x)$, $\rho(y)$, \dots), e a nossa esperança seria que, de alguma forma, a hipótese que não há demonstrações de $\Gamma \vdash \varphi$ fosse em algum sentido equivalente a `este sistema não é impossível'.
	
	A ideia será construir $\rho$ de forma gananciosa, atribuindo valores às variáveis sequencialmente. Cada variável nova que atribuímos `adiciona uma condição a $\Gamma$', e se nos certificarmos a cada passo que não quebramos a condição $\Gamma \nvdash \varphi$, no final acabaremos com a valoração desejada.
	
	Seguem-se os detalhes formais.
	
	\begin{teorema} (Completude do cálculo proposicional)
	Sejam $\Gamma \subseteq \F_p$ e $\varphi \in \F_p$ tal que $\Gamma \nvdash \varphi$. Então, $\Gamma \nvDash \varphi$.
	\end{teorema}
	
	\begin{proof}
	Esta demonstração é bastante elaborada, pelo que alguns detalhes são remetidos para lemas que serão demonstrados no final.
	
	Construiremos indutivamente uma valoração $\rho$ tal que $\rho \Vdash \Gamma$ e $\rho \nVdash \varphi$. Para o fazer, pressupomos uma enumeração $\bf x_1, \bf x_2, \dots$ de $X$. (Usamos aqui que o conjunto das variáveis, $X$, é contável!)
	
	Para definir $\rho(\bf x_1)$, averigue-se se $\Gamma, \bf x_1 \vdash \varphi$. Isto é: será que adicionando a afirmação `$\bf x_1$ é verdadeiro' às nossas hipóteses conseguimos demonstrar $\varphi$?
	
	\begin{itemize}
	\item Em caso afirmativo, o nosso $\rho$ não poderá atribuir $\rho(\bf x_1) = \lt$, ou então o `sistema' a resolver ($\rho \nVdash \varphi$) ficava impossível! Assim sendo, definimos $\rho(\bf x_1) = \lf$. 
	
	\item Em caso negativo, não haverá problema em definir $\rho(\bf x_1) = \lt$.
	\end{itemize}
	
	Podemos agora definir $\rho(\bf x_2)$, tendo em atenção que $\rho(\bf x_1)$ está já definido, o que condiciona a nossa escolha. Assim sendo, averiguamos se
	\[\Gamma, r(\bf x_1), \bf x_2 \vdash \varphi,\]
	onde $r(\bf x_1)$ é a fórmula $\bf x_1$ ou $\neg \bf x_1$ dependendo de como definimos $\rho(\bf x_1)$.
	
	Continuamos o processo, definindo $\rho(\bf x_n)$ para todo $n$, seguindo a regra:

	\begin{center}
	Se $\Gamma, r(\bf x_1), \dots, r(\bf x_{n-1}), \bf x_n \vdash \varphi$, definimos $\rho(\bf x_n) = \lf$. Caso contrário, $\rho(\bf x_n) = \lt$.
	\end{center}
	
	Resta agora mostrar que esta valoração $\rho$ satisfaz o desejado. Isto é, pretendemos usá-la para justificar que $\Gamma \nvDash \varphi$, pelo que há duas coisas a mostrar:
	
	\begin{itemize}
	\item Se $\gamma \in \Gamma$ então $\rho \Vdash \gamma$,
	
	\item $\rho \nVdash \varphi$.
	\end{itemize}
	
	Antes de o fazer, mostramos um facto auxiliar:
	\begin{equation}\label{eq:comp:prop:-1}
	\Gamma, r(\bf x_1), r(\bf x_2), \dots \nvdash \varphi.
	\end{equation}
	
	Para mostrar isto, começamos por fazer uma observação que nos será muito útil no futuro: as provas são finitas. Como tal, existe um número finito de hipóteses que se podem invocar, pelo que, assumindo por absurdo que
	\[\Gamma, r(\bf x_1), r(\bf x_2), \dots \vdash \varphi,\]
	haveria algum $N$ tal que $\Gamma, r(\bf x_1), r(\bf x_2), \dots, r(\bf x_N) \vdash \varphi$. Mostramos por indução em $N$ que tal não pode acontecer, isto é
	\begin{equation}\label{eq:comp:prop:0}
	\text{Para todo $N \in \N_0$, } \Gamma, r(\bf x_1), r(\bf x_2), \dots r(\bf x_N) \nvdash \varphi.
	\end{equation}
	
	\begin{itemize}
	\item Caso base: $N = 0$. Neste caso, a afirmação que se deseja provar reduz-se a $\Gamma \nvdash \varphi$, que é verdadeira por hipótese.
	
	\item Suponha-se verdade para algum $N$, mostre-se verdade para $N+1$. Há dois casos.
	
	Se $\rho(\bf x_{N+1}) = \lt$, então por construção isso significa que
	\[\Gamma, r(\bf x_1), \dots, r(\bf x_N), \bf x_{N+1} \nvdash \varphi,\]
	que, como neste caso $r(\bf x_{N+1})$ é $\bf x_{N+1}$, mostra o desejado.
	
	Se $\rho(\bf x_{N+1}) = \lf$, então, por construção, temos que
	\begin{equation}\label{eq:comp:prop:1}
	\Gamma, r(\bf x_1), \dots, r(\bf x_N), \bf x_{N+1} \vdash \varphi.
	\end{equation}
	
	Faremos os detalhes mais tarde (ver lema \ref{lema:1}), mas a ideia é a seguinte: supondo, por absurdo, que
	\begin{equation}\label{eq:comp:prop:2}
	\Gamma, r(\bf x_1), \dots, r(\bf x_N), \neg \bf x_{N+1} \vdash \varphi,
	\end{equation}
	ter-se-ia, juntando \eqref{eq:comp:prop:1} e \eqref{eq:comp:prop:2},
	\[\Gamma, r(\bf x_1), \dots, r(\bf x_N), \bf x_{N+1} \lor \neg \bf x_{N+1} \vdash \varphi.\]
	
	Sabendo que a hipótese $\bf x_{N+1} \lor \neg \bf x_{N+1}$ é uma tautologia (verifique), pode ser retirada das hipóteses (basta substituir qualquer sua invocação como \texttt{Hip} por uma invocação por \texttt{Ax}), pelo que concluímos
	\[\Gamma, r(\bf x_1), \dots, r(\bf x_N) \vdash \varphi,\]
	uma contradição com a hipótese de indução. Conclui-se então que
	\[\Gamma, r(\bf x_1), \dots, r(\bf x_N), \neg \bf x_{N+1} \nvdash \varphi,\]
	como desejado.
	\end{itemize}
	
	Podemos agora mostrar $\rho \nVdash \varphi$. Para o fazer, usaremos um facto que terá que ser provado em detalhe mais tarde (ver lema \ref{lema:3}).
	
	A ideia é que, supondo que $\rho \Vdash \varphi$, das hipóteses $r(\bf x_1), r(\bf x_2), \dots$ consegue-se concluir $\varphi$. Simbolicamente,
	\[r(\bf x_1), r(\bf x_2), \dots \vdash \varphi.\]
	
	Intuitivamente, isto é verdade porque as nossas hipóteses permitem-nos `substituir cada variável pelo seu valor de verdade dado por $\rho$', e a informação $\rho \Vdash \varphi$ permite-nos concluir que `ao avaliar a expressão resultante, obtemos $\lt$'. Claro que isto não é uma demonstração, mas será talvez suficiente para o leitor aceitar o facto como plausível.
	
	Este facto é útil porque, sob esta hipótese, ter-se-ia (visto que não há problema em adicionar hipóteses) que
	\[\Gamma, r(\bf x_1), r(\bf x_2), \dots \vdash \varphi,\]
	o que entra em contradição com \eqref{eq:comp:prop:0}.
	
	Mostre-se, agora, que para todo $\gamma \in \Gamma$ se tem $\rho \Vdash \gamma$. Usando novamente o lema \ref{lema:3}, temos que, assumindo por absurdo $\rho \nVdash \gamma$, ter-se-ia $\rho \Vdash \neg \gamma$ e como tal
	\begin{equation}\label{eq:comp:prop:3}
	r(\bf x_1), r(\bf x_2), \dots \vdash \neg \gamma.
	\end{equation}
	
	Assim sendo, chegamos às seguintes duas conclusões:
	\begin{gather}
	\Gamma, r(\bf x_1), r(\bf x_2), \dots \vdash \neg \gamma\label{shownotgamma}\\
	\Gamma, r(\bf x_1), r(\bf x_2), \dots \vdash \gamma\label{showgamma}
	\end{gather}
	a primeira por \eqref{eq:comp:prop:3}, e a segunda porque $\gamma \in \Gamma$. Recorremos agora ao chamado \emph{princípio da explosão}.
	
	Concatenando as demonstrações de \eqref{shownotgamma} e \eqref{showgamma} e ajustando índices consoante necessário, temos uma demonstração que tem $\Gamma, r(\bf x_1), r(\bf x_2), \dots$ como hipóteses e que mostra, em algum momento, $\gamma$ e $\neg \gamma$.
	
	No fim desta demonstração, adicione-se a linha
	\[(\gamma \imply (\neg \gamma \imply \varphi), \texttt{Ax}\footnote{A verificação de que a fórmula invocada é de facto uma tautologia é deixada ao leitor.}),\]
	seguida de duas aplicações de \textit{modus ponens}, de modo a construir uma prova de
	\[
	\Gamma, r(\bf x_1), r(\bf x_2), \dots \vdash \varphi
	\]
	que é uma contradição com \eqref{eq:comp:prop:-1}.
	
	Isto termina a demonstração de $\rho \Vdash \Gamma$ e $\rho \nVdash \varphi$, pelo que $\Gamma \nvDash \varphi$, como desejado.
	\end{proof}
	
	Falta agora preencher alguns buracos tomados por garantidos nesta demonstração. Antes de o fazermos, mostramos que o cálculo proposicional satisfaz o metateorema da dedução, que nos será muito útil no que se segue. Note-se que, até menção em contrário, não usaremos a completude do cálculo proposicional, visto que, em rigor, ainda não acabámos a sua demonstração!
	
	\begin{prop}
	(Metateorema da dedução, versão sintática)
	
	Sejam $\alpha, \beta \in \F_p$, $\Gamma \subseteq \F_p$. Então
	
	\[\Gamma, \alpha \vdash \beta \text{ sse } \Gamma \vdash \alpha \imply \beta.\]
	\end{prop}
	\begin{proof}
	Uma das implicações é trivial. ($\leftarrow$) De facto, se $\Gamma \vdash \alpha \imply \beta$, então existe uma demonstração $D$ (de comprimento, digamos, $N$) de $\alpha \imply \beta$ partindo apenas de hipóteses em $\Gamma$. Considere-se então a seguinte demonstração com hipóteses em $\Gamma \cup \{\alpha\}$:
	
	\[[\dots D \dots], (\alpha, \texttt{Hip}), (\beta, \texttt{MP}_{N, N+1})\]
	
	(A invocação de \textit{modus ponens} é justificada porque a $N$-ésima afirmação da prova, isto é, a última afirmação de $D$, é $\alpha \imply \beta$ por hipótese.)
	
	Esta demonstração mostra, então, que $\Gamma, \alpha \vdash \beta$, como desejado.
	\smallskip
	A implicação no outro sentido é um pouco mais elaborada, e introduz a ideia de prova que é construção de demonstrações.
	
	No que se segue, usaremos a sigla MTD com o significado de `metateorema da dedução, versão semântica'.
	
	Vamos partir do princípio que temos uma demonstração de $\Gamma, \alpha \vdash \beta$:
	\[(\varphi_1, j_1), \dots, (\varphi_n, j_n)\]
	e vamos, a partir desta, construir uma demonstração de $\Gamma \vdash \alpha \imply \beta$. Mais concretamente, construiremos uma demonstração (de hipóteses em $\Gamma$)
	\[(\psi_1, j'_1), \dots, (\psi_N, j'_N)\]
	em que alguns dos $\psi$, digamos $\psi_{i_1}, \dots, \psi_{i_n}$ (este último coincidindo com $\psi_N$) correspondem respetivamente a $\alpha \imply \varphi_1, \dots, \alpha \imply \varphi_n$. Esta prova terminará então com $\alpha \imply \beta$ (visto que $\varphi_n$ é $\beta$), pelo que teremos a demonstração desejada de $\Gamma \vdash \alpha \imply \beta$.
	
	Exemplificamos o processo com os primeiros três passos. O primeiro passo da demonstração é $(\varphi_1, j_1)$.
	
	\begin{itemize}
	\item Se $j_1$ é \texttt{Ax}, isso significa que $\varphi_1$ é uma tautologia. Assim sendo, (por transitividade de $\vDash$ e usando a versão semântica do MTD) temos que $\alpha \imply \varphi_1$ é também uma tautologia, pelo que a nossa prova modificada pode simplesmente começar com
	\[(\alpha \imply \varphi_1, \texttt{Ax}).\]
	
	\item Se $j_1$ é \texttt{Hip}, temos que $\varphi_1 \in \Gamma \cup \{\alpha\}$, pelo que há dois casos a considerar.
	\begin{itemize}
	\item Se $\varphi_1$ é $\alpha$, então basta reparar que a afirmação $\alpha \imply \alpha$ é uma tautologia, pelo que podemos começar a prova modificada com
	\[(\alpha \imply \varphi_1, \texttt{Ax}).\]
	
	\item Se $\varphi_1 \in \Gamma$, é preciso provar, com apenas hipóteses em $\Gamma$, que $\alpha \imply \varphi_1$. Para isto, usamos o facto que a seguinte afirmação é uma tautologia: (Como pode ser demonstrado através de dois usos do MTD)
	\[(\varphi_1 \imply (\alpha \imply \varphi_1), \texttt{Ax})\]
	e continuamos a demonstração modificada com
	\begin{gather*}
	(\varphi_1, \texttt{Hip})\\
	(\alpha \imply \varphi_1, \texttt{MP}_{12})
	\end{gather*}
	\end{itemize}
	
	\item Não consideramos aqui o caso de $j_1 = \texttt{MP}$ por razões óbvias.
	\end{itemize}
	
	O mesmo raciocínio funciona para o segundo passo, e aliás qualquer passo cuja justificação seja \texttt{Ax} ou \texttt{Hip}. Falta agora considerar o caso de \texttt{MP}.
	
	Para propósitos ilustrativos, suponha-se que $j_3 = \texttt{MP}_{12}$. A prova modificada construida até agora terá sido
	\[\dots, (\alpha \imply (\varphi_2 \imply \varphi_3), j_{i_1}), \dots, (\alpha \imply \varphi_2, j_{i_2}).\]
	
	Para completar a prova, precisamos de justificar que de $\alpha \imply (\varphi_2 \imply \varphi_3)$ e $\alpha \imply \varphi_2$ conseguimos concluir $\alpha \imply \varphi_3$.  Para tal, é necessário usar o facto que a seguinte afirmação é uma tautologia:
	
	\[[\alpha \imply (\varphi_2 \imply \varphi_3)] \imply [(\alpha \imply \varphi_2) \imply (\alpha \imply \varphi_3)]\]
	coisa esta que pode ser demonstrada semanticamente através de uso generoso do MTD e MP.
	
	Sabendo este facto, é fácil continuar a demonstração modificada, adicionando as linhas:
	\begin{gather*}
	([\alpha \imply (\varphi_2 \imply \varphi_3)] \imply [(\alpha \imply \varphi_2) \imply (\alpha \imply \varphi_3)], \texttt{Ax})\\
	((\alpha \imply \varphi_2) \imply (\alpha \imply \varphi_3), \texttt{MP}_{i_2 + 1, i_1})\\
	(\alpha \imply \varphi_3, \texttt{MP}_{i_2 + 2, i_2})
	\end{gather*}
	
	Note-se que neste raciocínio focámo-nos nos primeiro, segundo e terceiro passos, mas é evidente que este argumento poderá ser aplicado a todos os passos. Continuando o processo até o $n$-ésimo passo, obtém-se a demonstração modificada desejada, o que conclui a prova.
	\end{proof}
	
	Tal como a sua correspondente versão semântica, a versão sintática do metateorema da dedução é instrumental para simplificar imensas provas. Exemplificamos com o seguinte lema, que foi usado na demonstração de completude.
	
	\begin{lema}\label{lema:1}
	Seja $\Gamma \subseteq \F_p$ e $\alpha, \beta, \varphi \in \F_p$. Suponha-se que $\Gamma, \alpha \vdash \varphi$ e $\Gamma, \beta \vdash \varphi$. Então,
	\[\Gamma, \alpha \lor \beta \vdash \varphi.\]
	\end{lema}
	
	\begin{proof}
	Suponha-se, por hipótese, que $\Gamma, \alpha \vdash \varphi$. Então, pelo metateorema da dedução sintático, que $\Gamma \vdash \alpha \imply \varphi$. Identicamente, temos que $\Gamma \vdash \beta \imply \varphi$.
	
	Chamemos $(\varphi_1, j_1), \dots, (\varphi_n, j_n)$ e $(\psi_1, j'_1), \dots, (\psi_m, j'_m)$ às respetivas provas. Concatenando as duas (e ajustando índices de \texttt{MP}'s quando necessário) obtemos uma prova cuja $n$-ésima entrada é $\alpha \imply \varphi$ e cuja $(n+m)$-ésima entrada é $\beta \imply \varphi$.
	
	Adicionando a esta prova a linha
	\[((\alpha \imply \varphi) \imply [(\beta \imply \varphi) \imply ((\alpha \lor \beta) \imply \varphi)], \texttt{Ax})\]
	(ver lema \ref{lemaou} para justificar que esta é uma tautologia) seguida de duas aplicações de MP, obtemos uma demonstração, com hipóteses em $\Gamma$, de $(\alpha \lor \beta) \imply \varphi$. Mais uma aplicação do MTD sintático dá-nos
	\[\Gamma, \alpha \lor \beta \vdash \varphi,\]
	como desejado.
	\end{proof}
	
	\begin{lema}\label{lema:3}
	Seja $\bf x_1, \bf x_2, \dots$ uma enumeração das variáveis, $\rho$ uma valoração e $\varphi$ uma fórmula. Defina-se, para $i = 1, 2, \dots$, $r(\bf x_i)$ como sendo a fórmula $\bf x_i$ se $\rho(\bf x_i) = \lt$ e $\neg \bf x_i$ se $\rho(\bf x_i) = \lf$. Sob estas hipóteses,
	\[\rho \Vdash \varphi \text{ sse } r(\bf x_1), r(\bf x_2), \dots \vdash \varphi.\]
	\end{lema}
	
	\begin{proof}
	($\leftarrow$) Pela correção do cálculo proposicional, se
	\[r(\bf x_1), r(\bf x_2), \dots \vdash \varphi\]
	então
	\[r(\bf x_1), r(\bf x_2), \dots \vDash \varphi.\]
	
	É fácil verificar que $\rho \Vdash r(\bf x_i)$ para todo $i \in \N^+$, pelo que, por definição de consequência semântica, $\rho \Vdash \varphi$, como desejado.
	
	\medskip
	
	($\rightarrow$) Para provar esta implicação, começamos por reparar que o valor de $\overline\rho(\varphi)$ depende apenas do valor de $\rho(\bf x)$ para um número finito de variáveis $\bf x$. Isto pode ser provado diretamente, ou pode ser feito usando o facto que $\overline\rho(\varphi)$ depende apenas de $\rho|_{\var \varphi}$ e que o conjunto $\var \varphi$ é finito (em ambos os casos, a justificação pode ser concretizada com indução na estrutura da fórmula). Assim sendo, existe algum $N$ tal que $\overline\rho(\varphi)$ depende apenas de $\rho(\bf x_1), \dots, \rho(\bf x_N)$. Como tal, partindo do princípio que $\rho \Vdash \varphi$, qualquer valoração $\rho'$ que concorde com $\rho$ nestas variáveis também satisfará $\rho' \Vdash \varphi$.
	
	Repare-se agora que qualquer valoração $\rho'$ que satisfaz
	\[\rho' \Vdash r(\bf x_1), \dots, r(\bf x_N)\]
	está precisamente nestas condições, pelo que $\rho' \Vdash \varphi$. Assim sendo, conclui-se que
	\[r(\bf x_1), \dots, r(\bf x_N) \vDash \varphi.\]
	
	Pela completude do cálculo proposicional no caso finito, obtém-se
	\[r(\bf x_1), \dots, r(\bf x_N) \vdash \varphi,\]
	e visto que não há problema em adicionar hipóteses, temos
	\[r(\bf x_1), r(\bf x_2), \dots \vdash \varphi\]
	como desejado.
	\end{proof}
	
	Concluídas as demonstrações dos lemas necessários, damos por terminada a demonstração da completude do cálculo. Isto é: a partir de tautologias e \textit{modus ponens}, é possível demonstrar sintaticamente qualquer afirmação da forma $\Gamma \vDash \varphi$!
	
	Isto tem uma consequência interessante, que não tem demonstração fácil que não passe pela noção de sintática.
	
	\begin{prop}
	Seja $\Gamma \subseteq \F_p$ e $\varphi \in \F_p$. Então, se $\Gamma \vDash \varphi$ existe um subconjunto finito $\Gamma'$ de $\Gamma$ tal que $\Gamma' \vDash \varphi$.
	
	Por outras palavras, se $\varphi$ é consequência semântica das hipóteses em $\Gamma$, é possível arranjar um conjunto finito de hipóteses a partir dos quais ainda se conclui $\varphi$.
	\end{prop}
	
	\begin{proof}
	Suponha-se que $\Gamma \vDash \varphi$. Então, pela completude do cálculo proposicional, $\Gamma \vdash \varphi$, ou seja, existe uma demonstração de $\varphi$ com hipóteses em $\Gamma$. Como demonstrações são finitas, é possível apenas invocar um número finito de hipóteses. Assim sendo, se $\Gamma' \subseteq \Gamma$ é o conjunto de hipóteses invocadas nesta demonstração, temos $\Gamma' \vdash \varphi$, pelo que, por correção, $\Gamma' \vDash \varphi$.
	\end{proof}
	
	Note-se que o enunciado da proposição anterior não menciona o cálculo proposicional nem a noção de provabilidade.
	
	\section{Axiomática}
	
	Tivemos até agora bastante sucesso com o cálculo proposicional. Mostrámos que, partindo apenas do nosso conhecimento de tautologias e \textit{modus ponens}, é possível caracterizar completamente a relação de consequência semântica. Nesta secção, reduziremos o conjunto dos axiomas.
	
	Começamos por mostrar que para um conjunto de axiomas nos dar um cálculo correto e completo é necessário e suficiente que, em certo sentido, chegue para descrever as tautologias todas.
	
	\begin{prop}\label{axiomconditions}
	Seja $A$ um conjunto de fórmulas. Então, o cálculo proposicional usando $A$ como axiomas é
	
	\begin{itemize}
	\item Correto sse todos os elementos de $A$ são tautologias
	
	\item Completo sse para qualquer tautologia $\varphi$ existe uma demonstração (sem hipóteses) de $\varphi$ com os axiomas de $A$.
	\end{itemize} 
	\end{prop}
	
	\begin{proof}
	Há quatro implicações a provar.
	
	(Correção $\imply$ Todos os elementos de $A$ são tautologias) Esta implicação é trivial.
	
	(Todos os elementos de $A$ são tautologias $\imply$ Correção) A demonstração de correção feita em \ref{prop:correcao} aplica-se sem modificação.
	
	(Completude $\imply$ Qualquer tautologia é possível de provar) Esta implicação é trivial.
	
	(Qualquer tautologia é possível de provar $\imply$ Completude) Já mostrámos que sempre que $\Gamma \vDash \varphi$ existe uma demonstração de $\varphi$ com hipóteses em $\Gamma$ \emph{que usa as tautologias como axiomas}. Basta então mostrar que, sob estas hipóteses, é sempre possível substituir uma invocação de uma tautologia por uma `mini-demonstração' desta.
	
	Considere-se uma prova (no sentido de $A$ = tautologias) no meio da qual se invoca uma tautologia $\varphi$ que não esteja em $A$. Digamos que esta prova é da forma
	\[ [\dots D_1 \dots], (\varphi, \texttt{Ax}), [\dots D_2 \dots].\]
	
	Suponha-se que $[\dots C \dots]$ é uma demonstração de $\varphi$ com axiomas em $A$. Esta demonstração existe por hipótese. Então, concatenando demonstrações e ajustando índices, temos que
	\[ [\dots D_1 \dots] [\dots C \dots] [\dots D_2 \dots] \]
	é uma demonstração, com a mesma conclusão que a original, da qual foi removida uma invocação de um axioma que não está em $A$. O processo pode ser repetido até todas as tais invocações serem removidas. No final, a demonstração obtida é uma demonstração com a mesma conclusão que a original mas que só usa axiomas em $A$.
	\end{proof}
	
	O nosso problema fica então reduzido a arranjar um conjunto $A$ de tautologias, se possível `razoavelmente pequeno', a partir do qual se consiga demonstrar todas as tautologias. Para investigar este problema é razoável investigar as ferramentas que usámos para mostrar tautologias. De facto, uma ferramenta que nos foi útil em muitos casos é o metateorema da dedução. Como tal, é de esperar que o nosso conjunto $A$ tenha axiomas suficientes para o mostrar. Por inspeção da demonstração da versão sintática do MTD, vemos que as únicas tautologias que são realmente necessárias são:
	
	\begin{enumerate}
	\item Se $\varphi_1$ é um axioma, é necessário $\alpha \imply \varphi_1$
	
	\item $\alpha \imply \alpha$
	
	\item $\varphi_1 \imply (\alpha \imply \varphi_1)$
	
	\item $[\alpha \imply (\varphi_2 \imply \varphi_3)] \imply [(\alpha \imply \varphi_2) \imply (\alpha \imply \varphi_3)]$.
	\end{enumerate}
	
	Note-se que a condição 1 é trivialmente possível de demonstrar a partir de 3. Assim sendo, basta assegurar as tautologias
	\begin{gather}
	\alpha \imply \alpha\label{ax0}\\
	\alpha \imply (\beta \imply \alpha)\label{ax1}\\
	[\alpha \imply (\beta \imply \gamma)] \imply [(\alpha \imply \beta) \imply (\alpha \imply \gamma)]\label{ax2}
	\end{gather}
	para todas as fórmulas $\alpha, \beta, \gamma$.
	
	Mostramos agora que só as últimas duas é que são necessárias.
	
	\begin{prop}
	A partir dos axiomas \eqref{ax1} e \eqref{ax2} é possível mostrar \eqref{ax0}.
	\end{prop}
	
	\begin{proof}
	Usamos esta oportunidade para exemplificar uma estratégia útil para construir demonstrações usando o cálculo proposicional, que é a ideia de `construir demonstrações a partir do fim'.
	
	No nosso caso, queremos construir uma demonstração de $\alpha \imply \alpha$. Não parece óbvio que seja possível chegar a esta afirmação aplicando o axioma \eqref{ax1}, pelo que tentamos aplicar \eqref{ax2}. Para a conclusão ser $\alpha \imply \alpha$, faz sentido considerar o caso particular de $\gamma$ igual a $\alpha$. Ficamos com a seguinte instância:
	\[[\alpha \imply (\beta \imply \alpha)] \imply [(\alpha \imply \beta) \imply (\alpha \imply \alpha)].\]
	
	A afirmação $\alpha \imply (\beta \imply \alpha)$ é um dos axiomas que estamos a considerar, pelo que basta escolher $\beta$ adequadamente de modo a que a afirmação $\alpha \imply \beta$ seja verdadeira. Novamente, aplicamos o axioma \eqref{ax1}, escolhendo $\beta$ igual a, digamos $\alpha \imply \alpha$, que faz com que $\alpha \imply \beta$, isto é, $\alpha \imply (\alpha \imply \alpha)$, seja uma instância de \eqref{ax1}.
	
	Chegamos então à seguinte demonstração:
	\begin{align*}
	&[\alpha \imply ((\alpha \imply \alpha) \imply \alpha)] \imply [(\alpha \imply (\alpha \imply \alpha)) \imply (\alpha \imply \alpha)]&&\texttt{Ax}\\
	&\alpha \imply ((\alpha \imply \alpha) \imply \alpha)&&\texttt{Ax}\\
	&(\alpha \imply (\alpha \imply \alpha)) \imply (\alpha \imply \alpha)&&\texttt{MP}_{1,2}\\
	&\alpha \imply (\alpha \imply \alpha)&&\texttt{Ax}\\
	&\alpha \imply \alpha&&\texttt{MP}_{3,4}
	\end{align*}
	\end{proof}
	
	Temos então um conjunto de apenas dois (!) axiomas\footnote{Tecnicamente, é um número contável infinito de axiomas, e não apenas dois.} a partir dos quais obtemos o metateorema da dedução. Repare-se, no entanto, que os nossos axiomas não falam do operador `não'. Assim sendo, não há muita esperança de conseguir demonstrar todas as tautologias; em particular aquelas que usam este operador.
	
	Um aspeto principal a considerar para conseguir mostrar todas as tautologias é que \emph{algo} tem que fazer a ligação entre valorações e demonstrações. Na nossa demonstração de completude, este papel coube ao lema \ref{lema:3}. No entanto, a sua demonstração requereu o uso da completude no caso finito, que por sua vez requer o conhecimento que é possível mostrar qualquer tautologia. Assim sendo, é necessário uma forma mais construtiva de demonstrar este lema.
	
	Recordando o enunciado: supomos fixa uma enumeração $\bf x_1, \bf x_2, \dots$ das variáveis e uma valoração $\rho$. Pretendemos mostrar que, se $\varphi$ é uma fórmula,
	\[\rho \Vdash \varphi \text{ sse } r(\bf x_1), r(\bf x_2), \dots \vdash \varphi,\]
	onde $r(\bf x_i)$ é $\bf x_i$ ou $\neg \bf x_i$ consoante $\rho(\bf x_i)$ é $\lt$ ou $\lf$. Visto que não temos ao nosso dispor a capacidade de fazer uma demonstração direta, podemos tentar fazer uma prova por indução na estrutura da fórmula. Para nos ajudar a fazer o passo de indução, provamos aliás uma coisa algo mais forte.
	
	\begin{lema}\label{ligacao:rho:vdash}
	Seja $\rho$ uma valoração, e defina-se $r : \F_p \to \F_p$ como 
	\[r(\varphi) =
	\begin{cases}
	\varphi&\rho\Vdash\varphi\\
	\neg\varphi&\rho\nVdash\varphi
	\end{cases}
	\]
	
	Definimos também $\Gamma_\rho = \{r(\bf x_1), r(\bf x_2), \dots\}$ para abreviar a notação. Então, para qualquer fórmula $\varphi$,
	\[\Gamma_\rho \vdash r(\varphi).\]
	
	Para mais, as únicas hipóteses necessárias são as da forma $r(\bf x)$ para $\bf x \in \var \varphi$.
	\end{lema}
	
	\begin{proof}
	Apesar de nos referirmos a isto como `um lema' e `uma demonstração', não é precisamente isso que se está a passar, visto que precisaremos de usar alguns axiomas que ainda não adicionámos a $A$. O propósito desta `demonstração' é, então, determinar precisamente de que axiomas novos precisaremos.
	
	A demonstração é feita por indução na estrutura da fórmula $\varphi$.
	
	\begin{itemize}
	\item O caso base é trivial.
	
	\item Se a afirmação é verdadeira para $\varphi$, mostramos que é verdade para $\neg \varphi$.
	
	\begin{itemize}
	\item Se $\rho \Vdash \neg \varphi$, então temos que $\rho \nVdash \varphi$. Pela hipótese de indução, temos que $\Gamma_\rho \vdash \neg \varphi$, que é precisamente a conclusão desejada.
	
	\item Se $\rho \nVdash \neg \varphi$, então $\rho \Vdash \varphi$, pelo que, por hipótese de indução, existe uma demonstração de $\varphi$. Para a transformar numa demonstração de $r(\neg \varphi)$, isto é, $\neg \neg \varphi$, é necessário que o seguinte seja um teorema:
	\[\alpha \imply \neg \neg \alpha.\]
	\end{itemize}
	
	Note-se que em ambos os casos não foram adicionadas hipóteses, pelo que as únicas hipóteses necessárias são, por hipótese de indução, aquelas em que $\bf x \in \var \varphi = \var(\neg \varphi)$.
	
	\item Se a afirmação é verdade para as afirmações $\varphi_1$ e $\varphi_2$, mostramos que é verdade para $\varphi_1 \imply \varphi_2$. Há dois casos a considerar.
	
	\begin{itemize}
	\item Se $\rho \Vdash \varphi_1 \imply \varphi_2$, então ou $\overline\rho(\varphi_1) = \lf$ ou $\overline\rho(\varphi_2) = \lt$.
	\begin{itemize}
	\item Se $\overline\rho(\varphi_1) = \lf$ então $\Gamma_\rho \vdash \neg \varphi_1$ por hipótese de indução. Precisamos, então, que a seguinte tautologia seja um axioma:
	\[\neg \varphi_1 \imply (\varphi_1 \imply \varphi_2).\]
	
	\item Se $\overline\rho(\varphi_2) = \lt$ então, por hipótese de indução, $\Gamma_\rho \vdash \varphi_2$. Neste caso, $\Gamma_\rho \vdash \varphi_1 \imply \varphi_2$ visto que $\varphi_2 \imply (\varphi_1 \imply \varphi_2)$ é um axioma.
	\end{itemize}
	
	\item Se $\rho \Vdash \neg (\varphi_1 \imply \varphi_2)$, então $\overline\rho(\varphi_1) = \lt$ e $\overline\rho(\varphi_2) = \lf$. Assim sendo, por hipótese de indução, $\Gamma_\rho \vdash \varphi_1$ e $\Gamma_\rho \vdash \neg \varphi_2$. Precisamos então do axioma
	\[\varphi_1 \imply (\neg \varphi_2 \imply \neg (\varphi_1 \imply \varphi_2)).\]
	\end{itemize}
	
	Novamente, não foram adicionadas hipóteses, pelo que as únicas hipóteses necessárias são, por indução, aquelas em $\var \varphi_1 \cup \var \varphi_2 = \var \varphi$.
	\end{itemize}
	
	Necessitamos portanto de mostrar as seguintes três tautologias, todas usando o operador `não':
	\begin{gather}
	\alpha \imply \neg \neg \alpha\label{ax3}\\
	\neg \alpha \imply (\alpha \imply \beta)\label{ax4}\\
	\alpha \imply (\neg \beta \imply \neg (\alpha \imply \beta))\label{ax5}
	\end{gather}
	
	\end{proof}
	
	Repare-se que as fórmulas \eqref{ax4} e \eqref{ax5} podem facilmente ser reduzidas, usando o princípio do contrarrecíproco, a casos que já conseguimos mostrar. Isto é, suponha-se que temos ao nosso dispor os seguintes dois axiomas:
	\begin{gather}
	(\alpha \imply \beta) \imply (\neg \beta \imply \neg \alpha)\label{cr1}\\
	(\neg \alpha \imply \neg \beta) \imply (\beta \imply \alpha)\label{cr2}
	\end{gather}
	
	Então, \eqref{ax4} pode ser demonstrado facilmente, por uso do metateorema da dedução. Assumindo como hipótese $\neg \alpha$: (E supondo dado o axioma \eqref{cr2})
	\begin{align*}
	&\neg \alpha&&\texttt{Hip}\\
	&\neg \alpha \imply (\neg \beta \imply \neg \alpha)&&\texttt{Ax}\\
	&\neg \beta \imply \neg \alpha&&\texttt{MP}\\
	&(\neg \beta \imply \neg \alpha) \imply (\alpha \imply \beta)&&\texttt{Ax}\\
	&\alpha \imply \beta&&\texttt{MP}.
	\end{align*}
	
	Usando \eqref{cr1} como axioma, a fórmula \eqref{ax5} pode também ser demonstrada de forma semelhante. Isto sugere então o uso dos axiomas
	\begin{gather}
	\alpha \imply (\beta \imply \alpha) \tag{\ref{ax1}}\\
	[\alpha \imply (\beta \imply \gamma)] \imply [(\alpha \imply \beta) \imply (\alpha \imply \gamma)]\tag{\ref{ax2}}\\
	\alpha \imply \neg \neg \alpha \tag{\ref{ax3}}\\
	(\alpha \imply \beta) \imply (\neg \beta \imply \neg \alpha)\tag{\ref{cr1}}\\
	(\neg \alpha \imply \neg \beta) \imply (\beta \imply \alpha).\tag{\ref{cr2}}
	\end{gather}
	
	Acontece, no entanto, que, entre os últimos três, todos exceto \eqref{cr2} são redundantes. A justificação é elaborada, pelo que começamos por mostrar algo que parece, à primeira vista, apenas tangencial.
	
	Suponha-se que se deseja mostrar a afirmação $\neg \neg \alpha \imply \alpha$. De todos os cinco axiomas que temos até agora, o único que nos permite concluir algo sem negações a partir de hipóteses com negações é precisamente \eqref{cr2}.\footnote{Esta afirmação não é uma afirmação matemática, mas sim uma observação `empírica'.} Como tal, tentemos `des-aplicar \eqref{cr2}' de modo a descobrir o que é preciso mostrar para justificar $\neg\neg\alpha \imply \alpha$.
	\begin{gather*}
	(\neg\alpha \imply \neg\neg\neg\alpha) \imply (\neg\neg\alpha \imply \alpha)\\
	(\neg\neg\neg\neg\alpha \imply \neg\neg\alpha) \imply (\neg\alpha \imply \neg\neg\neg\alpha).
	\end{gather*}
	
	A primeira aplicação não nos dá algo em que se possa imediatamente mexer, mas após uma segunda aplicação, obtemos uma afirmação da forma $\beta \imply \neg\neg\alpha$. Nesta afirmação já é possível pegar, visto que, invocando o axioma \eqref{ax1}, temos $\neg\neg\alpha \imply (\beta \imply \neg\neg\alpha)$. Temos então material para construir a seguinte prova, que usa apenas os três axiomas \eqref{ax1}, \eqref{ax2} e \eqref{cr2}, juntamente com a hipótese $\neg\neg\alpha$.
	
	\begin{align*}
	&\neg\neg\alpha&&\texttt{Hip}\\
	&\neg\neg\alpha \imply (\neg\neg\neg\neg \alpha \imply \neg\neg\alpha)&&\texttt{Ax}\\
	&\neg\neg\neg\neg \alpha \imply \neg\neg\alpha&&\texttt{MP}\\
	&(\neg\neg\neg\neg \alpha \imply \neg\neg\alpha) \imply (\neg\alpha \imply \neg\neg\neg\alpha)&&\texttt{Ax}\\
	&\neg\alpha \imply \neg\neg\neg\alpha&&\texttt{MP}\\
	&(\neg\alpha \imply \neg\neg\neg\alpha)\imply(\neg\neg\alpha \imply \alpha)&&\texttt{Ax}\\
	&\neg\neg\alpha \imply \alpha&&\texttt{MP}\\
	&\alpha&&\texttt{MP}_{7,1}.
	\end{align*}
	
	Usando o MTD, é possível transformar isto numa demonstração de $\neg\neg\alpha \imply \alpha$.
	
	Estamos agora em posição de mostrar a redundância de \eqref{ax3} e \eqref{cr1}. Começamos primeiro por estabelecer uma proposição que temos andado a utilizar implicitamente.
	
	\begin{prop}
	Se a afirmação $\varphi$ é possível de demonstrar a partir do conjunto de axiomas $A$, então qualquer prova que use $\varphi$ como hipótese pode ser transformada numa prova que use apenas axiomas em $A$.
	\end{prop}
	
	\begin{proof}
	O raciocínio é idêntico ao usado no final da proposição \ref{axiomconditions}
	\end{proof}
	
	Como consequência desta proposição, dar-nos-emos ao luxo de, ao escrever algumas demonstrações, invocar teoremas já mostrados. Neste caso, invocá-los-emos com a justificação \texttt{Teo}. É de reparar, no entanto, que, tal como definições por abreviatura, uma demonstração em que usamos um teorema é suposto ser interpretada como uma \emph{abreviação de uma demonstração} usando apenas axiomas, hipóteses e \textit{modus ponens}!
	
	\begin{prop}
	Dados os cinco axiomas descritos acima, \eqref{ax3} e \eqref{cr1} são redundantes. Isto é, são ambos teoremas dos três restantes.
	\end{prop}
	
	\begin{proof}
	Comecemos por mostrar que a fórmula $\alpha \imply \neg\neg\alpha$ é um teorema dos três restantes. Infelizmente, não temos ao nosso imediato dispor ferramentas para `adicionar negações'. No entanto, o axioma \eqref{cr2} permite-nos trocar a ordem de implicações. `Des-aplicando' este axioma a $\alpha \imply \neg\neg\alpha$, obtemos
	\[(\neg\neg\neg\alpha \imply \neg\alpha) \imply (\alpha \imply \neg\neg\alpha).\]
	
	Note-se que $\neg\neg\neg\alpha \imply \neg\alpha$ é uma instância do teorema $\neg\neg\varphi \imply \varphi$, com $\varphi$ igual a $\neg \alpha$. Assim sendo, temos a seguinte prova:
	\begin{align*}
	&\neg\neg\neg\alpha \imply \neg\alpha&&\texttt{Teo}\\
	&(\neg\neg\neg\alpha \imply \neg\alpha) \imply (\alpha \imply \neg\neg\alpha)&&\texttt{Ax}\\
	&\alpha \imply \neg\neg\alpha&&\texttt{MP}.
	\end{align*}
	
	Isto mostra que \eqref{ax3} é redundante, pelo que basta agora mostrar que \eqref{cr1} também o é.
	
	A ideia é, partindo de $\alpha \imply \beta$, concluir $\neg \neg \alpha \imply \neg \neg \beta$, e depois, usando $\eqref{cr2}$, remover uma das negações. Começamos por mostrar $(\alpha \imply \beta) \imply (\neg\neg\alpha \imply \neg\neg\beta)$. Considere-se a seguinte demonstração:
	
	\begin{align*}
	&\alpha \imply \beta&&\texttt{Hip}\\
	&\neg\neg\alpha&&\texttt{Hip}\\
	&\neg\neg\alpha \imply \alpha&&\texttt{Teo}\\
	&\alpha&&\texttt{MP}\\
	&\beta&&\texttt{MP}\\
	&\beta \imply \neg\neg\beta&&\texttt{Teo}\\
	&\neg\neg\beta&&\texttt{MP}
	\end{align*}
	
	Através de duas aplicações do metateorema da dedução, obtemos que
	\[(\alpha \imply \beta) \imply (\neg\neg\alpha \imply \neg\neg\beta)\]
	é um teorema.
	
	O axioma \eqref{cr2} diz-nos
	\[
	(\neg\neg\alpha \imply \neg\neg\beta) \imply (\neg \beta \imply \neg \alpha)
	\]
	pelo que é fácil construir, através do MTD, uma demonstração de
	\[(\alpha \imply \beta) \imply (\neg \beta \imply \neg \alpha),\]
	como desejado. (Deixamos esta última parte como exercício.)
	\end{proof}
	
	Chegamos então à nossa lista \textbf{final} de axiomas $A$:
	
	\begin{gather}\label{prop:axiomasfinais}
	\alpha \imply (\beta \imply \alpha) \tag{\ref{ax1}}\\
	[\alpha \imply (\beta \imply \gamma)] \imply [(\alpha \imply \beta) \imply (\alpha \imply \gamma)]\tag{\ref{ax2}}\\
	(\neg \alpha \imply \neg \beta) \imply (\beta \imply \alpha).\tag{\ref{cr2}}
	\end{gather}
	
	Ainda não mostrámos, claro, que estes são suficientes. É nisso que consiste a seguinte proposição.
	
	\begin{teorema}
	A lista de axiomas $A$ acima é suficiente para mostrar qualquer tautologia. Como tal, visto que todos eles são tautologias, o cálculo proposicional que tem a lista $A$ como axiomas é correto e completo, pela proposição \ref{axiomconditions}.
	\end{teorema}
	
	\begin{proof}
	Seja $\varphi$ uma tautologia, e sejam $\bf x_1, \dots, \bf x_n$ as suas variáveis. Então, o lema \ref{ligacao:rho:vdash} diz-nos que, para qualquer valoração $\rho$, definindo $r$ como neste lema,
	\[r(\bf x_1), \dots, r(\bf x_n) \vdash \varphi,\]
	pois $r(\varphi)$ é sempre $\varphi$. Isto implica, por aplicação repetida do MTD,
	\[\vdash r(\bf x_1) \imply ( \dots (r(\bf x_n) \imply \varphi) \dots ).\]
	
	Como isto é para qualquer valoração $\rho$, temos os dois seguintes teoremas:
	\begin{gather*}
	\bf x_1 \imply ( r(\bf x_2) \imply (\dots (r(\bf x_n) \imply \varphi) \dots ))\\
	\neg \bf x_1 \imply ( r(\bf x_2) \imply (\dots (r(\bf x_n) \imply \varphi) \dots ))
	\end{gather*}
	
	Afirmamos que a seguinte fórmula é um teorema:
	\begin{equation}\label{eq:aornota}
	(\alpha \imply \psi) \imply ((\neg \alpha \imply \psi) \imply \psi).
	\end{equation}
	
	Esta afirmação é deixada como lema (lema \ref{lema:aornota}) de modo a não quebrar a prova.
	
	O teorema \eqref{eq:aornota} é-nos útil porque nos permite eliminar as hipóteses $r(\bf x_i)$ e então concluir $\vdash \varphi$. Concretamente, considere-se a seguinte prova, onde $r(\bf x_2) \imply (\dots (r(\bf x_n) \imply \varphi) \dots )$ é abreviado a $\psi$.
	\begin{align*}
	&\bf x_1 \imply \psi&&\texttt{Teo}\\
	&\neg \bf x_1 \imply \psi&&\texttt{Teo}\\
	&(\bf x_1 \imply \psi) \imply ((\neg \bf x_1 \imply \psi) \imply \psi)&&\texttt{Teo}\\
	&(\neg \bf x_1 \imply \psi) \imply \psi&&\texttt{MP}\\
	&\psi&&\texttt{MP}.
	\end{align*}
	
	Logo, para qualquer valoração $\rho$, a fórmula
	\[r(\bf x_2) \imply (\dots (r(\bf x_n) \imply \varphi) \dots )\]
	é um teorema. Ora, o mesmo raciocínio pode agora ser aplicado para eliminar a hipótese $r(\bf x_2)$, e assim por diante, para concluir, como desejado, que a tautologia $\varphi$ é também um teorema. Isto conclui a demonstração.
	\end{proof}
	
	Isto quase conclui a nossa exploração do sistema proposicional, faltando apenas provar o seguinte lema:
	
	\begin{lema}\label{lema:aornota}
	A seguinte fórmula é um teorema:
	\[(\alpha \imply \psi) \imply ((\neg \alpha \imply \psi) \imply \psi)\]
	\end{lema}
	
	\begin{proof}
	Comecemos com as hipóteses $\alpha \imply \psi$ e $\neg \alpha \imply \psi$. A partir da primeira, usando o princípio do contrarrecíproco, obtemos $\neg \psi \imply \neg \alpha$. Da segunda, conclui-se $\neg \psi \imply \neg \neg \alpha$. Logo, adicionando a hipótese $\neg \psi$, concluímos $\neg \alpha$ e $\neg\neg\alpha$.
	
	É razoavelmente intuitivo que a partir de factos contraditórios seja possível concluir qualquer coisa. E de facto, aplicando o princípio do contrarrecíproco ao axioma
	\[\neg \alpha \imply (\beta \imply \neg \alpha)\]
	é possível concluir que, para qualquer $\beta$,
	\[\neg \alpha \imply (\neg \neg \alpha \imply \neg \beta).\]
	
	Assim sendo, juntando os ingredientes que já temos, a partir das hipóteses
	\begin{gather*}
	\alpha \imply \psi\\
	\neg \alpha \imply \psi\\
	\neg \psi
	\end{gather*}
	é possível concluir $\neg \beta$, onde $\beta$ é uma fórmula arbitrária. Aplicando o MTD, obtemos
	\[\alpha \imply \psi, \neg \alpha \imply \psi \vdash \neg \psi \imply \neg \beta,\]
	e aplicando o princípio do contrarrecíproco mais uma vez
	\[\alpha \imply \psi, \neg \alpha \imply \psi \vdash \beta \imply \psi.\]
	
	Note-se que $\beta$ permanece uma fórmula arbitrária. Se escolhermos $\beta$ igual a, digamos, um axioma, é claro que de $\beta \imply \psi$ se conclui $\psi$, pelo que, finalmente,
	\[\alpha \imply \psi, \neg \alpha \imply \psi \vdash \psi.\]
	
	Duas aplicações do MTD permitem-nos concluir o teorema desejado.
	\end{proof}
	
	
	\chapter{Lógica de primeira ordem}
	
	A lógica proposicional é uma forma de relacionar afirmações. No entanto, é muito pouco expressiva. Não é possível, por exemplo, expressar afirmações sobre grupos ou números naturais na lógica proposicional.
	
	É neste contexto que surge a lógica de primeira ordem, que se aproxima mais do tipo de raciocínios que estamos habituados a fazer. Para além de manipular afirmações, temos a capacidade de manipular valores. Isto é, uma variável $x$, em vez de representar uma afirmação, representa agora um elemento do universo. Neste contexto, as hipóteses representam axiomas do universo de discurso (por exemplo, os axiomas de grupo para falar sobre grupos, ou os axiomas de Peano (ver adiante) para falar sobre números naturais).
	
	O poder expressivo adicional vem ao custo que o sistema é razoavelmente mais complexo. Por exemplo, em lógica de primeira ordem há dois tipos de expressões: expressões que simbolizam valores (e.g. $x + f(y)$, chamamos a estas termos) e expressões que simbolizam afirmações (e.g. $x = y \lor x < y$, chamamos a estas fórmulas). Para além disto, é necessário surgir a noção de assinatura, para identificar que símbolos e operações temos ao nosso dispor (na teoria dos naturais temos $+$ e $\times$, mas em teoria de grupos temos apenas uma operação $\cdot$) e a noção de valoração tem de ser alargada para permitir universos diversos. Pelo lado positivo, a lógica de primeira ordem é suficiente para expressar os axiomas de teoria de conjuntos, na qual é possível construir maior parte da matemática. Assim sendo, para estudar o raciocínio matemático em geral, é muito satisfatório estudar esta lógica.
	
	O objetivo deste capítulo é generalizar os resultados do capítulo anterior para este novo contexto. Por outras palavras, pretendemos descobrir um conjunto de regras de dedução que nos permita provar afirmações de primeira ordem de forma correta e completa. Isto é, queremos um conjunto de regras que nos permita concluir apenas afirmações verdadeiras, mas que nos deixe concluir \emph{todas} as afirmações verdadeiras.
	
	Um par de observações antes de iniciar a teoria:
	
	\begin{obs}
	O leitor poderá questionar porque é que esta se chama `lógica de \emph{primeira} ordem'. De facto, isto sugere a existência de lógicas de segunda ordem, terceira ordem, etc. Aquilo que distingue lógica de primeira ordem é o conceito de quantificador, isto é, os quantificadores $\forall$ e $\exists$. Na lógica de primeira ordem, estes podem ser usados para quantificar \emph{variáveis do universo}. Isto é importante, porque não nos permite expressar, por exemplo, o axioma do supremo no contexto da teoria dos reais. De facto, o axioma do supremo diz:
	
	\begin{center}
	Para qualquer conjunto de reais $X$ tal que $X \neq \emptyset$ e $X$ é majorado, existe um número real $m$ tal que $m$ é majorante de $X$ e para qualquer majorante $m'$ de $X$, $m' \geq m$.
	\end{center}
	
	Note-se que este axioma requer quantificar em `para qualquer \textbf{conjunto de reais}'. `Conjuntos de reais' não são elementos do universo $\R$, pelo que este axioma não pode ser expresso em lógica de primeira ordem.
	
	O conceito de lógica de segunda ordem provém da possibilidade de quantificar sobre conjuntos de elementos do universo. Lógica de terceira ordem permite quantificar sobre conjuntos de conjuntos de elementos do universo, e assim por diante. Estas lógicas de ordem superior a um não são frequentemente usadas porque têm algumas propriedades desagradáveis. Em particular, não existe nenhuma generalização do teorema de completude. Por outras palavras, não existe nenhum sistema razoável de demonstrações que nos permita demonstrar tudo o que é verdade em lógica de segunda ordem.
	\end{obs}
	
	\begin{obs}
	Note-se que as nossas variáveis representam sempre objetos do universo de discurso, e este universo é único. Isto não permite, por exemplo, expressar os axiomas de espaços vetoriais, visto que nestes espaços há dois `tipos de objetos': vetores e escalares. Existem lógicas que permitem lidar com isto, mas estas não estão no âmbito da cadeira de Lógica.
	\end{obs}
	
	\section{Fórmulas}
	
	Antes de podermos definir as expressões que vamos eventualmente manipular, é preciso estabelecer que operações temos ao nosso dispor.
	
	Novamente, consideramos fixo o conjunto $X$ das variáveis. Suponha-se dado um conjunto $F$ de \emph{símbolos de funções}. Isto são apenas símbolos sem significado, que usaremos para escrever expressões do género $f(x)$. Associamos também a cada símbolo $f \in F$ uma \emph{aridade}, isto é, quantos argumentos a função que será denotada por $f$ toma. A aridade de um símbolo de função é um número $\ar(f) \in \N_0$. Observe-se a possibilidade de um símbolo de função ter aridade zero. Isto corresponde a ser uma função sem argumentos. Em linguagens de programação, este tipo de funções aparece na forma $f()$. Para os nossos propósitos, funções de aridade zero correspondem a constantes, por exemplo, \textbf{0}, \textbf{1} ou a matriz identidade $\mathbf{I}$.
	
	Estamos agora em condições de definir o conjunto dos termos. Isto é, o conjunto das expressões que representam valores.
	
	\begin{definicao}
	O conjunto dos \emph{termos em $F$}, onde $F$ é um conjunto de símbolos de funções com aridades associadas, é representado como $\T_F$ e é definido indutivamente como:
	
	\begin{itemize}
	\item Se $\bf x$ é uma variável, $\bf x$ é um termo. (Chamamos a estes termos atómicos.)
	
	\item Se $f \in F$ é um símbolo de função com aridade $n$ e $t_1,\dots,t_n$ são termos, a seguinte expressão é um termo:\footnote{Recordamos o leitor que adotamos a convenção de interpretar expressões como sendo árvores sintáticas. Ver página \pageref{intro_syntatic_trees}.}
	
	\begin{center}
	\Tree [.$f$ $t_1$ $\dots$ $t_n$ ]
	\end{center}
	\end{itemize}
	
	Normalmente, esta expressão é representada como $f(t_1, \dots, t_n)$. Apontamos o caso particular de $n = 0$, no qual temos um símbolo de função constante. Em rigor, deveríamos representar este caso como $f()$, mas por conveniência e hábito omitimos os parênteses neste caso.
	
	Outra notação muito comum é no caso de símbolos de funções de aridade 2 que normalmente estamos habituados a escrever com notação \textit{infix}. Por exemplo, se estamos a escrever os axiomas dos números naturais, provavelmente escreveremos $x + y$ em vez de $+(x, y)$.
	
	Termos são normalmente representados pela letra $t$.
	\end{definicao}
	
	Antes de podermos construir expressões que comparam termos, é necessário estabelecer que comparações temos ao nosso dispor.
	
	Suponha-se dado um conjunto $P$ de \emph{símbolos de predicado}. Novamente, estes são símbolos sem significado, com uma aridade em $\N_0$ associada. Ao passo que os símbolos de função representavam aplicação de funções a termos, os símbolos de predicado representam, em certo sentido, `inspeção'. Exemplos de símbolos de predicado seriam o $=$ ou o $\leq$, ambos de aridade 2. Um exemplo de um símbolo de predicado de ordem 1 seria uma afirmação $p(n) : \text{`$n$ é par'}$, e um exemplo mais exótico seria um predicado $q(x, y, z)$ que diz se três números estão em progressão aritmética.
	
	\begin{definicao}
	Fixos um conjunto $F$ de símbolos de funções e um conjunto $P$ de símbolos de predicado, definimos o conjunto das \emph{fórmulas em $F$ e $P$}, representado como $\F_{FP}$, indutivamente da seguinte forma:
	
	\begin{itemize}
	\item Se $p \in P$ é um símbolo de predicado com aridade $n$ e $t_1,\dots,t_n$ são termos em $F$, a seguinte expressão é uma fórmula:
	
	\begin{center}
	\Tree [.$p$ $t_1$ $\dots$ $t_n$ ]
	\end{center}
	
	Chamamos a estas fórmulas \emph{atómicas}.
	
	\item Se $\alpha$ e $\beta$ são fórmulas e $\bf x$ é uma variável, as três seguintes são fórmulas:
	\begin{center}
	\Tree [.\texttt{not} $\alpha$ ]
	\qquad
	\Tree [.\texttt{implies} $\alpha$ $\beta$ ]
	\qquad
	\Tree [.\texttt{forall} $\bf x$ $\alpha$ ]
	\end{center}
	\end{itemize}
	
	Estes quatro tipos de fórmulas são normalmente representados em notação linear, respetivamente, como
	
	\[p(t_1, \dots, t_n) \qquad \neg \alpha \qquad \alpha \imply \beta \qquad \forall_{\bf x} \alpha.\]
	\end{definicao}
	
	Para não termos de levar o conjunto $F$ e $P$ para todo o lado, é normal juntá-los num único objeto ao qual chamamos assinatura,
	\[\Sigma = (F, P, \ar).\]
	
	Assim sendo, normalmente falaremos do conjunto de termos em $\Sigma$ ($\T_\Sigma = \T_F$) e conjunto de fórmulas em $\Sigma$ ($\F_\Sigma = \F_{FP}$). Frequentemente consideraremos a assinatura $\Sigma$ fixa e omitiremos o subscrito, escrevendo apenas $\T$ e $\F$.
	
	Para além da distinção entre termos e fórmulas, há uma grande novidade na lógica de primeira ordem em relação ao cálculo proposicional: quantificação de variáveis. Note-se, no entanto, que nem todas as variáveis de uma fórmula precisam de estar quantificadas. Por exemplo, no contexto dos naturais, $\forall_x (x = y)$ é uma fórmula legítima (mas não verdadeira), apesar de a variável $y$ não estar quantificada. (Dizemos que $y$ é uma variável livre; ver adiante.)
	
	Para interpretar estas fórmulas com variáveis livres, basta pensar no contexto do cálculo proposicional. Neste, as fórmulas não eram quantificadas, mas dizíamos que uma fórmula era uma tautologia se fosse verdade para \emph{qualquer} atribuição das variáveis. Da mesma forma, na lógica de primeira ordem dizemos que uma fórmula é verdadeira se for verdade `em qualquer universo, para qualquer atribuição das variáveis'. (Ver abaixo.)
	
	\textbf{O que se segue não se aplica para qualquer assinatura.} Em particular, há duas restrições no tamanho das assinaturas que vamos exigir. Primeiro que tudo, é desejável que haja pelo menos uma fórmula. Assim sendo, \textbf{Exigimos que o conjunto dos símbolos de proposição, $P$, seja não-vazio.} Em adição, também nos será necessário (ver lema de Lindenbaum) que haja um número contável de fórmulas. Assim sendo, exigimos que \textbf{tanto $F$ como $P$ sejam contáveis.} Isto é suficiente para que o conjunto de \emph{todas} as fórmulas seja contável, como mostramos agora:
	
	\begin{prop}\label{fol:size}
	Sob as hipóteses acima, há pelo menos uma fórmula em $\F_\Sigma$, e tanto $\F_\Sigma$ como $\T_\Sigma$ são contáveis.
	\end{prop}
	
	\begin{proof}
	A prova de que há pelo menos uma fórmula é trivial. Sabendo que há pelo menos um símbolo de proposição $p$ e uma variável $x$, basta considerar a fórmula
	\[p(x, \dots, x).\]
	
	Para justificar que há um número contável de termos e fórmulas, formulamos um argumento por indução no tamanho da expressão.
	
	Recordamos o leitor que os termos foram definidos indutivamente. Podemos pensar nessa definição como sendo um processo contável de criação de expressões:
	
	Definimos $T_0$ como o conjunto dos termos `de geração zero', isto é, as variáveis. De seguida, definido $T_n$, definimos $T_{n+1}$ como sendo os termos `um nível acima dos de $T_n$', isto é, os termos da forma
	\[f(t_1, \dots, t_a)\]
	com $t_1, \dots, t_a \in T_n$.
	
	É possível provar por indução que qualquer termo $t \in \T_\Sigma$ está nalgum $T_n$, ou seja, $\T_\Sigma = \bigcup_{i = 0}^\infty T_n$. O valor minimal de $n$ tal que $t \in T_n$ será chamado de \emph{geração de $t$}, e um processo semelhante pode ser feito para o conjunto das fórmulas $\F_\Sigma$. Isto permite-nos fazer uma forma mais forte de indução na estrutura da fórmula (indução na geração da fórmula), que nos será útil em particular nesta proposição, e mais uma vez no futuro (ver proposição \ref{fol:removerconsts}).
	
	Usamos indução na geração para mostrar que o conjunto de termos é contável. O mesmo argumento pode ser adaptado para mostrar o mesmo do conjunto de fórmulas.
	
	Repare-se que $T_0$ é contável, sendo a união do conjunto das variáveis $X$, contável por hipótese, e de um subconjunto dos símbolos de função, que são também contáveis por hipótese.
	
	Supondo que $T_n$ é contável para algum $n$, é possível mostrar que $T_{n+1}$ é contável reparando que há um número contável de símbolos de função, e cada símbolo de função $f$ adiciona apenas $(\# T_n)^{\ar f}$ termos novos. Visto que uma potência finita de um contável é contável, e soma contável de contáveis é contável, temos que $T_{n+1}$ é contável.
	
	Como $\T_\Sigma$ é a união de todos os $T_n$, havendo um número contável destes e cada um deles sendo contável, temos que $\T_\Sigma$ é ele próprio contável, terminando a demonstração.
	\end{proof}
	
	\section{Semântica}
	
	Introduzimos agora o conceito de estrutura de interpretação.
	
	No cálculo proposicional, interpretar fórmulas era muito fácil, visto que o universo estava fixo: cada variável ou era $\lt$ ou $\lf$. No contexto de primeira ordem, no entanto, há muitos universos plausíveis, pelo que, para além de dizer os valores que as variáveis tomam, é necessário especificar em que universo é que estas vivem.
	
	\begin{definicao}	
	Fixe-se uma assinatura $\Sigma = (F, P, \ar)$. Uma \emph{estrutura de interpretação} $I$ em $\Sigma$ consiste em:
	
	\begin{itemize}
	\item Um conjunto não-vazio(!) $U_I$, que representa o universo;
	
	\item Para cada símbolo $f \in F$, uma função $f_I : U^{\ar f} \to U$;\footnote{Novamente, chamamos atenção para o caso específico de $\ar f = 0$. Neste caso, $U^{\ar f} = U^0$. Isto é o conjunto que contém apenas o `zero-uplo' $()$. Ou seja, uma função $U^0 \to U$ é definida univocamente pela imagem do elemento único do seu domínio, e portanto podemos identificar as funções $U^0 \to U$ com os elementos de $U$. Dito por outras palavras: de acordo com o que foi dito antes, se $\ar f = 0$ então $f_I$ corresponde a um elemento constante de U.}
	
	\item Para cada símbolo $p \in P$, uma função $p_I : U^{\ar p} \to \{\lt, \lf\}$.
	\end{itemize}
	\end{definicao}
	
	Agora que podemos definir os universos nos quais os nossos objetos vivem, podemos definir valoração:
	
	\begin{definicao}
	Fixe-se uma assinatura $\Sigma$ e uma estrutura de interpretação $I$ (sobre $\Sigma$). Uma \emph{valoração sobre $I$} é uma função $\rho : X \to U_I$.
	\end{definicao}
	
	Para evitar sobrecarregar a notação, frequentemente omitiremos `sobre $I$' e `sobre $\Sigma$'s quando óbvio de contexto. Por exemplo, a afirmação `Seja $I$ uma estrutura de interpretação e $\rho$ uma valoração' deverá ser entendida como `Seja $\Sigma$ uma assinatura (se não estiver já uma implícita), $I$ uma estrutura de interpretação sobre $\Sigma$ e $\rho$ uma valoração sobre $I$'.
	
	Fixa uma estrutura de interpretação $I$ e uma valoração $\rho$, tal como no caso do cálculo propositcional, é possível extender $\rho$ a uma função $\T \to U_I$, `substituindo cada variável $\bf x$ por $\rho(\bf x)$ e fazendo as contas'. Concretizamos novamente esta noção usando uma definição por indução nas fórmulas:
	
	\begin{definicao}
	Fixo $\rho : X \to U_I$, definimos $\rho_\T : \T \to U_I$ indutivamente como:
	
	\begin{itemize}
	\item Se $t$ é um termo da forma $\bf x$, define-se $\rho_T(t) = \rho(\bf x)$
	
	\item Se $t$ é um termo da forma $f(t_1, \dots, t_n)$, define-se
	\[\rho_\T(t) = f_I(\rho_\T(t_1), \dots, \rho_\T(t_n)).\]
	\end{itemize}
	
	Podemos também extender a função para se aplicar a fórmulas, dizendo se a fórmula é verdadeira ou falsa. Isto é, definimos $\rho_\F : \F \to \{\lt, \lf\}$ como:
	
	\begin{itemize}
	\item Se $\varphi$ é uma fórmula atómica da forma $p(t_1, \dots, t_n)$, definimos
	\[\rho_\F(\varphi) = p_I(\rho_\T(t_1), \dots, \rho_\T(t_n))\]
	
	\item Se $\varphi$ é da forma $\neg \alpha$, definimos $\rho_\F(\varphi) = \pnot \rho_\F(\alpha)$
	
	\item Se $\varphi$ é da forma $\alpha \imply \beta$, definimos $\rho_\F(\varphi) = (\pnot \rho_\F(\alpha)) \por \rho_\F(\beta)$
	
	\item Deixamos o caso em que $\varphi$ é da forma $\forall_{\bf x} \alpha$ para adiante, visto que este caso requer alguma explicação.
	\end{itemize}
	\end{definicao}
	
	Para averiguar a definição de $\rho_\F(\forall_{\bf x} \alpha)$, é preciso averiguar o significado intuitivo do símbolo $\forall_{\bf x}$, particularmente em casos `esquisitos'.
	
	Por exemplo, não há nada a proibir a construção de uma fórmula da forma $\forall_{\bf x} \forall_{\bf x} (\bf x = \bf x)$. À primeira vista, no entanto, é difícil perceber o que esta fórmula significa. A solução é semelhante ao que é feito nalgumas linguagens de programação, que nestas tem o nome de `local scoping'.
	
	Considere-se o seguinte programa em Python:
	
	\begin{lstlisting}
x = 2
def f(x):
    print(x)
	\end{lstlisting}
	Suponha-se agora que se escreve a expressão \texttt{f(4)}. O que acontece? Sabemos que \texttt{f} imprimirá o valor de \texttt{x}, mas há dois \texttt{x} em jogo: o valor global de \texttt{x = 2} e o \texttt{x} que foi dado à função que tem o valor de \texttt{4}.
	
	O leitor poderá experimentar este exemplo por si próprio, mas o que se verifica é que o valor impresso é \texttt{4}. O que acontece é que o \texttt{x} de dentro da função `esconde' o \texttt{x} global, isto é, qualquer referência a \texttt{x} em que estão os dois \texttt{x} definidos é interpretada como referindo-se ao \texttt{x} da função. Em geral, a expressão \texttt{x} refere-se à variável \texttt{x} definida no contexto mais interior possível. É esta a estratégia que tomaremos na nossa formalização de lógica. Assim sendo, a expressão $\forall_{\bf x} \forall_{\bf x} (\bf x = \bf x)$ terá o mesmo significado que $\forall_{\bf x} \forall_{\bf x'} (\bf x' = \bf x')$, pois como a variável $\bf x$ está contida em dois quantificadores, o objeto a que se refere é àquele quantificado pelo quantificador mais interior.
	
	Podemos agora completar a nossa definição de $\rho_\F(\forall_{\bf x} \alpha)$:
	
	\begin{itemize}
	\item Establecemos a seguinte definição: se $\rho : X \to U_I$ é uma valoração, $\bf x$ é uma variável e $u \in U_I$, definimos o símbolo $\rho\!\downarrow^{\bf x}_u$ como a valoração:
	
	\[\rho\!\downarrow^{\bf x}_u (\bf y) = \begin{cases}
	\rho(\bf y) & \text{se $\bf y$ não é $\bf x$}\\
	u & \text{se $\bf y$ é $\bf x$}
	\end{cases}\]
	
	Então, dizemos que $\rho_\F(\forall_{\bf x} \alpha) = \lt$ sse, para todo $u \in U_I$,
	
	\[(\rho\!\downarrow^{\bf x}_u)_\F(\alpha) = \lt.\]
	\end{itemize}
	
	Estamos agora em condições de definir o conceito de consequência semântica no contexto de lógica de primeira ordem. A definição é muito parecida com aquela dada para o cálculo proposicional (definição \ref{def:prop:consequenciasemantica}), mas repare-se que aqui há dois `níveis' diferentes a ter em conta: o nível de estrutura de interpretação e o nível de valoração.
	
	\begin{definicao}
	Seja $\varphi$ uma fórmula e $I$ uma estrutura de interpretação, ambas sobre uma assinatura comum $\Sigma$.
	
	Se $\rho$ é uma valoração sobre $I$, dizemos que $\rho$ satisfaz $\varphi$, denotado $\rho \Vdash \varphi$, se $\rho_\F(\varphi) = \lt$. Isto é: substituindo cada variável $\bf x$ pelo seu valor $\rho(\bf x)$, a afirmação $\varphi$ fica verdadeira.
	
	Dizemos que $I$ satisfaz $\varphi$, escrito $I \Vdash \varphi$, se, para \emph{todas} as valorações $\rho$ sobre $I$, $\rho \Vdash \varphi$. Por outras palavras, `$\varphi$ é sempre verdade no universo $I$'.
	
	De forma análoga à definição \ref{def:prop:consequenciasemantica}, uma afirmação da forma $I \Vdash \Gamma$ (onde $\Gamma$ é um conjunto de fórmulas) equivale a dizer $I \Vdash \gamma$ para todo $\gamma \in \Gamma$.
	\end{definicao}
	
	Note-se que esta distinção entre uma fórmula ser satisfeita por um universo ou por uma valoração não existia no cálculo proposicional, visto que havia apenas um universo: $\{\lt, \lf\}$.
	
	\begin{obs}\label{structuresatisfactionnotnice}
	Cuidado! Alguns passos lógicos que estamos habituados a que sejam verdade no contexto de valorações não o são no contexto de estruturas de interpretação. Por exemplo, enquanto que é decerto verdade que $\rho \Vdash \neg \varphi$ sse $\rho \nVdash \varphi$, o mesmo não se pode dizer de estruturas de interpretação. Isto é, é certamente possível que $I \nVdash \varphi$ e, ao mesmo tempo, $I \nVdash \neg \varphi$: basta algumas valorações atribuírem o valor $\lt$ a $\varphi$ e outras $\lf$.
	\end{obs}
	
	Estas duas noções de satisfação dão azo a duas noções ligeiramente diferentes de consequência semântica: local e global.
	
	\begin{definicao}
	Fixa uma assinatura $\Sigma$, sejam $\Gamma \subseteq \F_\Sigma$ e $\varphi \in \F_\Sigma$. Dizemos que $\varphi$ é \emph{consequência semântica} (global) de $\Gamma$, denotado $\Gamma \vDash \varphi$, se para toda a estrutura de interpretação $I$ tal que $I \Vdash \Gamma$ temos que $I \Vdash \varphi$. Em particular, dizemos que $\varphi$ é uma afirmação verdadeira se $\emptyset \vDash \varphi$.
	
	Dizemos que $\varphi$ é \emph{consequência semântica} (local) de $\Gamma$, denotado $\Gamma \vDash_L \varphi$, se para toda a estrutura de interpretação $I$ e valoração $\rho$ sobre $I$ tal que $\rho \Vdash \Gamma$ temos que $\rho \Vdash \varphi$. Podemos definir uma afirmação como sendo localmente verdadeira se $\emptyset \vDash_L \varphi$, mas é fácil reparar que uma afirmação é verdadeira sse é localmente verdadeira, o que faz desta última definição desnecessária.
	\end{definicao}
	
	No que se segue, a noção de consequência semântica global estará em destaque. Como tal, quando nos referirmos a `consequência semântica', sem menção a local ou global, é pressuposto que nos referimos a consequência semântica global.
	
	Para entender a distinção entre estes dois conceitos, consideremos um exemplo. Seja $\mathbf{Gr}$ o conjunto dos axiomas de grupo, assumindo-se subjacente uma assinatura apropriada. Não entraremos nos detalhes técnicos, visto que não é esse o propósito da exposição atual. Por exemplo, para estes propósitos, os axiomas de grupo deveriam incluir axiomas para a igualdade. No entanto, no que se segue, assumiremos que o símbolo $=$ funciona como esperado.
	
	Neste caso, temos que $\varphi$ é consequência semântica global de $\mathbf{Gr} \cup \Gamma$ se em todos grupos $G$ que satisfazem $\Gamma$ também satisfazem $\varphi$. Por exemplo, considere-se $\Gamma = \{x = y\}$ e averigue-se que grupos o satisfazem.
	
	Note-se que um grupo $G$ satisfaz $x = y$ sse, para qualquer valoração das variáveis $\rho$, $\rho(x) = \rho(y)$. Note-se que a única forma possível de isto acontecer é se $G$ é o grupo trivial com um só elemento \textbf{id}, a identidade. Assim sendo, todos os grupos que satisfazem $\Gamma$ também satisfazem, por exemplo, $\varphi : (z \cdot w) = (w \cdot z)$, visto que o grupo trivial é comutativo. Como tal, concluímos a seguinte consequência semântica global:
	\[\mathbf{Gr}, (x = y) \vDash (z \cdot w) = (w \cdot z).\]
	
	Vamos agora ver que a correspondente afirmação \emph{local} não é verdade. De facto, a afirmação
	\[\mathbf{Gr}, (x = y) \vDash_L (z \cdot w) = (w \cdot z)\]
	corresponde ao seguinte: Se $G$ é um grupo e $\rho : X \to G$ é uma valoração tal que $\rho \Vdash (x = y)$, então $\rho \Vdash (z \cdot w) = (w \cdot z)$. É então fácil arranjar um contraexemplo. Seja $G$ é um grupo não-comutativo e sejam $a, b \in G$ elementos que não comutam. Então, se $\rho$ satisfaz
	\begin{gather*}
	\rho(x) = \rho(y) = \rho(z) = a\\
	\rho(w) = b
	\end{gather*}
	temos que $\rho$ satisfaz todas as premissas ($x = y$) mas não satisfaz a conclusão ($z\cdot w = w\cdot z$), donde concluímos, finalmente,
	\[\mathbf{Gr}, (x = y) \nvDash_L (z \cdot w) = (w \cdot z).\]
	
	Isto foi um exemplo que mostra que consequência semântica global não implica consequência semântica local. No entanto, o converso é verdade, isto é:
	\[\text{Se } \Gamma \vDash_L \varphi \text{ então } \Gamma \vDash \varphi.\]
	
	Deixamos a demonstração deste facto como exercício para o leitor.
	
	\section{Regras de dedução}
	
	Recordamos que o objetivo deste capítulo é arranjar um conjunto de regras a partir das quais é possível concluir afirmações verdadeiras. No contexto do cálculo proposicional, a primeira e mais importante tal regra era a regra de \textit{modus ponens}. Esta regra também se aplica a este novo contexto.
	
	Em tudo o que se segue, $\Sigma$ é uma assinatura fixa que deixamos implícita.
	
	\begin{prop}
	Sejam $\alpha, \beta \in \F$. Então,
	\[(\alpha \imply \beta), \alpha \vDash_L \beta.\]
	\end{prop}
	
	\begin{proof}
	A demonstração é completamente idêntica à da proposição \ref{prop:mp}.
	\end{proof}
	
	Note-se que enunciámos esta propriedade usando consequência semântica local. Implícito está, claro, que a mesma propriedade também é verdade para consequência semântica global, visto que uma implica a outra.
	
	A regra de \emph{modus ponens} não é a única propriedade do cálculo proposicional que se transpõe para a lógica de primeira ordem. De facto, em certo sentido, qualquer tautologia é verdadeira.
	
	Por exemplo, considere-se uma fórmula da forma $\varphi : \alpha \lor \neg \alpha$. Desejamos mostrar que esta fórmula é verdadeira. A prova não é particularmente difícil: basta considerar que, se $\rho$ é uma valoração sobre alguma estrutura de interpretação $I$, $\rho_\F(\alpha)$ é $\lt$ ou $\lf$. Verificando os dois casos, vemos que $\rho_\F(\varphi)$ é, respetivamente $\lt \lor \lf$ ou $\lf \lor \lt$. Em ambos os casos a expressão é avaliada para $\lt$, pelo que para qualquer valoração $\rho$ temos $\rho \Vdash \varphi$, que é a definição de fórmula verdadeira.
	
	Na realidade, há uma justificação que se aproxima mais do cerne da questão. A observação essencial é que a fórmula (proposicional!) $a \lor \neg a$ é uma tautologia e que, de alguma forma, isto implica que qualquer fórmula com essa forma é uma tautologia. De forma semelhante, por exemplo, sabendo que $a \imply (b \imply a)$ é uma tautologia conseguiríamos concluir que uma fórmula da forma $\varphi \imply (\psi \imply \varphi)$ é verdadeira.
	
	Para conseguirmos formalizar esta noção, formalizamos o que significa `uma fórmula de primeira ordem vir de uma fórmula proposicional'.
	
	\begin{definicao}
	Seja $\varphi$ uma fórmula proposicional. Suponha-se que a cada variável $\bf x \in X$ se atribui uma fórmula (de primeira ordem) $\psi(\bf x) \in \F_\Sigma$. Então, definimos $\overline\psi(\varphi)$ como a fórmula (de primeira ordem) obtida de substituir cada variável $\bf x$ por $\psi(\bf x)$.
	
	Por exemplo, a fórmula $\alpha \lor \neg \alpha$ vem de substituição de $a \lor \neg a$, substituindo a letra $a$ pela fórmula $\alpha$, isto é, $\psi(a) = \alpha$.
	
	Deixamos a formalização (por indução em $\varphi$) desta definição ao leitor.
	\end{definicao}
	
	\begin{prop}\label{fol:tautoistheo}
	Seja $\varphi \in \F_p$ uma tautologia e seja $\psi : X \to \F_\Sigma$. Então, $\overline\psi(\varphi)$ é uma fórmula verdadeira.
	\end{prop}
	
	\begin{proof}
	Seja $I$ uma estrutura de interpretação e $\rho$ uma valoração em $I$. Pretendemos mostrar que $\rho_\F(\overline\psi(\varphi)) = \lt$. Para o fazer, construimos, a partir de $\rho$, uma valoração (proposicional) $\rho_p$ tal que, para qualquer fórmula proposicional $\gamma$,
	\begin{equation}\label{fol:tautoistheo:eq1}
	\rho_\F(\overline\psi(\gamma)) = \overline \rho_p (\gamma).
	\end{equation}
	
	Defina-se simplesmente $\rho_p(\bf x) = \rho_\F(\psi(\bf x))$. É fácil provar por indução na estrutura da fórmula que para qualquer fórmula proposicional $\gamma$ a equação \eqref{fol:tautoistheo:eq1} é verdadeira.
	
	Em particular, isto é verdade para a nossa tautologia $\varphi$, donde, por definição de tautologia, temos que, para qualquer valoração $\rho$,
	\[\rho_\F(\overline\psi(\varphi)) = \overline \rho_p (\varphi) = \lt.\]
	
	Como tal, a fórmula $\overline\psi(\varphi)$ é verdadeira.
	\end{proof}
	
	Note-se que a proposição anterior poderia ser fortalecida: é possível pegar numa afirmação da forma $\Gamma \vDash \varphi$ (fórmulas proposicionais) e concluir algo da forma
	\begin{equation}\label{fol:tautoistheo:eq2}
	\overline\psi(\Gamma) \vDash \overline\psi(\varphi).
	\end{equation}
	
	O leitor interessado poderá fazer os detalhes, e deixamos como exercício a seguinte questão: será que o símbolo $\vDash$ em \eqref{fol:tautoistheo:eq2} poderia ser substituído por $\vDash_L$?
	
	\smallskip
	
	Outra ferramenta muito útil no cálculo proposicional era o metateorema da dedução. Veremos que este também se aplica no contexto de lógica de primeira ordem, apesar de ser preciso algum cuidado. Em particular, é este um caso em que consequência semântica local acaba por ser mais bem-comportada do que consequência semântica global.
	
	\begin{prop}(Metateorema da dedução, versão semântica, primeira ordem)
	
	Sejam $\Gamma \subseteq \F$ e $\alpha, \beta \in \F$. Então,
	\[\Gamma, \alpha \vDash_L \beta \text{ sse } \Gamma \vDash_L (\alpha \imply \beta).\]
	\end{prop}
	
	\begin{proof}
	A demonstração é muito semelhante à demonstração da proposição \ref{prop:mtd}.
	
	($\leftarrow$) Suponha-se que $\Gamma \vDash_L (\alpha \imply \beta)$. Seja $\rho$ uma valoração tal que $\rho \Vdash \Gamma \cup \{\alpha\}$. Então, $\rho \Vdash (\alpha \imply \beta)$, por definição de consequência semântica local. Temos também $\rho \Vdash \alpha$, por razões óbvias. Como tal, por \textit{modus ponens}, temos $\rho \Vdash \beta$. Como $\rho$ é uma valoração arbitrária tal que $\rho \Vdash \Gamma \cup \{\alpha\}$, concluímos, como desejado,
	\[\Gamma, \alpha \vDash_L \beta.\]
	
	($\rightarrow$) Suponha-se $\Gamma, \alpha \vDash_L \beta$. Seja $\rho$ uma valoração tal que $\rho \Vdash \Gamma$. Mostraremos que $\rho \Vdash (\alpha \imply \beta)$.
	
	Há dois casos a considerar: $\rho_\F(\alpha) = \lt$ ou $\rho_\F(\alpha) = \lf$.
	
	\begin{itemize}
	\item Se $\rho_\F(\alpha) = \lt$, então, sabendo que $\rho \Vdash \Gamma$ e  $\Gamma, \alpha \vDash_L \beta$, temos que $\rho \Vdash \beta$. Assim sendo, temos, como desejado,
	\begin{align*}
	\rho_\F(\alpha \imply \beta) &= (\pnot \rho_\F(\alpha)) \por \rho_\F(\beta)\\
	&= (\pnot \lt) \por \lt\\
	&= \lt.
	\end{align*}
	
	\item Se $\rho_\F(\alpha) = \lf$ então, como desejado,
	\begin{align*}
	\rho_\F(\alpha \imply \beta) &= (\pnot \rho_\F(\alpha)) \por \rho_\F(\beta)\\
	&= (\pnot \lf) \por \rho_\F(\beta)\\
	&= \lt \por \rho_\F(\beta)\\
	&= \lt.
	\end{align*}
	\end{itemize}
	
	Independentemente do caso temos $\rho \Vdash (\alpha \imply \beta)$. Isto conclui a demonstração.
	\end{proof}
	
	Mostramos agora um exemplo para exemplificar onde é que este metateorema falha para consequência semântica global. Obviamente o contraexemplo terá que ser no sentido ($\rightarrow$), visto que a implicação ($\leftarrow$), que é uma consequência direta de \textit{modus ponens}, também é verdade para consequência semântica global.
	
	Novamente, considere-se o exemplo dos axiomas de grupo $\Gamma = \textbf{Gr}$. Consideremos as fórmulas
	\begin{gather*}
	\alpha : x = \mathbf{id}\\
	\beta : y = \mathbf{id}
	\end{gather*}
	
	Então, é certamente verdade que
	\begin{equation}\label{mtd:contraex1}
	\Gamma, \alpha \vDash \beta,
	\end{equation}
	visto que se $I$ é um grupo que satisfaz $\alpha$, $I$ é necessariamente o grupo trivial, pois para qualquer valor de $\rho(x)$ temos $\rho(x) = \mathbf{id}_I$. O grupo trivial satisfaz também $\beta$, pelo que a afirmação \eqref{mtd:contraex1} é verdadeira.
	
	No entanto, a conclusão que seria dada pelo MTD é falsa. Isto é:
	\begin{equation}\label{mtd:contraex2}
	\Gamma \nvDash (\alpha \imply \beta).
	\end{equation}
	
	Para justificar esta afirmação, seja $I$ um grupo com pelo menos dois elementos, digamos $a = \mathbf{id}_I$ e $b \neq \mathbf{id}_I$. Como $I$ é um grupo, decerto que $I \Vdash \Gamma$. No entanto, $I \nVdash (\alpha \imply \beta)$. De facto, considere-se uma valoração $\rho$ que atribui
	\begin{gather*}
	\rho(x) = a\\
	\rho(y) = b.
	\end{gather*}
	
	Então, temos que
	\begin{align*}
	\rho_\F(\alpha \imply \beta) &= \rho_\F((x = \mathbf{id}) \imply (y = \mathbf{id}))\\
	&= (\pnot \rho_\F((x = \mathbf{id})) \por \rho_\F(y = \mathbf{id}))\\
	&= (\pnot \mathbf{eq}(\rho(x), \mathbf{id}_I)) \por \mathbf{eq}(\rho(y), \mathbf{id}_I)
	\end{align*}
	onde \textbf{eq} é a função que corresponde a $=_I$, que devolve $\lt$ sse ambos os seus argumentos são iguais.
	
	Sabendo que atribuímos $\rho(x) = \mathbf{id}_I$ e $\rho(y) \neq \mathbf{id}_I$, isto dá igual a
	\[(\pnot \lt) \por \lf = \lf,\]
	o que mostra que $\rho \nVdash (\alpha \imply \beta)$, e então, como consequência, $I \nVdash (\alpha \imply \beta)$. Isto termina a demonstração de \eqref{mtd:contraex2}.
	
	\smallskip
	
	Mostrámos agora um exemplo em que consequência semântica local tem propriedades agradáveis. Mostramos agora uma propriedade útil da consequência semântica global.
	
	Um género de raciocínio que fazemos a toda a hora é o seguinte. Pretende-se mostrar que todos os objetos $x$ de algum universo satisfazem $A$. Por exemplo, para todos os elementos $x$ de um anel, $0 \cdot x = 0$. Então, pegamos num $x$ arbitrário, e mostramos que satisfaz $A$. Como $x$ era arbitrário, concluímos que \emph{todos os $x$} satisfazem $A$.
	
	Simbolicamente, este tipo de raciocínio traduz-se na lógica de primeira ordem da seguinte forma:
	
	\begin{prop}
	Seja $\varphi \in \F$ e $\bf x \in X$. Então
	
	\[\varphi \vDash \forall_{\bf x} \varphi.\]
	\end{prop}
	
	\begin{proof}
	Seja $I$ uma estrutura de interpretação tal que $I \Vdash \varphi$. Desejamos mostrar que $I \Vdash \forall_{\bf x} \varphi$. Para tal, seja $\rho$ uma valoração arbitrária sobre $I$. Queremos verificar que $\rho \Vdash \forall_{\bf x} \varphi$.
	
	Por definição, isto acontece sse, para qualquer $u \in U_I$,
	
	\[\rho\!\downarrow^{\bf x}_u \Vdash \varphi.\]
	
	Visto que $I \Vdash \varphi$, para qualquer valoração $\rho'$ temos
	\[\rho' \Vdash \varphi\]
	e em particular
	\[\rho\!\downarrow^{\bf x}_u \Vdash \varphi,\]
	como desejado.
	\end{proof}
	
	Esta nova regra ($\varphi \vDash \forall_{\bf x} \varphi$) tem o nome de \emph{Generalização}. A generalização e \textit{modus ponens} formarão as regras de dedução do chamado \emph{cálculo de Hilbert}, o sistema de demonstração que, como provaremos, engloba completamente a noção de consequência semântica global.
	
	\section{Sintática}
	
	Definimos agora um sistema de demonstração, à semelhança do que foi feito no caso do cálculo proposicional.
	
	\begin{definicao}
	Suponha-se fixa uma assinatura $\Sigma$ e um conjunto de axiomas $A \subseteq \F_\Sigma$.
	
	Se $\Gamma \subseteq \F_\Sigma$, uma \emph{demonstração com base em $\Gamma$} é uma sequência
	\[(\varphi_1, j_1), \dots, (\varphi_n, j_n),\]
	em que cada $\varphi_i \in \F_\Sigma$ e $j_i$, que representa a justificação de $\varphi_i$, satisfaz as seguintes regras:
	
	\begin{itemize}
	\item Cada $j_i$ é um símbolo da forma \texttt{Ax}, \texttt{Hip}, $\texttt{MP}_{ab}$ com $a, b \in \N$ ou $\texttt{Gen}_a$ com $a \in \N$
	
	\item Se $j_i = \texttt{Ax}$ é necessário que $\varphi_i \in A$
	
	\item Se $j_i = \texttt{Hip}$ é necessário que $\varphi_i \in \Gamma$
	
	\item Se $j_i = \texttt{MP}_{ab}$, então $a, b < i$ e $\varphi_a$ é a fórmula $\varphi_b \imply \varphi_i$
	
	\item Se $j_i = \texttt{Gen}_a$, então $a < i$ e $\varphi_i$ é da forma $\forall_{\bf x} \varphi_a$ para alguma variável $\bf x$.
	\end{itemize}
	
	Dizemos que $\Gamma \vdash \varphi$ se existe uma demonstração com base em $\Gamma$ cuja conclusão é $\varphi$.
	
	Este sistema de demonstração tem o nome de \emph{cálculo de Hilbert}.\footnote{Na realidade, o cálculo de Hilbert corresponde ao caso particular dos axiomas finais (ver página \pageref{fol:finalaxioms}). No entanto, por conveniência, referir-nos-emos a esta versão modificada pelo mesmo nome.}
	\end{definicao}
	
	À semelhança do que foi feito com o cálculo proposicional, supomos por agora que que qualquer afirmação verdadeira é um axioma. Mais tarde, identificaremos um conjunto mais pequeno de axiomas que se mostrará ser suficiente para justificar qualquer afirmação verdadeira.
	
	Mostraremos que o cálculo de Hilbert corresponde exatamente à noção de consequência semântica global. Recordamos que isto corresponde a mostrar duas coisas: a \emph{correção} do sistema, e a sua \emph{completude}. À semelhança do caso proposicional, a correção é de fácil demonstração, pelo que a deixamos ao leitor:
	
	\begin{prop}
	O cálculo de Hilbert é correto. Isto é,
	\[\text{Se } \Gamma \vdash \varphi \text{ então } \Gamma \vDash \varphi.\]
	\end{prop}
	
	Seria desejado exemplificar agora uma demonstração em lógica de primeira ordem, mas infelizmente o cálculo de Hilbert é particularmente inadequado para uso humano. De facto, a prova de algo tão simples como `Num grupo, se $e \cdot x = x$ para todo o $x$, temos $e = \textbf{id}$' requer uma página inteira de demonstração, apesar de a ideia de prova ser simplesmente reparar que $\textbf{id} = e \cdot \textbf{id} = e$. Para nos permitir manusear e fabricar demonstrações em espaço útil, é necessário o uso de metateoremas.
	
	Um dos metateoremas mais úteis é o metateorema da dedução. No entanto, como visto anteriormente, este não pode ser usado tão livremente como no cálculo proposicional. Em particular, é preciso ter cuidado com a generalização. De facto, a demonstração do metateorema da dedução do cálculo proposicional pode facilmente ser adaptada para demonstrar:
	
	\begin{prop*}
	(Metateorema da dedução, versão sintática fraca, primeira ordem)
	
	Sejam $\Gamma \subseteq \F$ e $\alpha, \beta \in \F$. Suponha-se que existe uma demonstração de $\Gamma, \alpha \vdash \beta$ que não utiliza generalização. Então, $\Gamma \vdash \alpha \imply \beta$.
	\end{prop*}
	
	De modo a poder concluir um metateorema mais forte, é preciso encontrar condições nas quais de $\alpha \imply \varphi$ se conclua $\alpha \imply \forall_{\bf x} \varphi$. A única regra que temos que permita introduzir um quantificador é a generalização, pelo que é suficiente que a seguinte afirmação seja verdadeira:
	\begin{equation}\label{folax4}
	(\forall_{\bf x} (\alpha \imply \varphi)) \imply (\alpha \imply \forall_{\bf x} \varphi).
	\end{equation}
	
	Infelizmente, isto nem sempre é uma afirmação verdadeira. Por exemplo, no contexto dos naturais, se $\alpha : x = 0$ e $\varphi : x = 0$ obtemos que $\forall_x (\alpha \imply \varphi)$ é uma afirmação verdadeira, mas $\alpha \imply \forall_x \varphi$ não é (basta considerar uma valoração que atribui $\rho(x) = 0$). É, no entanto, suficiente exigir que $\alpha$ não dependa de $\bf x$. Intuitivamente, se $\alpha$ não depender de $\bf x$, se soubermos $\forall_{\bf x} (\alpha \imply \varphi)$ e $\alpha$, fixo um $\bf x$ arbitrário, temos que $\alpha \imply \varphi$ (porque se algo é verdade para todos os $\bf x$ é verdade para este em particular), temos $\alpha$, e portanto temos, para este $\bf x$ particular, $\varphi$. Como $\bf x$ é arbitrário, $\varphi$ é verdade para todo o $\bf x$.
	
	Claro que este argumento não é de todo rigoroso. Para o formalizar, é primeiro entender o que significa $\alpha$ depender de $\bf x$. Daí surge a noção de variável livre.
	
	Dada uma fórmula $\varphi$, o valor de verdade desta poderá depender da valoração usada. Em particular, poderá depender do valor de certas variáveis. Por exemplo, no contexto dos naturais, a fórmula $\forall_x (x \geq y)$ é verdadeira para $y = 0$ mas falsa caso contrário. Por outras palavras, a variável $y$ é um parâmetro que pode ser modificado para mudar o valor de verdade da fórmula. Note-se que o mesmo não pode ser dito de $x$, visto que, como essa variável está quantificada, o valor de verdade da fórmula não depende do valor que uma valoração atribui a $x$.
	
	\begin{definicao}
	Dada uma fórmula $\varphi$, define-se o conjunto das variáveis livres de $\varphi$, denotado $\fv \varphi$ (free variables) como as variáveis em $\varphi$ que são `visíveis do exterior', isto é, que não estão quantificadas. Isto pode ser formalizado indutivamente como:
	
	\begin{itemize}
	\item Se $\varphi : p(t_1,\dots,t_n)$ então $\fv \varphi = \var t_1 \cup \dots \cup \var t_n$ (ver abaixo)
	
	\item Se $\varphi : \alpha \imply \beta$ então $\fv \varphi = \fv \alpha \cup \fv \beta$
	
	\item Se $\varphi : \neg \alpha$ então $\fv \varphi = \fv \alpha$
	
	\item Se $\varphi : \forall_{\bf x} \alpha$ então $\fv \varphi = \fv \alpha \setminus \{\bf x\}$.
	\end{itemize}
	
	Por sua vez, esta definição requer definir $\var t$ para $t \in \T$. Por sua vez, isto também é definido indutivamente:
	
	\begin{itemize}
	\item $\var \bf x = \{\bf x\}$
	
	\item $\var f(t_1, \dots, t_n) = \var t_1 \cup \dots \var t_n$.
	\end{itemize}
	\end{definicao}
	
	Note-se o caso algo patológico em que algumas instâncias de uma variável estão quantificadas e outras não. Por exemplo, na fórmula $\varphi : p(x) \imply \forall_x p(x)$, a variável $x$ é livre, porque existe pelo menos uma instância de $x$ que é visível no exterior, apesar de haver também outra instância que está quantificada e como tal não conta como livre.
	
	Variáveis livres têm a particularidade que, tal como as variáveis do cálculo proposicional, o valor atribuido a uma fórmula $\varphi$ por uma valoração $\rho$ depende apenas do valor das variáveis livres.
	
	\begin{prop} (Lema das variáveis omissas) No que se segue, considere-se fixa uma estrutura de interpretação $I$.
	
	Seja $t \in \T$. Então, $\rho_\T(t)$ depende apenas do valor que $\rho$ atribui às variáveis em $\var t$.
	
	Seja $\varphi \in \F$. Então, $\rho_\F(\varphi)$ depende apenas do valor de $\rho$ atribui às variáveis em $\fv \varphi$.
	\end{prop}
	
	\begin{proof}
	Esta demonstração é feita por indução. Exemplificamos com a primeira parte (referente aos termos):
	
	\begin{itemize}
	\item Se $t$ é da forma $\bf x$, então $\rho_\T(t) = \rho(\bf x)$ depende apenas de $\rho(\bf x)$, ou seja, do valor que $\rho$ atribui às variáveis em $\var t$.
	
	\item Se $t$ é da forma $f(t_1, \dots, t_n)$ então $\rho_\T(t)$ é igual a $f_I(\rho_T(t_1), \dots, \rho_T(t_n))$, que depende apenas dos valores de $\rho_\T(t_1), \dots, \rho_\T(t_n)$. Por sua vez, estes dependem apenas de $\var t_1, \dots, \var t_n$ respetivamente, pelo que a sequencia total, e então $\rho_\T(t)$, depende apenas de $\var t_1 \cup \dots \cup \var t_n = \var t$.
	\end{itemize}
	
	A demonstração da a versão para fórmulas é semelhante, excetuando o caso de quantificadores no passo de indução. É este o caso que fazemos no que se segue, deixando os três outros casos para o leitor.
	
	Suponha-se, por hipótese de indução, que $\rho_\F(\varphi)$ depende apenas de $\rho|_{\fv \varphi}$. Então, mostramos que $\rho_\F(\forall_{\bf x} \varphi)$ depende apenas de $\rho|_{\fv \varphi \setminus \{\bf x\}}$.
	
	Sejam $\rho$ e $\rho'$ duas valorações que concordam em $\fv \varphi \setminus \{\bf x\}$. Então, por hipótese de indução, $(\rho\!\downarrow^{\bf x}_u)_\F(\varphi) = (\rho'\!\downarrow^{\bf x}_u)_\F(\varphi)$, visto que ambas estas valorações concordam em $\fv \varphi$. Como tal,
	\[\rho\!\downarrow^{\bf x}_u \Vdash \varphi \text{ para todo o $u \in U_I$}\]
	sse
	\[\rho'\!\downarrow^{\bf x}_u \Vdash \varphi \text{ para todo o $u \in U_I$.}\]
	
	Logo, $\rho_\F(\varphi) = \rho'_\F(\varphi)$, como desejado.
	\end{proof}
	
	Estamos agora prontos para provar que, se $\alpha$ não depende de $\bf x$, a fórmula \eqref{folax4} é verdadeira.
	
	\begin{prop}\label{fol:folax4verdadeira}
	Sejam $\alpha$ e $\varphi$ duas fórmulas e $\bf x$ uma variável tal que $\bf x \not \in \fv\alpha$. Então, a fórmula
	\begin{equation}\tag{\ref{folax4}}
	\psi : (\forall_{\bf x} (\alpha \imply \varphi)) \imply (\alpha \imply \forall_{\bf x} \varphi)
	\end{equation}
	é verdadeira,
	\end{prop}
	
	\begin{proof}
	Seja $I$ uma estrutura de interpretação e $\rho$ uma valoração arbitrária sobre esta estrutura. Então, averigue-se $\rho_\F(\psi)$. Há três casos a considerar, dependendo de $\rho_\F(\forall_{\bf x}(\alpha \imply \varphi))$ e $\rho_\F(\alpha)$:
	
	\begin{itemize}
	\item Se $\rho_\F(\forall_{\bf x}(\alpha \imply \varphi)) = \lf$, então $\rho_\F(\psi)$ é da forma $(\pnot \lf) \lor \text{(algo)}$, que é garantidamente $\lt$;
	
	\item Se $\rho_\F(\alpha) = \lf$, então $\rho_\F(\alpha \imply \forall_{\bf x} \varphi)$ é garantidamente $\lt$, pelo que $\rho_\F(\psi)$ é necessariamente $\lt$;
	
	\item Se $\rho_\F(\forall_{\bf x}(\alpha \imply \varphi)) = \rho_\F(\alpha) = \lt$, então, para qualquer $u \in U_I$, $\rho\!\downarrow^{\bf x}_u \Vdash (\alpha \imply \varphi)$. Pelo lema das variáveis omissas, como $\bf x \not \in \fv \alpha$ e $\rho \Vdash \alpha$, temos também $\rho\!\downarrow^{\bf x}_u \Vdash \alpha$. Pela versão local do \emph{modus ponens}, temos, para todo $u \in U_I$, $\rho\!\downarrow^{\bf x}_u \Vdash \varphi$. Como tal, temos $\rho \Vdash \forall_{\bf x} \varphi$, o que implica, como o leitor poderá verificar, que $\rho_\F(\psi) = \lt$.
	\end{itemize}
	
	Como em qualquer dos casos temos $\rho \Vdash \psi$, a fórmula $\psi$ é verdadeira, como desejado.
	\end{proof}
	
	Estamos agora em condições de deduzir o
	
	\begin{prop}
	(Metateorema da dedução, versão sintática, primeira ordem)
	
	Seja $\Gamma \subseteq \F$, $\alpha, \beta \in \F$ e suponha-se que existe uma demonstração de
	\[\Gamma, \alpha \vdash \beta\]
	tal que em nenhum passo é feita uma generalização sobre uma variável livre de $\alpha$. Então,
	\[\Gamma \vdash (\alpha \imply \beta).\]
	\end{prop}
	
	\begin{proof}
	A demonstração é idêntica à do caso proposicional, diferindo apenas no caso em que o passo da demonstração é feito por generalização. Para ser completos, no entanto, apresentaremos a demonstração completa.
	
	Suponha-se que a prova dada no enunciado é
	\[(\varphi_1, j_1), \dots, (\varphi_n, j_n).\]
	Apresentamos, indutivamente, um método de construir, a partir desta, uma demonstração de $\Gamma \vdash (\alpha \imply \varphi)$. Concretamente, construiremos uma sequência de prova
	\[(\psi_1, j'_1), \dots, (\psi_N, j'_N),\]
	assegurando que existem $i_1, \dots, i_n$ ($i_n = N$) tal que $\psi_{i_1} = (\alpha \imply \varphi_1)$, \dots, $\psi_{i_n} = (\alpha \imply \varphi_n)$.
	
	Suponha-se, por hipótese de indução, que a prova modificada já está construida até
	\[(\psi_1, j'_1), \dots, (\psi_{i_{p-1}}, j'_{i_{p-1}}),\]
	para algum $p \in \{1, \dots, n\}$ ($i_0 = 0$). Construimos agora a secção da prova seguinte,
	\[(\psi_{i_{p-1}+1}, j'_{i_{p-1}+1}), \dots, (\psi_{i_p}, j'_{i_p}).\]
	
	Esta construção é feita por casos, dependendo de $j_p$:
	
	\begin{itemize}
	\item Se $j_p = \mathtt{Ax}$, usamos o facto de que, como $\varphi_p$ é uma afirmação verdadeira, $\alpha \imply \varphi_p$ também é uma afirmação verdadeira. Assim sendo, a continuação da prova modificada pode ser construida simplesmente como
	\[(\psi_{i_{p-1}+1}, j'_{i_{p-1}+1}) = (\alpha \imply \varphi_p, \mathtt{Ax}).\]
	
	\item Se $j_p = \mathtt{Hip}$, temos que ou $\varphi_p = \alpha$ ou $\varphi_p \in \Gamma$.
	
	\begin{itemize}
	\item Se $\varphi_p = \alpha$, basta reparar que $\alpha \imply \alpha$ é uma afirmação verdadeira, e como tal o argumento acima pode ser realizado.
	
	\item Se $\varphi_p \in \Gamma$, usamos o facto que $\varphi_p \imply (\alpha \imply \varphi_p)$ é uma afirmação verdadeira, o que nos permite construir a seguinte (secção de) prova:
	
	\[(\varphi_p \imply (\alpha \imply \varphi_p), \mathtt{Ax}), (\varphi_p, \mathtt{Hip}), (\alpha \imply \varphi_p, \mathtt{MP}).\]
	\end{itemize}
	
	\item Se $j_p = \mathtt{MP}_{ab}$, começamos por invocar o primeiro axioma do cálculo proposicional (que, pela proposição \ref{fol:tautoistheo}, é uma afirmação verdadeira):
	\[([\alpha \imply (\varphi_b \imply \varphi_p)] \imply [(\alpha \imply \varphi_b) \imply (\alpha \imply \varphi_p)], \texttt{Ax})\]
	ou, por outras palavras (pela hipótese sobre a parte da demonstração já construída),
	\[(\psi_{i_a} \imply (\psi_{i_b} \imply (\alpha \imply \varphi_p)), \texttt{Ax}).\]
	
	De seguida, invocamos $\mathtt{MP}_{i_a, (i_{p-1}+1)}$ para concluir
	\[(\psi_{i_b} \imply (\alpha \imply \varphi_p), \texttt{MP}),\]
	e uma última aplicação de $\mathtt{MP}_{i_b, (i_{p-1}+2)}$ dá-nos, como desejado,
	\[(\alpha \imply \varphi_p, \texttt{MP}).\]
	
	\item Se $j_p = \mathtt{Gen}_a$, sabemos que $\varphi_p$ é da forma $\forall_{\bf x} \varphi_a$. Por hipótese sobre a demonstração, sabemos que $\bf x \not \in \fv \alpha$, que nos permite invocar a afirmação verdadeira (pela proposição \ref{fol:folax4verdadeira}):
	\[([\forall_{\bf x} (\alpha \imply \varphi_a)] \imply [\alpha \imply \forall_{\bf x} \varphi_a], \mathtt{Ax}),\]
	ou, por outras palavras,
	\[([\forall_{\bf x} \psi_{i_a}] \imply [\alpha \imply \varphi_p], \mathtt{Ax}).\]
	
	A secção de prova pode agora ser completada da seguinte forma:
	\begin{gather*}
	(\forall_{\bf x} \psi_{i_a}, \mathtt{Gen}_{i_a})\\
	(\alpha \imply \varphi_p, \mathtt{MP}).
	\end{gather*}
	\end{itemize}
	\end{proof}
	
	O metateorema da dedução pode ser interpretado em termos de provas em linguagem natural. A ideia básica é que se partirmos de um objeto $x$ arbitrário que satisfaz uma propriedade $\alpha$ e mostrarmos que então ele satisfaz uma propriedade $\beta$ então `$x$ satisfaz $\alpha$ implica $x$ satisfaz $\beta$'. No entanto, isto pode falhar se em algum momento pararmos de nos referir ao mesmo $x$. Isto é, se em algum momento dissermos `como $x$ é arbitrário, esta propriedade intermédia é verdadeira para todo o $x$...' já não estamos a assumir que um $x$ particular satisfaz $\alpha$, mas sim que qualquer $x$ satisfaz $\alpha$.
	
	Na realidade, a hipótese de não haver generalizações em variáveis livres de $\alpha$ pode ser enfraquecida. No entanto, o caso por nós demonstrado é suficiente para a vasta maioria das aplicações práticas. Para uma versão ligeiramente mais forte do metateorema da dedução, ver \cite{fltc}, p. 86.
	
	Um caso particular do metateorema da dedução acima provado vê muita utilidade na prática.
	
	\begin{prop*}
	Dizemos que uma fórmula $\alpha$ é \emph{fechada} se $\fv \alpha = \emptyset$.
	
	Se $\Gamma \subseteq \F$, $\alpha, \beta \in \F$ e $\alpha$ é uma fórmula fechada então
	
	\[\Gamma, \alpha \vdash \beta \text{ sse } \Gamma \vdash \alpha \imply \beta.\]
	\end{prop*}
	
	As fórmulas fechadas são, em alguns sentidos, muito mais simpáticas do que fórmulas gerais. Exemplificamos outra propriedade destas, que nos será útil no futuro.
	
	\begin{prop}\label{fol:closednegnvdash}
	Recordamos o leitor (Observação \ref{structuresatisfactionnotnice}) que, em geral, se $\varphi$ é uma fórmula e $I$ uma estrutura de interpretação, não é necessário que $I \Vdash \neg \varphi$ sse $I \nVdash \varphi$. No entanto, isto é verdade se adicionarmos a restrição de $\varphi$ ser fechada. Isto é, se $\varphi$ é uma fórmula fechada,
	\[I \Vdash \neg \varphi \text{ sse } I \nVdash \varphi.\]
	\end{prop}
	
	\begin{proof}
	($\leftarrow$) Se $I \nVdash \varphi$ então existe pelo menos uma valoração $\rho$ sobre $I$ tal que $\rho_F(\varphi) = \lf$, isto é, $\rho_F(\neg \varphi) = \lt$. Pelo lema das variáveis omissas, como $\neg \varphi$ não tem variáveis livres, $\rho'_F(\neg \varphi) = \lt$ para qualquer valoração $\rho'$, donde $I \Vdash \neg \varphi$.
	
	($\rightarrow$) Suponha-se que $I \Vdash \neg \varphi$. Então, para qualquer valoração $\rho$ sobre $I$ temos $\rho_F(\neg \varphi) = \lt$. Se $\rho$ for uma valoração qualquer particular (é aqui necessária a hipótese de o universo $U_I$ ser não-vazio, para haver pelo menos uma valoração!) então $\rho_F(\neg \varphi) = \lt$ donde $\rho_F(\varphi) = \lf$ e então $I \nVdash \varphi$.
	\end{proof}
	
	\section{Completude (Preliminar)}
	\setcounter{ideia}{0}
	\setcounter{obstaculo}{0}
	
	Começamos a esboçar a demonstração da completude do cálculo de Hilbert, isto é: Se $\Gamma \vDash \varphi$ então $\Gamma \vdash \varphi$. Este teorema tem o nome de `Teorema da completude de Gödel'. Nesta secção, estabelecemos algumas simplificações e proposições que serão necessárias para o mostrar.
	
	Tal como no caso do cálculo proposicional, é conveniente passar o problema ao contrarrecíproco. Isto é, mostrar
	\[\text{Se } \Gamma \nvdash \varphi \text{ então } \Gamma \nvDash \varphi.\]
	
	A razão é idêntica: é mais fácil construir valorações `resolvendo equações' do que construir demonstrações.
	
	Apresentaremos algumas simplificações que não foram feitas no caso proposicional. Começamos por supor, sem perda de generalidade, que $\varphi$ é uma fórmula fechada. Fazemos isto porque fórmulas fechadas são mais fáceis de mexer. Em particular, é possível aplicar nelas o metateorema da dedução com impunidade. Para mostrar que podemos trabalhar sob esta hipótese, sejam $\bf x_1, \dots, \bf x_n$ as variáveis livres de $\varphi$ (deixamos a verificação de que há um número finito destas para o leitor). Então, se definirmos
	\[\forall \varphi \text{ como } \forall_{\bf x_1} \dots \forall_{\bf x_n} \varphi,\]
	temos que $\varphi \vdash \forall \varphi$ por aplicação repetida de generalização, e $\forall \varphi \vdash \varphi$ por aplicação repetida do axioma $\forall_{\bf x} \alpha \imply \alpha$ (verifique que esta fórmula é de facto verdadeira. Sugestão: ver casos em $\rho_F(\alpha)$). Como tal,
	\[\Gamma \vdash \varphi \text{ sse } \Gamma \vdash \forall \varphi,\]
	e o mesmo se verifica para a relação de consequência global. Assim sendo, \textbf{não há perda de generalidade em supor que $\varphi$ é fechada.}
	
	Supor que $\varphi$ é fechada permite-nos aplicar o metateorema da contradição (que estamos prestes a provar) para, em certo sentido, juntar $\varphi$ às fórmulas de $\Gamma$. A ideia por trás disto é a seguinte: mostrar que $A$ implica $B$ é o mesmo que, a partir de $A$ e $\neg B$, deduzir uma contradição. Na lógica de primeira ordem, isto funciona tanto no nível semântico como no nível sintático.
	
	\begin{prop}
	Se $\Gamma \subseteq \F$, $\varphi$ é uma fórmula fechada e $\tau$ é uma fórmula verdadeira (por exemplo, $\varphi \imply \varphi$), então
	\[\Gamma \vDash \varphi \text{ sse } \Gamma, \neg \varphi \vDash \neg \tau\]
	e também (Metateorema da contradição)
	\[\Gamma \vdash \varphi \text{ sse } \Gamma, \neg \varphi \vdash \neg \tau.\]
	\end{prop}
	
	\begin{proof}
	A primeira afirmação é feita mostrando que as negações das duas afirmações são equivalentes. A afirmação $\Gamma \nvDash \varphi$ é equivalente à existência de uma estrutura de interpretação $I$ tal que $I \Vdash \Gamma$ e $I \nVdash \varphi$. Devido a $\varphi$ ser fechada (ver acima!) este último é equivalente a $I \Vdash \neg \varphi$ (ver proposição \ref{fol:closednegnvdash}) pelo que uma estrutura de interpretação $I$ satisfaz o desejado sse $I \Vdash \Gamma \cup \{\neg \varphi\}$. A condição que tal estrutura exista pode ser expressa em termos de $\vDash$. Em particular, a condição que tal estrutura \emph{não} exista pode ser escrita dizendo que qualquer estrutura $I$ que satisfaça $\Gamma \cup \{\neg \varphi\}$ também satisfaz uma coisa falsa. Isto é, existe $I$ tal que $I \Vdash \Gamma \cup \{\neg \varphi\}$ se e só se $\Gamma \cup \{\neg \varphi\} \nvDash \neg \tau$. Isto conclui a demonstração da afirmação semântica.
	
	Demonstramos agora o metateorema da contradição. ($\rightarrow$) Suponha-se que $\Gamma \vdash \varphi$. Então, como a seguinte fórmula é uma tautologia (verifique)
	\[\varphi \imply (\neg \varphi \imply \neg \tau)\]
	é fácil construir a demonstração de $\Gamma, \neg \varphi \vdash \neg \tau$ da seguinte forma:
	
	\begin{align*}
	&\text{...(Demonstração de $\Gamma \vdash \varphi$)...}\\
	&\varphi \imply (\neg \varphi \imply \neg \tau)&&\texttt{Ax}\\
	&\neg \varphi \imply \neg \tau&&\texttt{MP}\\
	&\neg \varphi&&\texttt{Hip}\\
	&\neg \tau&&\texttt{MP}.
	\end{align*}
	
	($\leftarrow$) Suponha-se agora que $\Gamma, \neg \varphi \vdash \neg \tau$. Pelo metateorema da dedução temos (como $\neg \varphi$ é fechado)
	\[\Gamma \vdash \neg \varphi \imply \neg \tau.\]
	Concatenando à prova disto as linhas:
	\begin{align*}
	&(\neg \varphi \imply \neg \tau) \imply (\tau \imply \varphi)&&\texttt{Ax}\\
	&\tau \imply \varphi&&\texttt{MP}\\
	&\tau&&\texttt{Ax}\\
	&\varphi&&\texttt{MP}
	\end{align*}
	obtemos uma demonstração com base em $\Gamma$ de $\varphi$, como desejado.
	\end{proof}
	
	Com base nesta proposição, reduzimos a questão de mostrar completude a mostrar que se $\Gamma \cup \{\neg \varphi\} \nvdash \neg \tau$ então $\Gamma \cup \{\neg \varphi\} \nvDash \neg \tau$. Por sua vez, esta questão pode ser ligeiramente generalizada.
	
	\begin{definicao}
	Em tudo o que se segue, considera-se fixa uma afirmação verdadeira $\tau$ na assinatura $\Sigma$.\footnote{É fácil construir afirmações verdadeiras, sabendo que (proposição \ref{fol:size}) existe pelo menos uma fórmula $\alpha$, pode-se considerar $\tau$ como sendo $\alpha \imply \alpha$.}
	
	Dizemos que um conjunto de fórmulas $\Gamma$ é \emph{incoerente} se $\Gamma \vdash \neg \tau$.
	\end{definicao}
	
	\begin{ideia}
	Para provar o teorema da completude é suficiente mostrar que, se $\Gamma$ é um conjunto de fórmulas coerente (i.e. $\Gamma \nvdash \neg \tau$) então existe uma estrutura de interpretação $I$ tal que $I \Vdash \Gamma$ (i.e. $\Gamma \nvDash \neg \tau$). É este o ponto de vista que tomaremos para mostrar o teorema da completude. Por outras palavras, começaremos com um conjunto coerente $\Gamma$ e construiremos um \emph{modelo do conjunto}, isto é, uma estrutura de interpretação $I$ tal que $I \Vdash \Gamma$.
	\end{ideia}
	
	\begin{obstaculo}
	Note-se que está aqui subjacente uma dificuldade que não aparecia no caso proposicional. Enquanto que no caso do cálculo proposicional, o universo onde as variáveis viviam já nos era conhecido ($\{\lt, \lf\}$), em lógica de primeira ordem é preciso criar este conjunto. Para este objetivo, na falta de alternativa mais sugestiva, usaremos aquilo que temos à mão: símbolos.
	\end{obstaculo}
	
	\begin{ideia}\label{ideia:simb}
	Para definir uma estrutura de interpretação $I$, podemos usar como universo $U_I$ o conjunto dos termos, $\T_\Sigma$. Para evitar ambiguidades, não permitimos variáveis nos nossos termos. Por outras palavras, restringimo-nos ao conjunto dos termos fechados, isto é,
	\[\T^f_\Sigma = \{\, t \in \T_\Sigma \mid \var t = \emptyset\,\}.\]
	
	Para cada símbolo de função $f \in F_\Sigma$, definimos simplesmente $f_I(t_1, \dots, t_n)$ como sendo a fórmula
	
	\begin{center}
	\Tree [.$f$ $t_1$ $\dots$ $t_n$ ]
	\end{center}
	
	Por outras palavras, a função é aplicada `simbolicamente'.
	
	Finalmente, para cada símbolo de predicado $p \in P_\Sigma$, podemos tentar definir $p_I(t_1, \dots, t_n)$ como $\lt$ se $\Gamma \vdash p(t_1, \dots, t_n)$.
	\end{ideia}
	
	\begin{obstaculo}
	Poderá não haver termos fechados de todo, em particular se não houver nenhum símbolo de função de aridade zero. No entanto, numa estrutura de interpretação é necessário o universo ser não-vazio. Como tal, poderá ser necessário adicionar alguns termos novos para servir de semente, para criar mais termos.
	\end{obstaculo}
	
	\begin{obstaculo}
	A definição dada de $p_I$ não funciona, porque é possível acontecer uma situação das seguintes:
	
	Suponha-se haver dois símbolos unários de predicado, $p$ e $q$. Seja $\Gamma = \{p(x) \eqv \neg q(x)\}$. Então, sob a construção dada, $p_I(x) = q_I(x) = \lf$, mas isto não satisfaz $\Gamma$.
	
	É possível adaptar um raciocínio `incremental' como o que foi feito para o caso proposicional (onde íamos adicionando condições à medida que estabelecíamos valores para variáveis) mas tomaremos uma abordagem diferente.
	\end{obstaculo}
	
	Uma ideia algo comum na matemática é a seguinte: é mais fácil resolver sistemas determinados do que sistemas indeterminados. Isto pois, em sistemas determinados, é possível fazer escolhas `localmente' sem medo de haver conflitos por escolhas mal-feitas, como acima. Assim sendo, podemos considerar `adicionar equações ao sistema até ele ficar determinado', tendo apenas que ter cuidado de em nenhum momento o transformar num sistema impossível.
	
	É esta a ideia subjacente ao \emph{Lema de Lindenbaum}. A noção de sistema `impossível', isto é, sem soluções, corresponde a um conjunto incoerente de fórmulas. Para formalizar a noção de sistema determinado, é necessária a noção de exaustividade:
	
	\begin{definicao}
	Um conjunto $\Gamma \subseteq \F_\Sigma$ de fórmulas diz-se \emph{exaustivo} se, para qualquer fórmula $\varphi \in \F_\Sigma$ \emph{fechada}, ou $\Gamma \vdash \varphi$ ou $\Gamma \vdash \neg \varphi$.
	\end{definicao}
	
	Por outras palavras, para qualquer fórmula fechada $\varphi$, $\Gamma$ consegue-nos dizer com certezas se $\varphi$ é verdadeira ou falsa, sem haver ambiguidade.
	
	Note-se a hipótese de $\varphi$ ser uma fórmula fechada. Isto é necessário, pois há fórmulas abertas tal que nem ela nem a sua negação são verdadeiras. Por exemplo, `$n$ é par' não é uma fórmula verdadeira no contexto dos naturais (porque é falsa para, por exemplo, $n = 1$), mas a sua negação, `$n$ é ímpar' também não é verdadeira.
	
	\begin{prop}
	(Lema de Lindenbaum)
	
	Seja $\Gamma$ um conjunto coerente de fórmulas. Então, existe um conjunto $\Gamma^e$ coerente e exaustivo de fórmulas tal que $\Gamma \subseteq \Gamma^e$.
	\end{prop}
	
	\begin{proof}
	Para mostrar esta proposição, construímos $\Gamma^e$ incrementalmente, adicionando fórmulas uma a uma, exceto se elas causam incoerência.
	
	Primeiro que tudo, recordamos o leitor (proposição \ref{fol:size}) que há um número contável de fórmulas. Seja $\varphi_1, \varphi_2, \dots$ uma enumeração de todas as fórmulas.
	
	Defina-se $\Gamma_0 = \Gamma$, e estando definido $\Gamma_n$, construímos $\Gamma_{n+1}$ como:
	
	\[\Gamma_{n+1} = \begin{cases}
	\Gamma_n \cup \{\varphi_n\}&\text{Se $\Gamma_n \cup \{\varphi_n\}$ é coerente}\\
	\Gamma_n&\text{Caso contrário}
	\end{cases}\]
	
	Defina-se $\Gamma^e = \bigcup_{n=0}^\infty \Gamma_n$. Desejamos mostrar que $\Gamma^e$ é coerente e exaustivo.
	
	Para mostrar que é coerente, suponha-se, por absurdo, que $\Gamma^e \vdash \neg \tau$. Então, existe uma demonstração de $\neg \tau$ usando hipóteses em $\Gamma^e$. Mas uma demonstração usa apenas um conjunto finito $\psi_1, \dots, \psi_n$ de hipóteses. Visto que cada hipótese pertence a algum $\Gamma_N$, tomando o máximo dos $N$ correspondentes a cada $\psi$, temos que $\Gamma_N \vdash \neg \tau$. Assim sendo, algum $\Gamma_N$ é incoerente.
	
	Mostramos agora por indução que isto não pode acontecer. $\Gamma_0$ é coerente por hipótese. Se $\Gamma_n$ é coerente, examinando a construção, é fácil mostrar que $\Gamma_{n+1}$ é coerente. Assim sendo, todos os $\Gamma_n$ são coerentes, que é uma contradição, e portanto $\Gamma^e$ é também coerente.
	
	Justificamos agora que $\Gamma^e$ é exaustivo. Fixe-se uma fórmula \emph{fechada} $\varphi$. Esta fórmula consta na enumeração das fórmulas. Suponha-se $\varphi = \varphi_n$. Então, há dois casos:
	
	\begin{itemize}
	\item Se $\Gamma_n \cup \{\varphi_n\}$ é coerente, então $\varphi \in \Gamma_{n+1}$, pelo que $\Gamma_{n+1} \vdash \varphi$ e então $\Gamma^e \vdash \varphi$.
	
	\item Caso contrário, $\Gamma_n, \varphi \vdash \neg \tau$. Assim sendo, como $\varphi$ é fechada, podemos aplicar o metateorema da contradição\footnote{É preciso fazer uma ligeira adaptação, pois o metateorema diz, em rigor, que de $\Gamma_n, \neg \neg \varphi \vdash \neg \tau$ se conclui $\Gamma_n \vdash \neg \varphi$. No entanto, usando o facto que $\varphi \imply \neg \neg \varphi$ é uma tautologia, é possível adaptar a demonstração para justificar o desejado.} para deduzir $\Gamma_n \vdash \neg \varphi$, e então $\Gamma^e \vdash \neg \varphi$.
	\end{itemize}
	
	Em qualquer dos casos, ou $\Gamma^e \vdash \varphi$ ou $\Gamma^e \vdash \neg \varphi$, o que mostra que $\Gamma^e$ é exaustivo.
	\end{proof}
	
	\begin{ideia}
	No seguimento da ideia \ref{ideia:simb}, podemos definir $p_I$ como sugerido após aplicar o lema de Lindenbaum. A definição fica, aliás, ligeiramente mais específica:
	\[p_I(t_1, \dots, t_n) = \begin{cases}
	\lt &\text{se $\Gamma^e \vdash p(t_1, \dots, t_n)$}\\
	\lf &\text{se $\Gamma^e \vdash \neg p(t_1, \dots, t_n)$}
	\end{cases}\]
	
	A exaustividade terá (esperamos) forçado as nossas escolhas de modo a não permitir escolhas incoerentes. A nossa esperança continua a ser que (ignorando por agora a possibilidade de o universo ser vazio) a estrutura de interpretação $I$ assim definida satisfará $\Gamma$. A forma mais direta de tentar mostrar isto é aliás mostrar algo ligeiramente mais forte: se $\varphi$ é uma fórmula, então $\Gamma^e \vdash \varphi$ sse $I \Vdash \varphi$.
	
	Não faremos a demonstração completa agora, mas faremos um esboço, visto que há um obstáculo importante do qual é preciso tomar nota. Suponha-se sem perda de generalidade que $\varphi$ é fechada (pois caso contrário considere-se $\forall \varphi$) e examine-se os casos para indução na estrutura de $\varphi$.
	
	Se $\varphi$ é da forma $p(t_1, \dots, t_n)$, todos os $t_i$ têm que ser fechados (por $\varphi$ ser fechada) e como tal, por definição de $p_I$, temos que
	\[p_I(t_1, \dots, t_n) = \lt \text{ sse } \Gamma^e \vdash p(t_1, \dots, t_n).\]
	
	No entanto, fixa uma valoração arbitrária $\rho$ (que existe sob a hipótese de existir pelo menos um termo fechado), $p_I(t_1, \dots, t_n) = \rho_F(p(t_1, \dots, t_n))$. Assim sendo,
	\[p_I(t_1, \dots, t_n) = \lt \text{ sse } \rho \Vdash p(t_1, \dots, t_n),\]
	e pelo lema das variáveis omissas, tendo em conta que esta fórmula não tem variáveis,
	\[\rho \Vdash p(t_1, \dots, t_n) \text{ sse } I \Vdash p(t_1, \dots, t_n),\]
	como desejado.
	
	Considere-se agora o caso de a fórmula $\varphi$ ser da forma $\neg \psi$. Então, por $\varphi$ ser fechada, $\psi$ também o é, pelo que podemos aplicar a hipótese de indução. Isto é,
	\[\Gamma^e \vdash \psi \text{ sse } I \Vdash \psi.\]
	
	Por $\psi$ ser fechada, sabemos que $I \Vdash \psi$ sse $I \nVdash \varphi$. Pelo outro lado, pela coerencia e exaustividade de $\Gamma^e$, temos que $\Gamma^e \vdash \psi$ sse $\Gamma^e \nvdash \varphi$. Isto conclui este passo de indução.
	
	O caso para $\varphi$ ser da forma $\alpha \imply \beta$ é muito semelhante, mas a parte complicada é aplicar o passo de indução com quantificadores.
	\end{ideia}
	
	\begin{obstaculo}
	A estrutura de interpretação sugerida até agora pode não jogar bem com quantificadores.
	
	Pela definição de interpretação de fórmulas com quantificadores, a condição necessária e suficiente para $I$ satisfazer uma fórmula da forma $\forall_{\bf x} \varphi$ é que, para qualquer $u$ no universo (isto é, para qualquer termo fechado) se tenha $\rho\!\downarrow^{\bf x}_u \Vdash \varphi$. Por outras palavras, intuitivamente, substituindo a variável $\bf x$ por qualquer termo fechado $u$ em $\varphi$ se obtenha sempre uma fórmula verdadeira.
	
	Queremos que $I \Vdash \forall_{\bf x} \varphi$ sse $\Gamma^e \vdash \forall_{\bf x} \varphi$. Supondo que a hipótese de indução é aplicável, obtém-se uma condição algo forte no nosso universo de fórmulas para podermos fazer o passo de indução.
	
	Usemos a seguinte notação (que será definida rigorosamente mais tarde) para substituição de variáveis. Dada uma fórmula $\varphi$, uma variável $\bf x$ e um termo $t$, denotamos a fórmula obtida de $\varphi$ por substituição de $\bf x$ por $t$ como $\varphi_{\bf x}[t]$. Com esta linguagem o que precisamos é que $\Gamma^e \vdash \forall_{\bf x} \varphi$ sse, para qualquer termo fechado $u$, $\Gamma^e \vdash \varphi_{\bf x}[u]$.
	
	Uma das implicações ($\rightarrow$) é sempre verdade, como veremos mais tarde. De facto, a implicação $\forall_{\bf x} \varphi \imply \varphi_{\bf x}[t]$ é sempre uma fórmula verdadeira para termos $t$ fechados. A outra implicação é algo rara, e requer, em certo sentido, que hajam suficientes símbolos de constante no nosso universo.
	
	Vejamos um exemplo em que essa implicação falha. Considere-se um conjunto de axiomas $\Gamma_\Z$ que descreva os números inteiros, mas que tem apenas os símbolos de função $\{0, 1, +\}$. Isto é, excluímos o símbolo de função unário `$-$' que dá o simétrico de um elemento, e escrevemos o axioma que afirma a existência de simétrico na forma $\forall_x \exists_y (x + y = 0)$. Estes axiomas descreveriam os inteiros, mas o conjunto de termos expressíveis a partir da assinatura não engloba todos os inteiros possíveis. E de facto, para qualquer termo fechado $t$ temos que a fórmula $0 \leq t$ é verdadeira, visto que os únicos termos fechados representam números naturais, mas a fórmula $\forall_x (0 \leq x)$ é falsa nos inteiros. Por outras palavras, se $\varphi$ é a fórmula $0 \geq x$, temos que $\Gamma_\Z \vdash \varphi_x[t]$ para todo o termo fechado $t$, e no entanto $\Gamma_\Z \vdash \neg \forall_x \varphi$.
	\end{obstaculo}
	
	Este último obstáculo mostra que será necessário adicionar termos fechados. Em particular, adicionaremos constantes suficientes para que a implicação `se $\Gamma \vdash \varphi_{\bf x}[t]$ para todo o $t$ então $\Gamma \vdash \forall_{\bf x} \varphi$' seja verdadeira.
	
	Note-se que através de um par de aplicações de contrarrecíproco e meta-contrarrecíproco, a afirmação acima é equivalente a dizer que, (no contexto de $\Gamma$ coerente e exaustivo) para qualquer $\varphi$, se $\Gamma \vdash \exists_{\bf x} \varphi$ então existe um termo fechado $t$ tal que $\Gamma \vdash \varphi_{\bf x}[t]$.\footnote{Oops, ainda não defini o símbolo $\exists_{\bf x}$. Bem, ele é definido por abreviatura como $\neg \forall_{\bf x} \neg$. Agradeço sugestões de onde enfiar a definição. Ah, e não me posso esquecer de remover esta footnote da versão final.} %todo delete this
	Por outras palavras, para todas as afirmações que afirmam a existência de um objeto que satisfaz uma certa propriedade existe pelo menos um termo fechado que a satisfaz. Normalmente referimo-nos a tais termos como `testemunhas'. Assim sendo, é razoável enumerar todas as afirmações verdadeiras da forma $\exists_{\bf x} \varphi$ e adicionar um símbolo de constante que servirá de sua testemunha. É esta a ideia por trás da chamada \emph{construção de Henkin}.
	
	\begin{ideia}
	Antes de construir o modelo acima descrito, adicionar um conjunto de termos que servirá de testemunhas, e a seguir aplicar os procedimentos acima (completar $\Gamma$ e construir o modelo `simbólico'), na esperança que, com a nossa teoria cheia de testemunhas, o modelo tenha constantes suficientes para que uma afirmação verdadeira da forma $\exists_{\bf x} \varphi$ seja satisfeita pelo modelo.
	\end{ideia}
	
	\begin{obstaculo}
	Este processo requer mudar a assinatura. De facto, estamos a adicionar símbolos de constantes. Isto não é problemático do lado semântico, visto que se sabemos interpretar todos os símbolos em $\Sigma$ e mais algumas constantes, sabemos em particular interpretar todos os símbolos em $\Sigma$, mas poderia hipoteticamente causar distúrbios na parte sintática.
	
	Mais concretamente: seja $\Sigma$ a assinatura original, $\Sigma^+$ a assinatura com os símbolos de constante adicionados, $\Gamma$ o conjunto de fórmulas inicial e $\widehat\Gamma$ o conjunto de fórmulas modificado (em $\Sigma^+$) que consiste em $\Gamma$, juntamente com as afirmações que relacionam as testemunhas com as afirmações. Isto é, para cada $\varphi$ tal que $\Gamma \vdash \exists_{\bf x} \varphi$, uma afirmação da forma $\varphi_{\bf x}[c]$ para um símbolo de constante $c$ adequado.
	
	Esperamos, neste contexto mais forte, conseguir demonstrar que, para todo o $\varphi \in F_{\Sigma^+}$,
	\[\widehat\Gamma \vdash \varphi \text{ sse } I \Vdash \varphi.\]
	
	De seguida, o plano é transpor este resultado para o contexto original.
	
	Para todo o $\varphi \in F_{\Sigma}$, decerto que
	\[\widehat\Gamma \vdash \varphi \text{ sse } I \Vdash \varphi,\]
	visto que $F_{\Sigma} \subseteq F_{\Sigma^+}$. Seria agora desejado mostrar que $\widehat\Gamma \vdash \varphi$ sse $\Gamma \vdash \varphi$. Mas há aqui um detalhe escondido: enquanto que a demonstração cuja existência é afirmada na afirmação $\widehat\Gamma \vdash \varphi$ poderá usar qualquer símbolo em $\Sigma^+$, a demonstração cuja existência pretendemos assegurar com a afirmação $\Gamma \vdash \varphi$ não poderá usar nenhum dos símbolos de constante novos. Isto é, para transformar uma demonstração de $\widehat\Gamma \vdash \varphi$ numa de $\Gamma \vdash \varphi$ não chega eliminar as hipóteses adicionadas: é também necessário eliminar qualquer uso dos símbolos novos. Felizmente, isto é um obstáculo superável sem excessiva dificuldade.
	\end{obstaculo}
	
	Sucede que este é o último obstáculo ao teorema de completude, pelo que podemos seguir com a demonstração propriamente dita. Recapitulando o plano de ataque:
	
	\begin{itemize}
	\item Adicionar símbolos de constantes para servir de testemunhas para afirmações existenciais. Chamemos à assinatura nova, com estes símbolos de constantes, $\Sigma^+$.
	
	\item Completar o conjunto de fórmulas resultante, usando o lema de Lindenbaum. Chamemos ao conjunto de fórmulas resultante $\widehat\Gamma$.
	
	\item Construir o modelo `sintático' descrito acima e provar que
	\[\widehat\Gamma \vdash \varphi \text{ sse } I \Vdash \varphi,\]
	para $\varphi$ em $F_{\Sigma^+}$.
	
	\item Transpor este resultado para o contexto original de $\Sigma$, encontrando forma de eliminar os símbolos de constante em provas de $\widehat\Gamma \vdash \varphi$.
	\end{itemize}
	
	\section{Completude}
	
	Para dar início à demonstração do teorema de completude de Gödel, começamos por descrever em mais detalhe a construção de Henkin.
	
	Recordemo-nos que, sob as nossas hipóteses, existe um número contável de fórmulas. Para a construção de Henkin, estamos interessados apenas naquelas que têm apenas uma variável livre.
	
	Sejam $c_1, c_2, \dots$ um conjunto numerável de símbolos novos. Isto é, partimos do princípio que estes símbolos não são símbolos em $\Sigma$ nem variáveis. Definimos a assinatura $\Sigma^+$ como sendo $\Sigma$, com os símbolos $c_n$ adicionados como símbolos de função constantes.
	
	Seja $\pi_n$ uma enumeração das fórmulas em $F_{\Sigma^+}$\footnote{Repare-se que os $\pi_n$ percorrem todas as fórmulas de $F_{\Sigma^+}$, não só de $F_\Sigma$!} que têm apenas uma variável livre, e seja $\bf x_n$ a variável livre correspondente a $\pi_n$. Por razões técnicas, exigimos também que o símbolo de constante $c_n$ não conste em $\pi_n$. Isto é fácil de assegurar, se necessário reordenando a sequência.
	
	Para cada $n$, definimos a seguinte fórmula em $F_{\Sigma^+}$:
	
	\[\psi_n : (\exists_{\bf x_n} \pi_n) \imply \pi_n[c_n],\]
	onde $\pi_n[c_n]$ representa a fórmula $\pi_n$ com a variável $\bf x_n$ substituída por $c_n$. Isto requer definir a noção de substituição.
	
	\begin{definicao}
	Seja $\theta$ um termo, $\bf x$ uma variável e $t$ um termo. Definimos o termo $\theta_{\bf x}[t]$ indutivamente em $\theta$:
	
	\begin{itemize}
	\item Se $\theta$ é $\bf x$, isto é $t$,
	
	\item Se $\theta$ é qualquer outra variável, isto é $\theta$,
	
	\item Se $\theta$ é da forma $f(\theta_1, \dots, \theta_n)$, definimos $\theta_{\bf x}[t]$ como
	\[f(\theta_1[t], \dots, \theta_n[t]),\]
	onde estas substituições são feitas sobre $\bf x$.
	\end{itemize}
	
	Seja $\varphi$ uma fórmula, $\bf x$ uma variável e $t$ um termo. Definimos a fórmula $\varphi_{\bf x}[t]$ indutivamente em $\varphi$:
	
	\begin{itemize}
	\item Se $\varphi$ é da forma $p(\theta_1, \dots, \theta_n)$, definimos $\varphi_{\bf x}[t]$ como
	\[p(\theta_1[t], \dots, \theta_n[t]),\]
	
	\item Se $\varphi$ é da forma $\neg \psi$, definimos $\varphi[t]$ como $\neg \psi[t]$ e semelhante para o caso de implicação,
	
	\item Se $\varphi$ é da forma $\forall_{\bf y} \psi$, definimos $\varphi_{\bf x}[t]$ como:
	
	\begin{itemize}
	\item $\forall_{\bf y} \psi_{\bf x}[t]$ se as variáveis $\bf x$ e $\bf y$ forem \textbf{diferentes},
	
	\item $\varphi$ se $\bf y$ e $\bf x$ são a mesma variável.
	\end{itemize}
	\end{itemize}

	Quando for claro de contexto a variável que estamos a substituir, escreveremos $\theta[t]$ em vez de $\theta_{\bf x}[t]$.
	\end{definicao}
	
	A construção de Henkin consiste em pegar em $\Gamma \subseteq F_{\Sigma}$ e adicionar-lhe todas as fórmulas $\psi_n$, obtendo um novo conjunto $\Gamma^\exists \subseteq F_{\Sigma^+}$. Desejamos mostrar que se $\Gamma$ é coerente então $\Gamma^\exists$ é também coerente. A demonstração disto é feita em duas partes:
	
	\begin{itemize}
	\item Mostrar que se $\Gamma$ é coerente como subconjunto de $F_\Sigma$ então também é coerente como subconjunto de $F_{\Sigma^+}$,
	
	\item Mostrar que se $\Gamma$ é coerente então $\Gamma^\exists$ é coerente.
	\end{itemize}
	
	Começamos por justificar a primeira afirmação. Note-se que ela não é óbvia. De facto, se pensarmos em $\Gamma$ como subconjunto de $F_{\Sigma^+}$ temos ao nosso dispor mais símbolos, e como tal mais fórmulas e mais demonstrações. Como tal, não é totalmente descabido conceber que possa aparecer uma demonstração nova que justifique uma contradição. Felizmente, isto não sucede, como a próxima proposição demonstra.
	
	A ideia é que, sem nenhuns axiomas que se refiram a eles, os símbolos de constante novos funcionam efetivamente como variáveis. Como tal, se substituirmos os símbolos de constante por variáveis, a prova continuará igualmente válida.
	
	\begin{prop}\label{coherenceinplus}
	Seja $\Gamma \subseteq F_\Sigma$ e $\varphi \in F_\Sigma$. Suponha-se que $\Gamma \vdash \varphi$ como conjunto de fórmulas em $F_{\Sigma^+}$. Então, $\Gamma \vdash \varphi$ como conjunto de fórmulas em $F_\Sigma$.
	
	Em particular, se $\Gamma$ é incoerente em $\Sigma^+$ também o é em $\Sigma$.
	\end{prop}
	
	\begin{proof}
	Começamos por formalizar a noção de `substituir os símbolos de constante por variáveis'.
	
	Comecemos por supor que $\Gamma$ é finito. Podemos fazer isto porque numa demonstração é usado um número finito de hipóteses. Então, no decurso da demonstração é usado um número finito de variáveis, pelo que temos um número infinito de variáveis $\bf v_1, \bf v_2, \dots$ `novas' ao nosso dispor.
	
	Dada uma fórmula $\psi \in F_{\Sigma^+}$, definimos temporariamente $\psi'$ como sendo uma cópia de $\psi$ em que cada símbolo de constante $c_n$ é substituído por $\bf v_n$. Note-se que se $\psi \in F_\Sigma$ então $\psi' = \psi$, como é o caso para as fórmulas em $\Gamma$ e para $\varphi$. A ideia é substituir todas as fórmulas na demonstração pela sua versão modificada.
	
	Seja $(\varphi_1, j_1), \dots, (\varphi_N, j_N)$ a demonstração de $\Gamma \vdash \varphi$ em $\Sigma^+$. Justificamos que
	\[(\varphi'_1, j_1), \dots, (\varphi'_N, j_N)\]
	é também uma demonstração válida, mostrando que cada passo individualmente é válido.
	
	Como tal, seja $(\varphi_n, j_n)$ um passo da demonstração. Observemos os casos possíveis em $j_n$:
	
	\begin{enumerate}
	\item Se $j_n = \mathtt{MP}$, claramente $(\varphi'_n, j_n)$ é um passo válido na demonstração nova, visto que terão havido passos anteriores da forma $\alpha' \imply \varphi'_n$ e $\alpha'$.
	
	\item Se $j_n = \mathtt{Gen}$, o mesmo raciocínio é válido.
	
	\item Se $j_n = \mathtt{Hip}$, $\varphi'_n = \varphi_n \in \Gamma$, pelo que $\varphi'_n$ pode também ser invocado como hipótese.
	
	\item Finalmente, o caso $j_n = \mathtt{Ax}$ é o mais complicado. Este é também um dos casos nos quais ter menos axiomas é melhor. De facto, com o conjunto limitado de axiomas que teremos eventualmente, este caso é tão trivial como os outros. Assim sendo, um leitor que pretenda ler a secção de axiomática poderá ignorar o resto da demonstração. No entanto, para não deixar lacunas, prosseguimos com a demonstração partindo do princípio que qualquer afirmação verdadeira pode ser usada como axioma.
	
	Suponha-se que $\varphi_n$ é uma afirmação verdadeira em $F_{\Sigma^+}$. Mostramos que $\varphi'_n$ é uma afirmação verdadeira em $F_\Sigma$.
	
	Seja $I$ uma estrutura de interpretação sobre $\Sigma$ e $\rho$ uma valoração arbitrária sobre $I$. Desejamos mostrar que $\rho \Vdash \varphi'_n$.
	
	Construa-se a estrutura de interpretação $\tilde I$ sobre $\Sigma^+$ como sendo uma cópia de $I$, com a adição dos símbolos de constante, que são interpretados como $(c_n)_I = \rho(\bf v)$. Seja $\tilde \rho$ uma valoração idêntica a $\rho$, mas esta interpretada como valoração sobre $\tilde I$.
	
	É fácil mas trabalhoso mostrar por indução que para qualquer fórmula $\psi \in F_{\Sigma^+}$ se tem $\rho_F(\psi') = \tilde\rho_F(\psi)$. Como tal, sabendo que $\varphi_n$ é verdadeira em $F_{\Sigma^+}$ temos que $\tilde\rho_F(\varphi_n) = \lt$. Assim sendo, temos também $\rho_F(\varphi'_n) = \lt$. Como $\rho$ é uma valoração arbitrária, $I \Vdash \varphi'_n$ e como $I$ é arbitrário temos $\vDash \varphi'_n$, pelo que esta última fórmula pode ser invocada como axioma. Isto conclui a demonstração.
	\end{enumerate}
	
	Concluímos então que a prova modificada é uma prova válida com hipóteses em $\Gamma$, e a sua conclusão é $\varphi'_N$, que é igual a $\varphi'$ que por sua vez é $\varphi$. Assim sendo, $\Gamma \vdash \varphi$ em $\Sigma$ e a demonstração é dada por terminada.
	\end{proof}

	Repare-se que $\Gamma$ é coerente em $F_\Sigma$ sse o é em $F_{\Sigma^+}$. Mostrámos agora apenas uma das implicações, mas a outra implicação é óbvia. Assim sendo, não haverá ambiguidade no significado de `$\Gamma$ é coerente'.
	
	\begin{prop}
	Seja $\Gamma \subseteq F_\Sigma$. Usando a notação acima, se $\Gamma$ é coerente então $\Gamma^\exists$ é coerente.
	\end{prop}
	
	\begin{proof}
	Note-se que $\Gamma^\exists$ pode ser construido usando um processo indutivo. Em particular, defina-se $\Gamma_0 = \Gamma$ e, para cada $n$,
	\[\Gamma_{n+1} = \Gamma_n \cup \{(\exists_{\bf x_n} \pi_n) \imply \pi_n[c_n]\}.\]
	
	Então, $\Gamma^\exists$ é a união de todos os $\Gamma_n$, e como tal, por finitude de demonstrações, se todos os $\Gamma_n$ forem coerentes como subconjunto de $F_{\Sigma^+}$ então $\Gamma^\exists$ é também coerente.
	
	Sabemos que $\Gamma_0$ é coerente, por hipótese. Provemos então que se $\Gamma_n$ é coerente então $\Gamma_{n+1}$ também o é.
	
	Suponha-se, por absurdo, que $\Gamma_{n+1}$ é incoerente. Então $\Gamma_{n+1} \vdash \neg \tau$ para algum $\tau$ verdadeiro em $F_{\Sigma^+}$. Aplicando o metateorema da dedução, obtemos
	\[\Gamma_n \vdash ((\exists_{\bf x_n} \pi_n) \imply \pi_n[c_n]) \imply \neg \tau,\]
	e aplicando a equivalência tautológica $(\varphi \imply \neg \tau) \eqv (\neg \varphi)$ concluimos
	\[\Gamma_n \vdash \neg ((\exists_{\bf x_n} \pi_n) \imply \pi_n[c_n]).\]
	
	Por outras palavras, usando a definição por abreviatura do símbolo $\land$, a equivalência $\varphi \eqv \neg \neg \varphi$ e o metateorema de substituição de equivalentes do cálculo proposicional,
	\begin{equation}\label{eq:drinkersparadox}
\Gamma_n \vdash (\exists_{\bf x_n} \pi_n) \land \neg \pi_n[c_n].
	\end{equation}
	
	Podemos aplicar a proposição de \ref{coherenceinplus} para obter uma demonstração de
	\[\Gamma_n \vdash (\exists_{\bf x_n} \pi_n') \land \neg (\pi_n[c_n])'.\]
	
	Note-se que, como $\pi_n$ poderá conter vários símbolos de constante $c_k$, não nos basta substituir $c_n$ por uma variável nova $\bf y$... Seria necessário adicionar várias variáveis, $\bf y_1, \bf y_2$, e assim por diante. Para não sobrecarregar a notação, ignoraremos este problema e trabalharemos com os $c_k$ como se fossem variáveis em $X$, sabendo que na nossa cabeça estamos implicitamente a substitui-los por variáveis novas.

	Desejamos mostrar que a afirmação $(\exists_{\bf x_n} \pi_n) \land \neg \pi_n[c_n]$ é falsa, o que mostra que $\Gamma_n$ é incoerente e entra em contradição com a hipótese de indução. No entanto, isto pode não ser verdade, pelo que aplicamos generalização:
	\[\Gamma_n \vdash \forall_{c_n}((\exists_{\bf x_n} \pi_n) \land \neg \pi_n[c_n]).\]

	Recordamos que estamos a trabalhar pelo momento com os $c_k$ como se fossem variáveis. De facto, se $c_n$ fosse um símbolo de constante, a quantificação $\forall_{c_n}$ não faria sentido.
	
	Basta agora mostrar que a negação desta afirmação é um axioma para obter uma demonstração de algo da forma $\varphi \land \neg \varphi$, o que, aplicando a definição de conjunto incoerente com $\tau = (\neg \varphi \lor \varphi)$, mostra que $\Gamma_n$ é incoerente, em contradição com a hipótese de indução.
	
	Mostramos agora que a fórmula
	\[\neg \forall_{c_n}((\exists_{\bf x_n} \pi_n) \land \neg \pi_n[c_n])\]
	é um axioma. Antes de o justificar formalmente, vale a pena dedicar um parágrafo a entender a fórmula em questão e intuir porque é que ela haveria de ser verdade.
	
	A fórmula que desejamos mostrar é equivalente a
	\[\exists_{c_n}((\forall_{\bf x_n} \neg \pi_n) \lor \pi_n[c_n]),\]
	por deMorgan e metateorema da equivalência. Expandindo a definição de $\lor$, podemos escrever isto como
	\[\exists_{c_n}(\neg \pi_n[c_n] \imply \forall_{\bf x_n} \neg \pi_n).\]
	
	Esta afirmação tem um nome: o chamado `drinker's paradox', que pode ser enunciado em linguagem natural da seguinte forma: considere-se um bar não-vazio. Então, existe alguém neste bar tal que, se essa pessoa não está sóbria, ninguém está sóbrio. A afirmação acima pode ser interpretada como esta frase, em que `$\bf x$ está sóbrio' é representado como $\pi_n[\bf x]$.

Apesar do nome, esta afirmação não tem nada de paradoxical. A demonstração em linguagem natural é simples: ou ninguém está sóbrio, e neste caso escolha-se uma pessoa arbitrária e a implicação é verdadeira, ou então pelo menos uma pessoa está sóbria. Neste caso, escolha-se uma tal pessoa. Então, a implicação `se esta pessoa não está sóbria então ninguém está' é verdadeira porque o antecedente é falso. Isto conclui a demonstração.
	
	Passamos agora de conversa sobre bares para uma demonstração formal. Seja $I$ uma estrutura de interpretação e $\rho$ uma valoração sobre $I$. Pretendemos mostrar que
	
	\[\rho \Vdash \neg \forall_{c_n}((\exists_{\bf x_n} \pi_n) \land \neg \pi_n[c_n]).\]
	
	Isto é equivalente a mostrar que
	\[\rho_F(\forall_{c_n}((\exists_{\bf x_n} \pi_n) \land \neg \pi_n[c_n])) = \lf,\]
	
	o que acontece sse existe $u \in U_I$ tal que
	
	\begin{equation}\label{gammaexists:eq1}
	\rho\!\downarrow^{c_n}_u \nVdash (\exists_{\bf x_n} \pi_n) \land \neg \pi_n[c_n].
	\end{equation}
	
	Neste momento, a demonstração reduz-se a dois casos, de forma análoga à demonstração intuitiva atrás. Ou $\rho \Vdash \exists_{\bf x_n} \pi_n$ ou $\rho \nVdash \exists_{\bf x_n} \pi_n$.
	
	No segundo caso podemos aplicar o lema das variáveis omissas. Visto que $\bf x_n$ não é livre em $\exists_{\bf x_n} \pi_n$,
	\[\rho \Vdash \exists_{\bf x_n} \pi_n \text{ sse } \rho\!\downarrow^{c_n}_u \Vdash \exists_{\bf x_n} \pi_n, \]
	e como o antecedente é falso temos que $\rho\!\downarrow^{c_n}_u \nVdash (\exists_{\bf x_n} \pi_n)$ e portanto \eqref{gammaexists:eq1}.
	
	No primeiro caso, em que $\rho \Vdash \exists_{\bf x_n} \pi_n$, é possível provar que\footnote{Lembrete para eu introduzir o $\exists$ decentemente, e nessa altura provar isto.} existe $u \in U_I$ tal que $\rho\!\downarrow^{x_n}_u \Vdash \pi_n$. Então\footnote{E talvez devesse também fazer um anexo ou secção dedicada à substituição para provar coisas destas. Para quem está a ler isto nestas condições: indução. Montes e montes de indução.} é possível mostrar que, como a `variável' $c_n$ não consta em $\pi_n$,
	\[\rho\!\downarrow^{c_n}_u \Vdash \pi_n[c_n],\]
	donde se conclui facilmente \eqref{gammaexists:eq1}.
	\end{proof}
	
	Agora que já demonstrámos que a construção de Henkin preserva coerência, visto que já demonstrámos também o lema de Lindenbaum, podemos começar a construir o modelo que será usado na demonstração do teorema da completude.
	
	Fixe-se $\Gamma \subseteq \F_\Sigma$ coerente. Então, começamos por aplicar a construção de Henkin para obter $\Gamma^\exists \subseteq \F_{\Sigma^+}$, que é também coerente. A este último, podemos aplicar o lema de Lindenbaum para obter $\widehat\Gamma \subseteq \F^{\Sigma^+}$ que satisfaz as seguintes condições:
	
	\begin{itemize}
	\item $\widehat\Gamma$ é completo e coerente
	
	\item Para qualquer fórmula $\pi$ com apenas uma variável $\bf x$ livre, existe um símbolo de constante $c$ tal que $\widehat\Gamma \vdash (\exists_{\bf x} \pi) \imply \pi_{\bf x}[c]$.
	\end{itemize}
	
	É neste $\widehat\Gamma$ que vamos basear a nossa construção.
	
	Seja $U$ o conjunto dos termos fechados em $\Sigma^+$. Note-se que $U$ não é vazio, visto que, por construção, $\Sigma^+$ tem um número infinito contável de termos de constante. Em particular, tem pelo menos um.
	
	Seja $f$ um símbolo de função de aridade $n$. Definimos a função $f_I : U^n \to U$ simbolicamente: se $t_1, \dots, t_n$ são termos fechados então definimos $f_I(t_1, \dots, t_n)$ como o termo fechado $f(t_1, \dots, t_n)$. (Note-se que este último $f$ é meramente um símbolo sintático, enquanto que $f_I$ é uma função `na meta-teoria').
	
	Finalmente, seja $p$ um símbolo de predicado de aridade $n$. Definimos a função $p_I : U^n \to \{\lt, \lf\}$ da seguinte forma: para termos fechados $t_1, \dots, t_n$, a fórmula $\varphi : p(t_1, \dots, t_n)$ é também fechada. Como tal, pela coerência e exaustividade de $\widehat\Gamma$, ou $\widehat\Gamma \vdash \varphi$ ou $\widehat\Gamma \vdash \neg\varphi$, mas não os dois. Como tal, definimos $p_I(t_1, \dots, t_n)$ em cada caso como sendo, respetivamente, $\lt$ ou $\lf$.
	
	Tendo definido o universo e a interpretação dos símbolos de função e de predicado, temos nas nossas mãos uma estrutura de interpretação $I$. Mostramos agora a proposição principal relacionada com esta construção, que mostra essencialmente que $I$ representa fielmente $\widehat\Gamma$. Isto é:
	
	\begin{prop}
	Tenham $\widehat\Gamma$ e $I$ o significado acima descrito. Então, para qualquer fórmula $\varphi \in \F_{\Sigma^+}$ temos que
	\begin{equation}\label{gammahatsemantics}
	\widehat\Gamma \vdash \varphi \text{ sse } I \Vdash \varphi.
	\end{equation}
	\end{prop}
	
	\begin{proof}
	Esta demonstração é feita por indução na estrutura da fórmula $\varphi$. Para mais, assumimos sempre que $\varphi$ é fechada sem perda de generalidade.
	
	Assumimos isto para podermos fazer o passo inicial: de facto, no caso em que $\varphi$ é da forma $p(t_1, \dots, t_n)$ a proposição é verdade por definição de $p_I$, \textbf{apenas se todos os termos são fechados}. Assim sendo, para podermos sequer começar a demonstração, é preciso assumir que $\varphi$ é fechada.
	
	Esta hipótese pode ser feita sem perda de generalidade porque, em ambos os casos, a afirmação é verdade para $\varphi$ sse é verdade para o seu fecho sintático, $\forall \varphi$. Assim sendo, mostramos a seguinte proposição por indução: Se $\varphi$ é uma fórmula fechada então a afirmação \eqref{gammahatsemantics} verifica-se. Seguem-se as verificações dos casos:
	
	\begin{itemize}
	\item Caso base: se $\varphi$ é da forma $p(t_1, \dots, t_n)$, então se ela for fechada todos os termos $t_i$ são fechados. Assim sendo, \eqref{gammahatsemantics} é verdade por definição de $p_I$.
	
	\item Casos $\imply$ e $\neg$: estes casos são triviais. Basta aplicar a hipótese de indução, o facto de que se uma fórmula é fechada as fórmulas componentes também são fechadas e a definição dos operadores lógicos.
	
	\item Caso $\varphi : \forall_{\bf x} \psi$. Este caso é o que requer mais atenção.
	
	O ingrediente principal é que a nossa estrutura de interpretação é pequena o suficiente para poder ser representada fielmente na linguagem. Isto reflete-se nas seguintes afirmações:
\begin{gather}
I \Vdash \forall_{\bf x} \psi \text{ sse para qualquer termo fechado $t$ se tem } I \Vdash \psi[t],\label{eq:smallstructure1}\\
\widehat\Gamma \vdash \forall_{\bf x} \psi \text{ sse para qualquer termo fechado $t$ se tem } \widehat\Gamma \vdash \psi[t].\label{eq:smallstructure2}
\end{gather}

	A demonstração de \eqref{eq:smallstructure1} é feita facilmente usando o facto que
\[\rho_F(\psi) = \rho_F(\psi_{\bf x}[\rho(\bf x)]),\]
	facto este que pode ser demonstrado por indução em $\psi$. Usando-o, temos que $I \Vdash \forall_{\bf x} \psi$ sse $I \Vdash \psi$ sse para qualquer valoração $\rho$ se tiver $\rho_F(\psi) = \lt$, sse para qualquer valoração $\rho$ se tiver $\rho_F(\psi_{\bf x}[\rho(\bf x)]) = \lt$. Como a fórmula $\psi_{\bf x}[\rho(\bf x)]$ é fechada, pelo lema das variáveis omissas, esta última igualdade acontece sse para qualquer valoração $\rho$ a fórmula $\psi_{\bf x}[\rho(\bf x)]$ for verdadeira em $I$. Visto que a única propriedade de $\rho$ que se está a usar é $\rho(\bf x)$, podemos mudar o nome desta quantidade para $t$ e dizer que, para qualquer termo fechado $t$, a fórmula $\psi[t]$ é verdadeira. Isto conclui a demonstração de \eqref{eq:smallstructure1}.

	Pelo outro lado, \eqref{eq:smallstructure2} é consequência elementar das fórmulas adicionadas na construção de Henkin e da veracidade das fórmulas $\forall_{\bf x} \psi \imply \psi[t]$ para $t$ fechado.

	O passo de indução pode agora ser feito sem dificuldade, pois $I \Vdash \forall_{\bf x} \psi$ sse $I \Vdash \psi[t]$ para todo o $t$, sse (por H.I.) $\widehat\Gamma \vdash \psi[t]$ para todo o $t$, sse $\widehat\Gamma \vdash \forall_{\bf x} \psi$.
	\end{itemize}
	\end{proof}

	Como consequência trivial disto temos que $I$ (como estrutura de interpretação sobre $\Sigma$) satisfaz todas as fórmulas em $\Gamma$, o que mostra um teorema muito importante em teoria de modelos:

	\begin{prop} (Teorema de Löwenheim) Se $\Gamma$ é um conjunto coerente de fórmulas existe uma estrutura de interpretação $I$, de universo contável, tal que $I \Vdash \Gamma$.
	\end{prop}

	O facto que o universo de $I$ é contável é irrelevante para os nossos propósitos (e trivial de mostrar porque $U_I \subseteq T_{\Sigma^+}$ que é contável), mas este teorema (e uma generalização, chamada teorema de Löwenheim-Skolem) encontram aplicação em teoria de conjuntos.

	A partir do teorema de Löwenheim o teorema de completude de Gödel fica uma trivialidade.

	\begin{prop} (Teorema da Completude de Gödel)
	Se $\Gamma \vDash \varphi$ então $\Gamma \vdash \varphi$.
	\end{prop}

	\begin{proof}
	Suponha-se que $\Gamma \nvdash \varphi$. Então $\Gamma \nvdash \forall \varphi$, pelo que o conjunto $\Gamma \cup \{\neg \forall \varphi\}$ é coerente e então tem um modelo $I$. Este modelo satisfaz $\Gamma$ mas não $\forall \varphi$, pelo que não satisfaz $\varphi$. Logo, $\Gamma \nvDash \varphi$.
	\end{proof}
	

	\section{Axiomática}

	Nesta secção, mostraremos que os 5 axiomas do cálculo de Hilbert são suficientes para mostrar qualquer afirmação verdadeira.

	\appendix

	\chapter{Substituição}

	Um apêndice para formalizar algumas questões de substituição e demonstrar algumas relações entre substituição e valorações que quebrariam o texto se estivessem nele inseridas.
	
	\nocite{fltc}
	\nocite{shoenfield}
	
	\bibliographystyle{plain}
	\bibliography{bibliography}
\end{document}
