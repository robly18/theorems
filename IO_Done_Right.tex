\documentclass{article}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage[utf8]{inputenc}
\usepackage[portuguese]{babel}
\usepackage{tikz}
\usetikzlibrary{patterns}
\usepackage{listings}
\usepackage{ifxetex}

\ifxetex
%xetex is recommended!
\else
\lstset{
  literate=
  {á}{{\'a}}1
  {à}{{\`a}}1
  {ã}{{\~a}}1
  {é}{{\'e}}1
  {ê}{{\^e}}1
  {í}{{\'i}}1
  {ó}{{\'o}}1
  {õ}{{\~o}}1
  {ú}{{\'u}}1
  {ü}{{\"u}}1
  {ç}{{\c{c}}}1
}
\fi

\addto\captionsportuguese{
	\renewcommand*{\proofname}{Dem}
}

\DeclareMathOperator{\interior}{int}
\DeclareMathOperator{\im}{Im}
\DeclareMathOperator{\CC}{CC}

\title{IO Done Right}
\author{Duarte Maia}
\date{}

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}

\newtheorem{teorema}{Teorema}
\newtheorem{prop}{Prop}
\newtheorem{conjetura}{Conjetura}

\theoremstyle{definition}
\newtheorem{definition}{Definição}
\newtheorem*{definition*}{Definição}
\newtheorem*{notacao}{Notação}

%todo, make def environment bolder, and bolden the just defined terms

\begin{document}
	\maketitle
	
	\tableofcontents
	
	\section{Introdução}
	
	TODO: Escrever palavras motivadoras ou algo assim
	
	\section{Pré-requisitos}
	
	Vou aqui escrevendo coisas à medida que elas são necessárias.
	
	Obviamente é preciso saber um pouco de cálculo I e AL.
	
	É recomendado (i.e. necessário) que o leitor saiba os seguintes conceitos topológicos em $\R^n$:
	
	Conceito de conjunto aberto e conjunto fechado; interior, fronteira e extrior de um conjunto; função contínua; teorema de Bolzano e Weierstrass em $\R^n$, definição topológica de continuidade ($f : D \rightarrow \R^m$ é contínua sse para todo conjunto aberto $A$ se tem $f^{-1}(A)$ é aberto relativamente a $D$ sse para todo conjunto fechado $A$ se tem $f^{-1}(A)$ é fechado relativamente a $D$)
	
	No que se segue vão ser usados alguns factos sobre estes conceitos que não vão ser justificados, visto que caem no âmbito da cadeira de Cálculo II. Estes serão devidamente assinalados, e o leitor é encorajado a prová-los se não estiver familiar com eles.
	
	\section{Problemas de Otimização Linear}
	
	\subsection{Introdução}
	
	Um problema de otimização é um tipo específico de problema em que o objetivo é minimizar ou maximizar uma certa função (função-objetivo), dentro de um certo domínio, usualmente (no contexto desta cadeira) um subconjunto de $\R^n$ parametrizado por um conjunto de (in)equações.
	
	Mais concretamente, o tipo de problemas com que se lida nesta cadeira são problemas de otimização linear. Estes são problemas em que a função-objetivo é uma função linear $\R^n \rightarrow \R$ e todas as condições no domínio são da forma $ax \leq b$, $ax = b$ ou $ax \geq b$, onde $a$ é um vetor-linha e $b$ é um escalar.
	
	\begin{definition}
	Um \emph{problema de otimização linear} (normalmente abreviado a \emph{pol}) é um problema da forma:
	
	\smallskip
	
	\textbf{Objetivo: } maximizar/minimizar (em função de $x \in \R^n$) o valor de ${c_1 x_1 + c_2 x_2 + \ldots + c_n x_n}$
	
	\smallskip
	
	\textbf{Restrições: } $x$ tem de obedecer a todas as seguintes igualdades

\[
\begin{cases}
	a_{11} x_1 + a_{12} x_2 + \ldots + a_{1n} x_n = b_1 \\ 
	a_{21} x_1 + a_{22} x_2 + \ldots + a_{2n} x_n = b_2 \\
	\vdots \\
	a_{p1} x_1 + a_{p2} x_2 + \ldots + a_{pn} x_n = b_p
\end{cases}
\]

E todas as seguintes desigualdades

\[
\begin{cases}
	a'_{11} x_1 + a'_{12} x_2 + \ldots + a'_{1n} x_n \leq b'_1 \\ 
	\vdots \\
	a'_{q1} x_1 + a'_{q2} x_2 + \ldots + a'_{qn} x_n \leq b'_q
\end{cases}
\]
\[
\begin{cases}
	a''_{11} x_1 + a''_{12} x_2 + \ldots + a''_{1n} x_n \geq b''_1 \\ 
	\vdots \\
	a''_{r1} x_1 + a''_{r2} x_2 + \ldots + a''_{rn} x_n \geq b''_r
\end{cases}
\]
	\end{definition}
	
	Infelizmente, isto é algo trabalhoso de escrever. Como tal, normalmente estas condições são escritas de forma mais compacta usando a linguagem da álgebra linear.
	
	\begin{notacao}
Dados dois vetores $x,y \in \R^n$, dizemos $x \leq y$ se $x_i \leq y_i$ para todo $i$.
	\end{notacao}
	\begin{notacao}
	Considere-se o pol escrito acima. Este é normalmente escrito da seguinte forma:
	
	(Supõe tratar-se de um problema de maximização; caso contrário escreva-se $\min$ no lugar de $\max$)
	
	Seja $c$ o vetor linha $[\,c_1\,c_2\, \cdots \,c_n\,]$,

	$A$ a matriz $p \times n$ cujo $i,j$-ésimo elemento é $a_{ij}$, e considerações análogas para $A'$ a $A''$, com $a'_{ij}$ e $a''_{ij}$ respetivamente
	
	$b$ o vetor coluna $(b_1, b_2, \cdots, b_n)$ e análogamente para $b'$ e $b''$.
	
	\[
	\begin{cases}
	\max\limits_x cx \\
	Ax = b \\
	A'x \leq b' \\
	A''x \geq b''
	\end{cases}
	\]
	\end{notacao}
	
	Normalmente, não é útil considerar um tipo de problema tão geral, pelo que discutímos em baixo formas de passar de um tipo de problemas para outros.
	
	\subsection{Tradução}
	
	Considere-se os seguintes problemas de otimização linear:
	
	\[
	\begin{cases}
	\max\limits_x 2x_1 - 3x_2 \\
	5x_1 + x_2 = 6 \\
	2x_1 + 0x_2 \leq 2
	\end{cases}
	\begin{cases}
	\min\limits_x -2x_1 + 3x_2 \\
	5x_1 + x_2 \leq 6 \\
	5x_1 + x_2 \geq 6 \\
	-2x_1 + 0x_2 \geq -2
	\end{cases}
	\]
	
	Apesar de terem uma aparência diferente, alguma inspeção leva à conclusão que estes são, na realidade, exatamente o mesmo problema. Ou seja, o mesmo problema pode ser representado de mais do que uma forma diferente. Isto leva à possibilidade de considerar `formas canónicas' de expressar os problemas, que sejam mais fáceis de estudar. Se, por exemplo, dissermos que todos os problemas da forma XYZ podem ser resolvidos fazendo ABC, e arranjarmos forma de traduzir qualquer pol para esta forma XYZ, estamos numa boa situação.
	
	\begin{notacao}
	Dado um pol $P$, o conjunto de $x \in \R^n$ que satisfazem as suas condições é denominado de \emph{conjunto admissível de $P$}, representado por $X_P$, ou só $X$ se o pol em questão for óbvio de contexto.
	
	Da mesma forma, o \emph{conjunto solução de $P$}, $S_P$, ou $S$ se $P$ é claro de contexto, é o conjunto de $x \in X$ que maximizam, de facto, a função objetivo. Por outras palavras,
	
	\[S_P = \{\,x \in X_p \mid \forall_{y \in X_p} cx \geq cy\,\}\]
	
	Isto no caso de maximização. No caso de minimização, a desigualdade deve estar trocada.
	\end{notacao}
	
	Há várias formas possíveis de transformar um problema noutro.
	
	\subsubsection{Substituição de equivalências}
	
	Por exemplo, a condição de igualdade $a = b$ pode ser expressa como duas desigualdades: $a \leq b$ e $a \geq b$. Assim sendo, sabemos à partida que qualquer pol que nos venha à cabeça pode ser expresso só com condições de desigualdade.
	
	Para mais, a desigualdade $a \geq b$ é equivalente a $-a \leq -b$. Logo, podemos sempre assumir que as condições que restringem um pol são sempre da forma $Ax \leq b$.
	
	Podemos também assumir sem perda de generalidade que um problema de otimização linear é um problema de maximização, pois minimizar $cx$ é o mesmo que maximizar $-cx$.
	
	Logo, dado um pol qualquer, podemos sempre assumir, sem perda de generalidade, que é da forma
	
	\[
	\begin{cases}
	\max\limits_x cx\\
	Ax \leq b
	\end{cases}
	\]
	
	A esta forma vamos-nos referir, no futuro, como `forma semicanónica', na falta de melhor nome. Sugestões são aceites.
	
	O que foi até agora descrito é o tipo mais simples possível de tradução de um problema a outro: trocar as condições por condições equivalentes, e substituir `maximizar $f$' por `minimizar $-f$'.
	
	Este tipo de tradução tem a propriedade que não muda o conjunto admissível e conjunto solução, mas há traduções que não são tão triviais.
	
	\subsubsection{Adição de positividades (Forma canónica)}
	
	É, em muitos casos, útil estudar um problema de otimização em que sabemos que todas as variáveis são não-negativas. Assim sendo, vamos agora ver que podemos sempre, sem perda de generalidade, assumir que o nosso pol é da forma
	
	\[
	\begin{cases}
	\max\limits_x cx\\
	Ax \leq b\\
	x \geq 0
	\end{cases}
	\]
	
	O truque é modificar o conjunto em que estamos.
	
	Dado um número real $x$, define-se a sua parte positiva, $x^+$, como
	
	\[
	x^+ =
	\begin{cases}
	x & \text{se } x \geq 0 \\
	0 & \text{caso contrário}
	\end{cases}
	\]
	
	e a parte negativa, $x^-$, como
	
	\[
	x^- =
	\begin{cases}
	0 & \text{se } x \geq 0 \\
	-x & \text{caso contrário}
	\end{cases}
	\]
	
	Dado um vetor $x \in \R^n$, define-se $x^+$ como o vetor das partes positivas de $x$, e $x^-$ como o vetor das partes negativas. É claro que $x = x^+ - x^-$, e $x^+, x^- \geq 0$. Isto permite-nos escrever qualquer vetor como a diferença de dois vetores não-negativos.
	
	Assim sendo, considere-se a condição $Ax \leq b$. Esta condição é equivalente a $Ax^+ - Ax^- \leq b$, com $x^+, x^- \geq 0$. Para mais, $cx = cx^+ - cx^-$, pelo que isto sugere a consideração de transformar o pol
	
	\[
	P =
	\begin{cases}
	\max\limits_x cx\\
	Ax \leq b
	\end{cases}
	\]
	
	em
	
	\[
	Q =
	\begin{cases}
	\max\limits_{(x_+, x_-)} cx_+ - cx_-\\
	Ax_+ - Ax_- \leq b\\
	x_+, x_- \geq 0
	\end{cases}
	\]
	
	(Aqui, $(x_+,x_-)$ representa apenas um vetor, de dimensão igual ao dobro da de $x$. O $+$ e $-$ em subscrito é suposto ser sugestivo de que um deles é a parte positiva e o outro a parte negativa de $x$.)
	
	Repare-se que este segundo pol não tem a mesma dimensão que o primeiro: se no primeiro, o vetor $x$ pertence a $\R^n$, no segundo, o vetor $(x_+, x_-)$ pertence a $\R^{2n}$. Portanto, não é imediatamente óbvio que estes são `o mesmo' pol, pelo que vamos fazer a seguinte definição:
	
	\begin{definition}
	Dados dois problemas de otimização linear, $P$ e $Q$, dizemos que \emph{$Q$ é uma $f$-tradução de $P$} se se for possível resolver $P$ sabendo a resposta de $Q$, usando $f$.
	
	Em termos mais precisos, se $f$ é uma função sobrejetiva $X_Q \rightarrow X_P$ tal que $q \in X_Q$ tem uma pontuação melhor que $q' \in X_Q$ (menor se $Q$ é de minimzação, maior se maximização) sse $f(q)$ tem melhor pontuação que $f(q')$.
	\end{definition}
	
	Esta definição tem interesse devido à seguinte proposição:
	
	\begin{prop}
	Se $Q$ é uma $f$-tradução de $P$, $S_Q = f(S_P)$.
	\end{prop}
	\begin{proof}
	Basta reparar que, por definição, $x \in S_Q$ sse $x$ tem pontuação melhor ou igual que $y$ para todo $y \in X_Q$, sse $f(x)$ tem pontuação melhor que $f(y)$ para todo $y \in X_Q$. Queremos então mostrar que isto é sse $f(x) \in S_P$.
	
	Como $f$ é sobrejetivo, isto implica que $f(x)$ tem pontuação melhor ou igual que todo $z$ em $X_P$, o que por definição implica $f(x) \in S_P$. Pelo outro lado, se houver $y \in X_Q$ tal que $f(y)$ tem pontuação melhor que $f(x)$, então $f(x)$ claramente não pode ser solução, pois $f(y)$ é admissível de pontuação melhor que ele.
	\end{proof}
	
	Isto mostra que se resolvermos uma tradução de um pol, é fácil recuperar a solução e conjunto admissível do problema original.
	
	O caso anterior, em que $X_P = X_Q$ e $S_P = S_Q$ é o caso trivial em que $f$ é a identidade. Neste caso, em que pretendemos adicionar a condição de positividade, é ligeiramente mais complicado.
	
	Vamos mostrar que, para o $P$ e $Q$ definidos em cima, $Q$ é uma $f$-tradução de $P$, em que $f$ é a função $f(x_+, x_-) = x_+ - x_-$.
	
	\begin{proof}
	Primeiro, mostre-se que se um vetor pertence a $X_Q$, a sua imagem pertence a $X_P$.
	
	Repare-se que, se $(x_+, x_-) \in X_Q$, então a sua imagem é $f(x_+, x_-) = x_+ - x_-$. Por definição, estes satisfazem $A(x_+ - x_-) \leq b$, donde $A f(x) \leq b$, que é a condição para $f(x) \in X_P$.
	
	Para mostrar sobrejetividade, repare-se que se $x \in X_P$, tem-se $(x^+, x^-) \in X_Q$, e $f(x^+, x^-) = x^+ - x^- = x$.
	
	Agora, queremos mostrar que $(x_+, x_-)$ tem pontuação melhor que $(y_+, y_-)$ sse $f(x_+, x_-)$ tem pontuação melhor que $f(y_+, y_-)$. Mas isto é trivial, pois a pontuação de $f(x_+, x_-)$ é a mesma que a de $(x_+, x_-)$, e o objetivo é, tanto em $P$ como em $Q$, maximizar.
	\end{proof}
	
	Assim sendo, podemos sempre assumir, sem perda de generalidade, que qualquer problema que tenhamos está na chamada \emph{forma canónica}, isto é, é da forma
	
	\[
	\begin{cases}
	\max\limits_x cx\\
	Ax \leq b\\
	x \geq 0
	\end{cases}
	\]
	
	\subsubsection{Desigualdades para igualdades (Forma padrão)}
	
	Outra forma útil de expressar um problema de otimização linear é a chamada forma padrão, da forma seguinte:
	
	\[
	\begin{cases}
	\min\limits_x cx\\
	Ax = b\\
	x \geq 0
	\end{cases}
	\]
	
	Já sabemos que podemos supor que os nossos problemas estão na forma canónica, sem perda de generalidade. Portanto, considere-se o pol
	
	
	\[
	P =
	\begin{cases}
	\max\limits_x cx\\
	Ax \leq b\\
	x \geq 0
	\end{cases}
	\]
	
	E vamos reescrevê-lo na forma padrão.
	
	Para fazer a tradução, é preciso representar desigualdades com igualdades. Para isso, repare-se que $a \leq b$ é a mesma coisa que dizer que $a + y = b$, para algum $y$ não-negativo. Assim, adicionamos as chamadas \emph{variáveis de folga}: Considere-se o pol
	
	\[
	Q =
	\begin{cases}
	\min\limits_{(x,y)} -cx\\
	Ax + y = b\\
	x, y \geq 0
	\end{cases}
	\]
	
	Vamos mostrar que $Q$ é uma $f$-tradução de $P$, com $f(x,y) = x$.
	
	\begin{proof}
	Primeiro, verifique-se que se $(x,y) \in X_Q$ se tem $f(x,y) \in X_P$. Repare-se que a condição $f(x,y) \geq 0$ é trivial, pois $x \geq 0$ por hipótese, e a condição $A f(x,y) \leq 0$ é verídica pois $A f(x,y) = Ax \leq Ax + y = b$.
	
	De seguida, é preciso verificar sobrejetividade. Suponhamos que se quer um objeto cuja imagem sob $f$ seja $x \in X_P$. Considere-se, então, o vetor $(x,y)$, com $y = b - Ax$. Repare-se que este vetor pertence a $X_Q$, e a sua imagem é precisamente $x$, como queríamos demonstrar.
	
	Finalmente, provemos que $(x,y)$ tem melhor pontuação que $(x',y')$ em $Q$ sse $x$ tem melhor pontuação que $x'$ em P.
	
	Mas isto é trivial, pois $(x,y)$ tem melhor pontuação que $(x',y')$ sse $-cx < -cx'$ sse $cx > cx'$, como queríamos demonstrar.
	\end{proof}
	
	Isto mostra, então, que qualquer pol tem uma tradução para um pol na forma padrão.
	
	\subsubsection{Padrão para Canónico}
	
	Viu-se agora uma forma para traduzir problemas da forma canónica para a forma padrão. É possível, também, passar da forma padrão para a canónica (provou-se, aliás, que é possível passar de qualquer pol para a forma canónica) usando os procedimentos usados antes.
	
	No entanto, é muito mais económico fazer o processo simples de transformar todas as igualdades em desigualdades do tipo $\leq$. Ou seja, passar de:
	
	\[
	\begin{cases}
	\min\limits_x cx\\
	Ax = b\\
	x \geq 0
	\end{cases}
	\]
	
	Para
	
	\[
	\begin{cases}
	\max\limits_x -cx\\
	Ax \leq b\\
	-Ax \leq -b\\
	x \geq 0
	\end{cases}
	\]
	
	Já vimos antes que estas traduções são válidas.
	
	\section{Noções Topológicas}
	
	\subsection{Intuição geométrica}
	
	Esta secção é dedicada a dar intuição geométrica para motivar os teoremas que se seguem. Vamos, a título de exemplo, partir de um pol específico a duas dimensões na forma canónica.
	
	Considere-se o pol
	
	\[
	P =
	\begin{cases}
	\max\limits_{(x,y)} 2x + y\\
	x + 2y \leq 8\\
	x - y \leq 2 \\
	x, y \geq 0
	\end{cases}
	\]
	
	Vamos representar, no plano cartesiano, a região admissível $X_P$ e o vetor $c^T = (2,1)$.
	
	\begin{tikzpicture}
	\draw[->] (-2, 0) -- (6, 0) node[anchor=south] {x};
	\draw[->] (0, -1) -- (0, 5) node[anchor=west] {y};
	\filldraw[color=red, fill=blue!80, very thick] (0,0) node[anchor=north west, color=black]{0} -- (0,4) node[anchor=east, color=black]{(0,4)} -- (4,2) node[anchor=north west, color=black]{\,(4,2)} -- (2,0) node[anchor=north, color=black]{(2,0)} -- (0,0);
	
	\draw[->, color=green, very thick] (4,2) -- (6,3) node[anchor=west, color=black]{$c^T = (2,1)$};
	\draw[very thick, color=green] (3.5,3) -- (4.5,1);
	\end{tikzpicture}
	
	Uma interpretação para o conjunto solução $S_P$ é o conjunto de vetores admissíveis que maximiza o produto escalar com $c^T$. Intuitivamente, os vetores que `estão o mais possível na direção de $c^T$'.
	
	É possível ver (isto em termos intuitivos, claro) que não há nenhum vetor admissível `para lá' da linha verde desenhada, o que indica que o conjunto solução será o conjunto $\{(4,2)\}$.
	
	Neste caso, o conjunto solução tem apenas um elemento, mas é concebível uma situação em que tivesse mais. Por exemplo, se o vetor $c^T$ estivesse perpendicular ao lado $(0,4)-(4,2)$.
	
	Isto é só especulação com base num desenho, mas dita especulação pode ser útil para motivar conclusões.
	
	Por exemplo, com base nestas considerações, seria de esperar que, por exemplo, em geral o conjunto solução estará contido na fronteira do conjunto admissível, o que quer que isso signifique (a área marcada a vermelho na imagem). Também é fácil uma pessoa convencer-se que tem necessariamente de conter pelo menos um dos vértices -- novamente, o que quer que isso signifique.
	
	Seria também de esperar que no caso de o conjunto admissível ser limitado e não vazio, existe, necessariamente, solução. E, já agora, que o conjunto admissível é convexo.
	
	Todos estes factos aleatórios serão provados, e mais, e alguns deles provar-se-ão úteis na procura de formas de resolver problemas de otimização.
	
	\subsection{Factos introdutórios}
	
	Recorde-se o leitor da definição de interior, fronteira e exterior de conjunto, e da definição de conjunto aberto e fechado.
	
	\begin{prop}
		Fixo um pol $P$, $X_P$ e $S_P$ são fechados.
	\end{prop}
	
	\begin{proof}
	Primeiramente, mostre-se que $X_P$ é fechado.
	
	Recorde-se que interseções finitas de fechados são fechadas. De seguida, note-se que, dado um pol $P$ (que podemos supor sem perda de generalidade estar na forma semicanónica. Assim, o conjunto admissível é o conjunto de $x \in \R^n$ que satisfazem as condições
	
	\[a_i x \leq b_i \text{ para $i = 1, 2, \cdots, m$}\]
	
	Onde os $a_i$ são vetores-linha, e os $b_i$ são escalares.
	
	Ou seja, $X_P$ é a interseção
	
	\[\bigcap_{i=1}^m \{\,x \in \R^n \mid a_i x \leq b_i \,\}\]
	
	Repare-se agora que $\{\,x \in \R^n \mid a_i x \leq b_i \,\}$ é a pré-imagem sob uma função linear (e então contínua) de um conjunto fechado ($]-\infty, b_i]$), pelo que usando Cálculo II se conclui que estes conjuntos são todos fechados, pelo que $X_P$ é fechado.
	
	Para mostrar que $S_P$ é fechado, repare-se que há dois casos: ou $S_P$ é vazio, e então fechado; ou contém pelo menos um elemento $s$. Neste último caso, $S_P = X_P \cap \{\,x \mid c^T x \geq c^T s\,\}$ por definição, e isto é a interseção de dois conjuntos fechados, e então fechado.
	\end{proof}
	
	Recorde-se do teorema de Weierstrass, que deverá ter dado em Cálculo II: uma função contínua definida num subconjunto fechado, limitado e não-vazio de $\R^n$ (compacto não-vazio) tem máximo e mínimo. Assim sendo, conseguimos facilmente o seguinte corolário:
	
	\begin{prop}
	Fixo um problema de otimização linear $P$, se $X_P$ é limitado e não-vazio, $S_P$ é não-vazio.
	\end{prop}
	\begin{proof}
	Basta reparar que se $X_P$ é limitado, por ser fechado (ver acima), é compacto. Assim sendo, como a função objetivo é linear e então contínua, tem valor máximo algures em $X_P$. Os valores maximizantes formam o conjunto solução.
	\end{proof}
	
	Mais um detalhe topológico simples, antes de avançarmos para coisas mais específicas: denomine-se o interior de um conjunto $X$ por $\interior X$, e a sua fronteira por $\partial X$.
	
	\begin{prop}
	Se $P$ é um pol de função objetivo não-nula, $S_P \subseteq \partial X_P$.
	\end{prop}
	
	\begin{proof}
	Basta reparar que, se $x \in \interior  X_P$, então há uma vizinhança $\varepsilon$ de $x$ contida em $X_P$. A ideia é que, então, se $c^T$ não é o vetor nulo, podemos `avançar um bocadinho na direção de $c^T$'. Em termos mais precisos, o vetor $y = x + \frac \varepsilon 2 \frac{c^T}{\lvert c^T \rvert}$ pertence a $X_P$, por distar de $x$ menos de $\varepsilon$. Mas este vetor é `melhor' que $x$, pois $cy = cx + c \frac \varepsilon 2 \frac {c^T}{\lvert c^T \rvert} = cx + \frac{\varepsilon |c^T|} 2 > cx$. Logo, nenhum $x \in \interior X_P$ pertence a $S_P$, donde $S_P$ tem que estar todo contido na fronteira.
	\end{proof}
	
	Só um último detalhe, antes de avançarmos para outras pastagens: uma caraterização do interior de $X_P$ para um pol $P$, que se assume spdg estar na forma semicanónica.
	
	\begin{prop}
	Seja $P$ o pol
	
	\[
	P =
	\begin{cases}
	\max\limits_x cx\\
	Ax \leq b
	\end{cases}
	\]
	
	e assuma-se que nenhuma linha da matriz $A$ é composta somente de zeros. (Isto é denotado por \emph{$A$ é uma matriz própria})
	
	Então, $\interior X_P$ é o conjunto de $x > 0$ tal que $Ax < b$. Consequentemente, $\partial X_P$ é o conjunto de $x \geq 0$ tal que $Ax \leq b$ e há pelo menos uma igualdade.
	\end{prop}
	
	\begin{proof}
	Relembre-se da seguinte identidade: $\interior (A \cap B) = \interior A \cap \interior B$. Mais geralmente, o interior de uma interseção finita é a interceção dos interiores.
	
	Como tal, sendo que
	
	\[X_P = \bigcap_{i=1}^m \{\,x \in \R^n \mid a_i x \leq b_i \,\}\]
	
	onde $a_i$ representa a $i$-ésima linha da matriz $A$,
	
	Para determinar $\interior X_P$ basta determinar os interiores destes conjuntos. Afirmamos que o interior do $i$-ésimo conjunto é
	
	\[\interior \{\,x \in \R^n \mid a_i x \leq b_i \,\} = \{\,x \in \R^n \mid a_i x < b_i \,\}\]
	
	Para justificar a inclusão $\supseteq$, note-se que o lado direito é um conjunto aberto contido em $\{\,x \in \R^n \mid a_i x \leq b_i \,\}$, e então contido no seu interior.
	
	Para justificar a inclusão $\subseteq$, suponha-se que $x$ pertence ao lado esquerdo. Mostrar-se-á que pertence ao lado direito.
	
	Por pertencer ao interior daquele conjunto, sabemos que, para $y$ numa vizinhança $\varepsilon$ de $x$, $a_i y \leq b_i$. Considere-se, então, $y = x + \frac \varepsilon 2 \frac{a_i^T}{\lvert a_i^T \rvert}$. Por hipótese, $a_i y \leq b_i$. Mas tem-se também $a_i y = a_i x + a_i \frac \varepsilon 2 \frac{a_i^T}{\lvert a_i^T \rvert} = a_i x + \frac {\varepsilon \lvert a_i^T \rvert} 2 > a_i x$, donde se conclui $a_i x < a_i y \leq b_i$. Logo, $x$ pertence ao lado direito.
	\end{proof}
	
	\section{Álgebra Linear}
	
	\subsection{Intuição geométrica}
	
	Queremos agora investigar a intuição que o conjunto solução contém sempre um vértice, no caso de este ser não-vazio.
	
	Para este estudo, no entanto, é útil considerar, em vez do pol na forma canónica, o pol na forma padrão, visto que é mais fácil caraterizar o significado de `vértice' neste contexto.
	
	Considere-se o seguinte pol:
	
	\[
	P =
	\begin{cases}
	\min\limits_{(x,y,z)} 2x + y + 5z\\
	3x + 6y + 4z = 24\\
	x, y, z \geq 0
	\end{cases}
	\]
	
	O desenho do conjunto admissível é o seguinte:
	
	\resizebox{\columnwidth}{!}{
	\begin{tikzpicture}
	\draw[->] (0, 0) -- (10, 0) node[anchor=south] {x};
	\draw[->] (0, 0) -- (-5, -5) node[anchor=north west] {y};
	\draw[->] (0, 0) -- (0, 7) node[anchor=west] {z};
	\filldraw[color=red, pattern color=blue, very thick, pattern = north east lines] (8,0) node[anchor=north, color=black]{(8,0,0)} -- (-4,-4) node[anchor=east, color=black]{(0,4,0)} -- (0,6) node[anchor=west, color=black]{(0,0,6)} -- (8,0);
	\end{tikzpicture}
	}
	
	Temos agora de definir o conceito de vértices no contexto de otimização linear, mas a figura deve tornar evidente que se tratam dos pontos com o maior número de coordenadas 0.
	
	Vamos primeiro tentar definir o que queremos dizer por `vértice'.
	
	\subsection{Geometria}
	
	Dado o nosso contexto, é possível atribuir significado aos termos `vértice', `aresta', `lado' e assim por diante. No entanto, por simplicidade, vamos apenas definir o significado de vértice.
	
	Relembre-se da definição de plano em Álgebra Linear. Um plano em $\R^n$ é um conjunto da forma $U = a + V$, em que $a \in \R^n$ e $V$ é um subespaço vetorial de $\R^n$. Dizemos que $U$ é um plano de dimensão $m$ se $V$ tiver dimensão $m$.
	
	Um \emph{hiperplano em $\R^n$} é um plano de dimensão $n-1$.
	
	Relembre-se que os planos podem também ser caraterizados como conjuntos-solução de equações lineares $Ax = b$ a $n$ dimensões. A dimensão do plano é, então, a dimensão do núcleo da matriz $A$. Relembre-se também que, se um plano $a + V$ é definido por $Ax = b$, as colunas de $A^T$ geram o espaço $V^\perp$.
	
	Uma particularidade dos hiperplanos é que o seu espaço ortogonal tem dimensão 1. Assim sendo, um hiperplano pode sempre ser expresso da forma $w \cdot x = b$, para algum vetor não-nulo $w$ e escalar $b$. Para mais, quaisquer duas expressões que representem o mesmo hiperplano têm vetores colineares.
	
	Isso permite fazer a seguinte definição:
	
	\begin{definition}
	Dado um plano $H$ definido por $w \cdot x = b$, dizemos que este separa o espaço em dois semiespaços: $H^+$, definido por $w \cdot x > b$ e $H^-$, definido por $w \cdot x < b$.
	
	Devido à observação anterior, esta definição não depende do $w$ escolhido, a menos de sinal. Ou seja, $H^+$ e $H^-$ não dependem de $w$, exceto para distinguir qual é qual.
	
	Diz-se que $H$ \emph{separa o espaço em $H^+$ e $H^-$}.
	\end{definition}
	
	Estamos agora prontos para caraterizar o que queremos dizer com `vértice'.
	
	\begin{definition}
	Dado um conjunto $S \subseteq \R^n$, dizemos que $v \in S$ é um \emph{vértice exterior de $S$} se existem $w \in \R^n$ e $b \in \R$ tal que $w \cdot v = b$ e $w \cdot x < b$ para todo $x \in S \setminus \{v\}$.
	\end{definition}
	
	Esta definição é equivalente à seguinte:
	
	\begin{definition*}
	Nas mesmas condições, dizemos que $v \in S$ é vértice exterior se existe um hiperplano que separa $v$ do resto do conjunto. Ou seja, se existe um hiperplano $H$ que separe o espaço em $H^+$ e $H^-$ tal que $v \in H$ e $S \setminus \{v\} \subseteq H^-$.
	\end{definition*}
	
	Repare-se que é óbvio que esta segunda definição implica a primeira, devido à forma como se definiram os subespaços. No entanto, a outra implicação não é tão óbvia, pois repare-se que não é exigido do vetor $w$ que seja diferente de zero. Este detalhe tem pequena relevância, mas permitir-nos-á simplificar uma prova algures no futuro. Claro que se o vetor $w$ for $0$, o conjunto $S$ contém apenas o ponto $v$, e portanto a segunda definição aplica-se com qualquer hiperplano que passe por $v$. Isto mostra que as duas definições são equivalentes.
	
	Para exemplificar esta definição, considere-se o conjunto:
	
	\[S = \{\, (x,y) \mid x,y \geq 0, y \leq 2+x, y \geq -2 + 2x\,\}\]
	
	Em baixo, está uma representação da afirmação: ``o ponto $(4, 6)$ é vértice de $S$, pois pondo $w = (1,1)$ e $b = 10$, tem-se $w \cdot (4,6) = b$ e para todos os pontos $(x,y)$ em $S \setminus \{(4,6)\}$ tem-se $w \cdot (x,y) < b$''.
	
	\begin{tikzpicture}
	\draw[->] (-2, 0) -- (6, 0) node[anchor=south] {x};
	\draw[->] (0, -1) -- (0, 8) node[anchor=west] {y};
	\filldraw[color=blue, fill=blue!80, very thick] (0,0) -- (0,2) -- (4,6) -- (1,0) -- (0,0);
	
	\draw[->, color=purple, very thick] (4,6) -- (5,7) node[anchor=west, color=black]{$w = (1,1)$};
	
	\draw[color=green, very thick] (2.5, 7.5) -- (5.5, 4.5);
	\fill[pattern=north east lines, pattern color=green] (-1.5, -0.5) -- (-1.5, 7.5) -- (2.5, 7.5) -- (5.5, 4.5) -- (5.5, -0.5);
	\end{tikzpicture}
	
	A reta verde representa o hiperplano (reta) $H$ definido por ${w \cdot (x,y) = 10}$, e a área pintada a verde representa o semiespaço $H^-$ definido por ${w \cdot (x,y) < 10}$. Repare-se que todo o conjunto $S$, exceto o ponto $(4,6)$, está contido neste semiespaço.
	
	\subsection{Vértices de conjuntos admissíveis}
	
	Estamos agora prontos para tentar caraterizar os vértices de conjuntos admissíveis do pol na forma padrão. A seguinte proposição ser-nos-á útil:
	
	\begin{prop} \label{vertexvpmh}
	Seja $S$ um conjunto em $\R^n$. As seguintes quatro afirmações não podem ser todas verdade ao mesmo tempo:
	
	\begin{itemize}
	\item $v$ é vértice de $S$
	\item $v + h \in S$
	\item $v - h \in S$
	\item $h \neq 0$
	\end{itemize}
	\end{prop}
	
	\begin{proof}
	Para mostrar isto, vamos supor que três destes são verdade, e mostramos que o quarto tem que ser falso. Nomeadamente, suponhamos que $v$ é vértice de $S$, $v+h \in S$ e $h \neq 0$.
	
	Assim sendo, visto que $v$ é vértice, existem $w$ e $b$ tal que $w \cdot v = b$ e $w \cdot x < b$ para todo $x \in S$ diferente de $v$. Em particular, $w \cdot (v + h) < b$. Mas então $w \cdot h < 0$, donde $w \cdot (v-h) > b$, e então concluímos que $v-h$ não pode pertencer a $S$.
	\end{proof}
	
	Na caraterização de vértices é-nos útil falar das coordenadas positivas de vetores, e das colunas correspondentes da matriz $A$. (A razão para isto será evidente em breve.) Assim sendo, adotamos as seguintes notações:
	
	\begin{notacao}
	No nosso contexto, considere-se o seguinte pol a $n$ dimensões:
	
	\[
	\begin{cases}
	\min\limits_x cx\\
	Ax = b\\
	x \geq 0
	\end{cases}
	\]
	
	Em que $x$ é um vetor em $\R^n$, $A$ é uma matriz $m \times n$ e $b$ é um vetor em $\R^m$. Por conveniência, seja $N$ o conjunto $\{1,2,\cdots,n\}$.
	
	Fixo um vetor $x$:
	
	Usamos $P_x$ para denotar o conjunto de índices $i$ tal que $x_i > 0$.
	
	Dado um conjunto de índices $B \subseteq N$, $x_B$ representa o vetor em $\R^{\#B}$ obtido de $x$ resultante apenas das coordenadas em $B$. Por exemplo, se $B$ é o conjunto $\{2, 5, 9\}$, $x_B$ seria o vetor em $\R^3$ dado por $(x_2, x_5, x_9)$. Análogamente, $A_B$ denota a submatriz de $A$ considerando apenas as colunas com índices em $B$.
	\end{notacao}
	
	Antes de fazer a prova `a sério', será feito um esboço de prova para suportar a afirmação feita há bocado: ``os vértices tratam-se dos pontos com o maior número de coordenadas 0''. Este esboço de prova vai-nos servir para motivar a caraterização (completa) dos vértices de $X_P$, para $P$ na forma padrão.
	
	Seja $k$ a caraterística da matriz $A$. Recorde-se que a caraterística de uma matriz equivale à dimensão do seu espaço de colunas. Afirmamos que qualquer vértice de $X_P$ tem no máximo $k$ coordenadas maiores que zero.
	
	Para justificar isto, considere-se um $x \in X_P$ tal que $\# P_x > k$. Queremos justificar que este não pode ser um vértice, e vamos fazer isso usando a proposição \ref{vertexvpmh}.
	
	Nomeadamente, vamos arranjar um $h$ diferente de zero tal que $x+h$ e $x-h$ pertencem a $X_P$, o que, devido a esta proposição, implica que $x$ não é vértice de $X_P$. Para arranjar este $h$, repare-se nas condições que ele tem que obedecer:
	
	\begin{itemize}
	\item $h \neq 0$
	\item $Ah = 0$
	\item $x \pm h \geq 0$
	\end{itemize}
	
	Em particular, a última condição diz-nos algo importante: para todo $i$, se $x_i = 0$, $h_i$ tem que ser $0$. Caso contrário, um daqueles dois vetores teria a $i$-ésima casa negativa.
	
	Assim sendo, isso limita as nossas escolhas de $h$. Este tem necessariamente as casas em $N \setminus P_x$ igual a zero, pelo que resta definir $h_{P_x}$.
	
	Repare-se, agora, que, sob estas condições, $Ah = A_{P_x} h_{P_x}$, e visto que $\# P_x$ é maior do que a caraterística de $A$, a matriz $A_{P_x}$ é singular, e então existe um $h$ diferente de zero (chamemos-lhe $\tilde h$) que satisfaz $A \tilde h = 0$ e $\tilde h_i$ é igual a zero para $i \not \in P_x$. Falta apenas assegurar que $x \pm \tilde h \geq 0$, mas isto pode não ser verdade.
	
	Para o passo final, repare-se que estas duas condições ($A \tilde h = 0, h \neq 0$) não deixam de ser verdade se multiplicarmos $\tilde h$ por um escalar $t$ diferente de zero.
	
	A ideia é a seguinte: para todo $i \in N$ tem-se um dos dois casos: ou $x_i = 0$ e então, como $\tilde h_i = 0$, $x_i + t \tilde h_i \geq 0$ independentemente de $t$, ou $x_i > 0$ e então $x_i + t \tilde h_i \geq 0$ para qualquer $t$ de módulo pequeno o suficiente. Considere-se o $m$ mínimo destes módulos, e tem-se que, para $t$ de módulo menor que $m$, $x \pm t \tilde h_i \geq 0$. Pondo $h = t \tilde h_i$, obtemos o resultado desejado: $x \pm h \in X_P$ e $h \neq 0$. Assim sendo, $x$ não pode ser vértice de $X_P$.
	
	Repare-se que neste esboço de prova, a condução que $\#P_x > k$ foi apenas usada para justificar que as colunas de $A_{P_x}$ são linearmente dependentes. Acontece que esta nova condição, mais fraca, não é só suficiente como necessária. Provamos, agora, a
	
	\begin{prop}
	Seja $P$ o seguinte pol na forma padrão:
	
	\[
	\begin{cases}
	\min\limits_x cx\\
	Ax = b\\
	x \geq 0
	\end{cases}
	\]
	
	Então, $x \in X_P$ é vértice de $X_P$ sse $A_{P_x}$ tem as suas colunas todas linearmente independentes.
	\end{prop}
	
	\begin{proof}
	Primeiro, a parte que já está feita: se as colunas de $A_{P_x}$ não forem linearmente independentes, $x$ não é vértice. ($\rightarrow$) Construa-se $\tilde h \neq 0$ tal que $\tilde h_i = 0$ para $i \not \in P_x$, e $\tilde h_{P_x}$ é tal que $A_{P_x} \tilde h_{P_x} = 0$. Isto pode ser feito porque, por hipótese, a solução $A_{P_x} = 0$ tem soluções não-triviais.
	
	Considere-se a função $f(t) = x + t \tilde h$. Pretendemos arranjar $t$ tal que \allowbreak ${f(t), f(-t) \geq 0}$, pois assim ter-se-ia que $x \pm t \tilde h \in X_P$ com $t \tilde h \neq 0$, o que justifica $x$ não ser vértice conforme a prop \ref{vertexvpmh}.
	
	Para este objetivo, decomponha-se esta função em $f_{P_x}(t)$ e $f_{N \setminus P_x}(t)$. A segunda é sempre $\geq 0$ (por ser sempre igual a zero), pelo que basta examinar a primeira.
	
	Mas repare-se que, como $f$ é contínua (soma de uma constante com uma função linear), a sua restrição às coordenadas de $P_x$ também. Para mais, note-se que $f_{P_x}(0) > 0$, pelo que, por continuidade, existe uma vizinhança $\varepsilon$ de zero onde $f_{P_x}(t) > 0$, e então $f(t) \geq 0$. Escolhendo um $t$ diferente de zero nesta vizinhança, por exemplo, $t = \varepsilon/2$, temos $f(t), f(-t) > 0$, como desejado.
	
	A partir daqui, aplicando o raciocínio anterior, usando $h = t \tilde h$, concluimos que $x$ não é vértice de $X_P$.
	
	Agora, a segunda parte ($\leftarrow$): mostrar que se $x \in X_P$ e as colunas de $A_{P_x}$ são linearmente independentes, então $x$ é vértice.
	
	Para fazer isto, considere-se o vetor $w$ definido como
	
	\[w_i = (\text{$0$ se $i \in P_x$, $-1$ caso contrário})\]
	
	E o escalar $0$.
	
	Então, claramente temos $w \cdot x = 0$.
	
	Agora, mostre-se que, para todo $y \in X_P \setminus \{x\}$ se tem $w \cdot y < 0$. Isto é claramente equivalente (dado que $y \in X_P$ e então $y \geq 0$) a mostrar que $y_i \neq 0$ para algum $i \not \in P_x$. Ou seja, que $P_y \not \subseteq P_x$.
	
	Para este efeito, suponha-se que $y \in X_P$ e $P_y \subseteq P_x$. Vamos mostrar que $y = x$.
	
	Se $y \in X_P$, temos $Ay = b$. Mas como $P_y \subseteq P_x$, temos que $Ay = A_{P_x} y_{P_x} = b$. Mas como as colunas de $A_{P_x}$ são linearmente independentes, a solução do sistema $A_{P_x} v = b$, se existir (neste caso existe), é única. Assim sendo, $y_{P_x} = x_{P_x}$, e como as outras coordenadas de $y$ são todas 0, como as de $x$, temos $y = x$, como queriamos demonstrar.
	
	Logo, para $y \in X_P \setminus \{x\}$ tem-se sempre $w \cdot y < 0$, como se pretendia demonstrar, e então $x$ é vértice de $X_P$.
	\end{proof}
	
	Estamos agora em condições de justificar o seguinte:
	
	\begin{prop}
	Seja $P$ o pol
	
	\[
	\begin{cases}
	\min\limits_x cx\\
	Ax = b\\
	x \geq 0
	\end{cases}
	\]
	
	Se $S_P$ é não-vazio, contém pelo menos um vértice de $X_P$.
	\end{prop}
	
	\begin{proof}
	Suponha-se que $S_P$ é não-vazio. A função $x \mapsto \#P_x$ vai de $S_P$ para $\N_0$, pelo que é minimizada nalgum ponto. Chamemos-lhe $z$. Mostraremos que $z$ é vértice de $X_P$.
	
	Para fazer isto, usaremos a caraterização feita há pouco: mostraremos que as colunas de $A_{P_z}$ são linearmente independentes, o que justifica que $z$ é vértice.
	
	A prova faz-se por contrarecíproco. Para mostrar que um $z$ que minimize $\#P_z$ é vértice, supõe-se que não é vértice e mostra-se que não minimiza $\#P_z$.
	
	Suponha-se, então, que $z$ não é vértice, ou seja, que as colunas de $A_{P_z}$ são linearmente dependentes. Vamos arranjar um elemento $\tilde z$ de $S_P$ tal que $\#P_{\tilde z} < \#P_z$
	
	Sabendo que as colunas de $A_{P_z}$ são linearmente dependentes, existe solução não-trivial do sistema $A_{P_z} x = 0$. Seja $\tilde h$ tal que $\tilde h_i = 0$ para $i \not \in P_z$, e $\tilde h_{P_z}$ é solução não-trivial de $A_{P_z} \tilde h_{P_z} = 0$ Suponha-se, sem perda de generalidade, que $\tilde h$ tem pelo menos uma coordenada positiva. Podemos fazer isto, pois, se não for o caso, substitua-se $\tilde h$ por $-\tilde h$.
	
	Repare-se, primeiro, no seguinte detalhe importante: $c \tilde h = 0$. Isto é pois, usando o mesmo raciocínio que na prova anterior, existe $t \neq 0$ tal que $z \pm t \tilde h \in X_P$. Estes dois vetores têm pontuação $cz \pm t c \tilde h$, pelo que, se $c \tilde h$ não fosse zero, um destes teria pontuação melhor do que $z$. Isto contraria a hipótese de $z$ ser solução.
	
	Assim sendo, para todo o $t$ tal que $z - t \tilde h$ é admissível, este é solução.
	
	A ideia é, agora, escolher $t$ de modo a que $z - t \tilde h$ tenha menos coordenadas positivas do que $z$, mas continue admissível. Para fazer isto, efetivamente, o que se faz é `subir o valor de $t$ continuamente até uma das coordenadas ser zero'. (Isto vai ser formalizado já a seguir.) Essa coordenada ficará zero, e nenhuma coordenada que fosse antes zero deixou de o ser, pelo que o número de coordenadas positivas diminuiu... E o vetor continua a ser admissível porque a condição $Ax = b$ nunca deixa de ser verdade, e a condição $x \geq 0$ continua a ser verdade porque não deixamos nenhuma coordenada ir para os negativos.
	
	Em termos formais: considere-se o conjunto $T = \{\,\frac {z_i} {\tilde h_i} \mid i \in P_{\tilde h}\,\}$. Este conjunto é não-vazio (assumimos que $\tilde h$ tinha pelo menos uma coordenada positiva), finito e todos os seus elementos são positivos (pois $P_{\tilde h} \subseteq P_z$). Considere-se, então, o seu mínimo, $\alpha$. Vamos ver que o vetor $\tilde z = z - \alpha \tilde h$ é admissível e tem menos coordenadas positivas do que $z$.
	
	Como já vimos, obedece à condição $A \tilde z = b$, pelo que basta certificarmo-nos que $\tilde z \geq 0$. Fazemos isto coordenada a coordenada.
	
	Examine-se $\tilde z_i = z_i - \alpha \tilde h_i$. Se $i \not \in P_{\tilde h}$, claramente $\tilde z_i \geq z_i \geq 0$. Pelo outro lado, se $i \in P_{\tilde h}$, temos que $\alpha \leq z_i/\tilde h_i$, e então $-\alpha \geq -z_i/\tilde h_i$, donde $z_i - \alpha \tilde h_i \geq z_i - \frac {z_i}{\tilde h_i} h_i = 0$. Como se pretendia demonstrar.
	
	Assim sendo, o vetor $\tilde z$ é admissível. Já vimos que é solução. E, finalmente, mostramos que tem menos coordenadas positivas do que $z$.
	
	Repare-se que para todo $i$, se $z_i = 0$ então $\tilde h_i = 0$, pelo que nenhumas coordenadas positivas `novas' aparecem. Pelo outro lado, pelo menos uma coordenada positiva fica nula. Nomeadamente, visto que $\alpha$ é o valor mínimo do conjunto $\{\,\frac {z_i} {\tilde h_i} \mid i \in P_{\tilde h}\,\}$, é da forma $\alpha = z_j / \tilde h_j$ para algum $j$. Vamos examinar a coordenada $j$ de $\tilde z$.
	
	Sabemos que $z_j > 0$. Mas também temos que $\tilde z_j = 0$, pois $\tilde z_j = z_j - \alpha \tilde h_j = z_j - \frac {z_j}{\tilde h_j} \tilde h_j = 0$. Isto mostra, então, que $\tilde z$ tem pelo menos uma coordenada positiva a menos que $z$, o que mostra que $\tilde z$ é o elemento de $S_P$ que pretendiamos arranjar. Isto conclui a nossa prova.
	\end{proof}
	
	\subsection{O método dos vetores básicos}
	
	Estamos perto de ter um método primitivo de resolução de problemas de otimização linear. Dado um pol, podemos sempre pô-lo na forma padrão. Uma vez nesta forma, podemos examinar os vértices.
	
	A ideia é que, em principio (vamos provar isto) haverá um número finito de vértices. Se o problema tiver solução, o vértice de melhor pontuação será solução. Daí em diante, é preciso verificar que, de facto, $S_P$ é não-vazio. Os detalhes de tal verificação serão feitos mais tarde: por agora, arranjaremos um método de encontrar os vértices de $X_P$ para $P$ na forma padrão.
	
	A ideia é a seguinte: considere-se um conjunto de índices $B \subseteq N$ tal que $A_B$ tem colunas linearmente independentes. Então, a equação $A_B x = b$ tem zero ou uma soluções. Isto dá-nos o primeiro método, mais primitivo possível, de encontrar os vértices de $X_P$:

	\begin{lstlisting}[mathescape=true, keepspaces=true]
Para todo o $B \subseteq N$:
  Se as colunas de $A_B$ são linearmente independentes:
    Resolver o sistema $A_B x = b$.
    Se este tiver solução $x$:
      Se $x \geq 0$:
        Definir $v \in \R^n$ de modo a que $v_B = x$ e $v_{N\setminus B} = 0$
        Adicionar $v$ à lista de vértices.
	\end{lstlisting}
	
	Este método funciona, mas tem um enorme problema: se estamos em dimensão $n$, temos $2^n$ sistemas para resolver. Procuramos, então, uma forma mais computacionalmente eficiente de fazer as coisas.
	
	Lembre-se da Álgebra Linear que qualquer conjunto linearmente independente de vetores pode ser extendido a uma base. Isso é refletido na seguinte proposição:
	
	\begin{prop}
	Dada uma matriz $A$ $m \times n$ e um conjunto de índices $B \subseteq N$, dizemos que $B$ é uma \emph{base de índices} se as colunas de $A_B$ formam uma base de $\im A$.
	
	Se $B \subseteq N$ é tal que as colunas de $A_B$ são linearmente independentes, existe uma base de índices $B' \supseteq B$.
	\end{prop}
	
	\begin{proof}
	Esta prova será feita mostrando que, se $B$ é tal que $A_B$ tem colunas linearmente independentes mas $B$ não é base de índices, é possível adicionar um elemento $b$ a $B$ tal que as colunas de $B \cup \{b\}$ continuem a ser linearmente independentes. Feito isto, o algoritmo para extender $B$ a uma base passa a ser repetir este processo até se ter um conjunto de tamanho igual à caraterística de $A$.
	
	Assim sendo, basta mostrar este passo. Suponha-se que $B$ está sob estas condições, mas não é uma base de índices. Então, tem de existir um vetor-coluna de $A$ que não está em $\im A_B$. Isto pois, caso contrário, $\im A = \im A_B$, e $B$ seria uma base de índices. Assim sendo, escolha-se uma tal coluna, e seja $b$ o seu índice. As colunas de $A_{B \cup \{b\}}$ são linearmente independentes, como o leitor poderá facilmente verificar, o que termina a nossa prova.
	\end{proof}
	
	Dizemos que um vetor $x$ é básico (no contexto de um pol $P$ na forma padrão) se $P_x$ está contido numa base de índices $B$.
	
	O que a proposição anterior mostra é que todo vértice é básico (pegue-se em $P_x$ e extenda-se a uma base). Pelo outro lado, é fácil ver que qualquer vetor básico $x$ tem a propriedade que as colunas de $A_{P_x}$ são linearmente independentes. Temos, então, que os vetores admissíveis básicos e os vértices de $X_P$ são os mesmos.
	
	As bases de índices têm uma propriedade notável: no contexto do pol
	
	\[
	P =
	\begin{cases}
	\min\limits_x cx\\
	Ax = b\\
	x \geq 0
	\end{cases}
	\]
	
	Se a equação $Ax = b$ é impossível, este problema trivialmente não tem solução, visto que o conjunto admissível é vazio. Se isto acontece, dizemos que o pol $P$ é \emph{vazio}.
	
	Pelo outro lado, se $b \in \im A$, dada um base de índices $B$, temos que o sistema de equações $A_B x = b$ tem uma e só uma solução.
	
	Outra observação importante é que as bases de índices de $A$ têm todas tamanho igual à dimensão do espaço de colunas de $A$, ou seja, a sua caraterística.
	
	Assim sendo, considere-se o seguinte (novo) algoritmo.
	
	\begin{lstlisting}[mathescape=true, keepspaces=true]
Verificar se o problema é vazio.
Caso afirmativo, a lista de vértices é vazia.
Caso contrário:
  Seja $k$ a caraterística de $A$.
  Para todo o $B \subseteq N$ tal que $\#B = k$:
    Se as colunas de $A_B$ são linearmente independentes:
      Resolver o sistema $A_B x = b$.
      Se $x \geq 0$:
        Definir $v \in \R^n$ de modo a que $v_B = x$ e $v_{N\setminus B} = 0$
        Adicionar $v$ à lista de vértices.
	\end{lstlisting}
	
	Este algoritmo é muito mais económico do que o anterior. Ao passo que no outro havia $2^n$ possibilidades para verificar, aqui é preciso apenas verificar $\binom{n}{k}$ conjuntos, onde $k$ é a caraterística de $A$.
	
	Para obter um candidato a solução basta, agora, calcular os vértices de $X_P$, $v_1, v_2, \cdots, v_\ell$, calcular $c v_i$ para $i = 1, \cdots, \ell$ e ver qual destes é menor. Pegando, então, num vértice $v_j$ que minimize $c v_j$, se o conjunto solução for não-vazio, contém $v_j$. Precisamos, então, de um método de verificar se um vértice é solução. É a isso que nos dedicamos agora.
	
	\section{Cones}
	
	\subsection{Introdução}
	
	No estudo do pol na forma padrão e canónica, é muito comum ver expressões da forma $Ax$ com $x \geq 0$. Ou seja, combinações lineares não-negativas das colunas de $A$.
	
	Este fenómeno é tão comum que tem um nome.
	
	\begin{definition}
	Sejam $a_1, a_2, \cdots, a_r$ vetores de $\R^n$. O \emph{cone formado por $a_1, \cdots, a_r$} é o conjunto:
	
	\[\{\, \sum_{i=1}^r t_i a_i \mid t_i \in \R^+_0 \text{ para } i = 1, \cdots, r \,\}\]
	
	Em particular, é muito comum estes vetores serem as colunas de uma matriz. Assim sendo, dada uma matriz $A$, definimos \emph{o cone das colunas de $A$}, denominado $\CC A$, como:
	
	\[\{\, Ax \mid x \in \R^n, x \geq 0\,\}\]
	
	É fácil verificar que isto é o mesmo que o cone formado pelas colunas de $A$.
	\end{definition}
	
	Nesta secção, estudaremos os cones, que têm surpreendente aplicação ao problema de otimização linear. O nosso principal resultado será o chamado \emph{lema de Farkas}. Este e as suas variantes serão de grande utilidade para encontrar métodos para descobrir se um vetor é solução.
	
	Um destes métodos é o chamado \emph{método das linhas ativas}, e vamos esboçar metade da sua prova de correção, com o intuito de motivar o que se segue.
	
	\subsection{Método das linhas ativas (Parte 1)}
	
	Suponhamos que temos um pol na forma semicanónica:
	
	\[
	P =
	\begin{cases}
	\max\limits_x cx\\
	Ax \leq b
	\end{cases}
	\]
	
	Já vimos que, para o pol na forma padrão, o conjunto-solução, sempre que não-vazio, contém pelo menos um vértice. Em geral, infelizmente, isto não é verdade. Por exemplo, o pol mais trivial possível a uma dimensão:
	
	\[
	\begin{cases}
	\max\limits_x 0x\\
	\text{(Sem condições...)}
	\end{cases}
	\]
	
	Este pol tem solução (qualquer $x$, na realidade) mas não tem nenhum vértice. Isto mostra que a proposição ``o conjunto solução, se não-vazio, contém pelo menos um vértice'' não é verdade em geral. No entanto, é possível arranjar resultados parecidos, em espírito.
	
	Para arranjar uma medida de `quão na fronteira estamos', fazemos a seguinte definição:
	
	\begin{definition}
	Dado o pol $P$ definido acima, a $i$-ésima linha de $A$ (denotada, por agora, de $a_i$) diz-se \emph{ativa relativamente a $x$} se $a_i x = b$.
	\end{definition}
	
	Já vimos no início que $x$ pertence à fronteira de $X_P$ sse $A$ tem pelo menos uma linha ativa relativamente a $x$. O que vamos ver nesta secção é que é possível averiguar se um vetor é solução examinando as linhas ativas.
	
	\begin{definition}
	Dado um vetor $x$ de $X_P$, $A^x$ representa a matriz das linhas ativas de $x$ em $A$.
	\end{definition}
	
	%todo add motivação?
	
	\begin{prop}
	Seja $x$ um vetor admissível de $P$. $x$ não é solução sse existe $w \in \R^n$ tal que $cw > 0$ e $A^x w \leq 0$
	\end{prop}
	
	\begin{proof}
	Prove-se primeiro a implicação ($\leftarrow$). Suponha-se que existe $w \in \R^n$ tal que $cw > 0$ e $A^x w \leq 0$. A ideia é `somar $w$ em pequenas quantidades'.
	
	É óbvio que se existe $t$ positivo tal que $x + tw \in X_P$, $x$ não é solução, pois este novo vetor tem pontuação melhor do que $x$.
	
	Para arranjar tal $t$, considere-se cada linha de $A(x + tw)$.
	
	Para todo $i$, há dois casos:
	
	Ou $a_i$ é linha ativa, e então $a_i (x + tw) = a_i x + t a_i w \leq a_i x = b_i$ para qualquer $t$,
	
	Ou $a_i$ não é linha ativa, e então $a_i (x + t w) = a_i x + t a_i w$. Como $a_i x < b_i$, temos que, para $t$ de módulo menor que $\lvert \frac{b_i - a_i x}{a_i w} \rvert$, $a_i x + t a_i w \leq b_i$.
	
	Considere-se então $t$ pequeno o suficiente para que $a_i (x + t w) \leq b_i$ para todo $i$ (é fácil ver que tal $t$ existe) e temos, como desejado, que este vetor é admissível e tem pontuação melhor do que $x$.
	
	Para provar a implicação ($\rightarrow$) suponha-se que $x$ não é solução. Então, existe um vetor $x+w$ de pontuação melhor do que $x$. Vamos mostrar que este $w$ obedece às condições do enunciado.
	
	Obviamente $cw > 0$, pois $c(x+w) = cx + cw > cx$ por hipótese. Falta então provar que $A^x w \leq 0$.
	
	Mas se isso não fosse o caso, e houvesse $i$ tal que $a_i$ é linha ativa e $a_i w > 0$, ter-se-ia que a linha $i$ de $A(x+w)$ seria $a_i x + a_i w > a_i x  = b_i$, e então $x+w$ não seria admissível. Contradição.
	\end{proof}
	
	Esta proposição vai-nos ajudar a desenvolver o método das linhas ativas. Falta, agora, caraterizar quando é que existe $w$ tal que $cw > 0$ e $A^x w \leq 0$. Para dar uma ideia, visualize-se a situação.
	
	Repare-se, primeiro que tudo, que a condição $cw > 0$ implica que $w$ é diferente de zero, pelo que $w$ define um hiperplano. Para mais, recorde-se que os hiperplanos dividem o espaço em dois. A condição que temos, então, é que, se considerarmos $c^T$ e as colunas de $A^{xT}$ como vetores em $\R^n$, o hiperplano definido por $w$ separa $c^T$ das colunas da matriz:
	
	\begin{tikzpicture}
	\draw[->] (-4, 0) -- (4, 0) node[anchor=south] {x};
	\draw[->] (0, -4) -- (0, 4) node[anchor=west] {y};
	
	\draw[->, color=green, very thick] (0,0) -- (-0.33333,1) node[anchor=south, color=black]{$w$};
	\draw[color=green, very thick] (-3 * 1.5,-1 * 1.5) -- (3 * 1.5,1 * 1.5);
	
	\draw[->, color=purple, very thick] (0,0) -- (3,1);
	\draw[->, color=purple, very thick] (0,0) -- (1,-2) node[anchor=north, color=black]{(As colunas de $A^{xT}$)};
	\draw[->, color=purple, very thick] (0,0) -- (-1,-3);
	
	
	\draw[->, color=blue, very thick] (0,0) -- (-2,4) node[anchor=south west, color=black]{$c^T$};
	\end{tikzpicture}
	
	Vamos ver esta mesma imagem, mas, a vermelho, está o cone das colunas de $A^{xT}$.
	
	\begin{tikzpicture}
	\fill[->, color=purple!50, very thick] (0,0) -- (3 * 1.5, 1*1.5) --  (3 * 1.5, -3.5) -- (-1 * 3.5/3, -3.5);
	
	\draw[->] (-4, 0) -- (4, 0) node[anchor=south] {x};
	\draw[->] (0, -4) -- (0, 4) node[anchor=west] {y};
	
	\draw[->, color=green, very thick] (0,0) -- (-0.33333,1) node[anchor=south, color=black]{$w$};
	\draw[color=green, very thick] (-3 * 1.5,-1 * 1.5) -- (3 * 1.5,1 * 1.5);
	
	
	
	\draw[->, color=purple, very thick] (0,0) -- (3,1);
	\draw[->, color=purple, very thick] (0,0) -- (1,-2) node[anchor=north, color=black]{(O cone das colunas de $A^{xT}$)};
	\draw[->, color=purple, very thick] (0,0) -- (-1,-3);
	
	\draw[->, color=blue, very thick] (0,0) -- (-2,4) node[anchor=south west, color=black]{$c^T$};
	
	\end{tikzpicture}
	
	A ideia é, então, a seguinte: mostrar que se $c^T$ está fora deste cone, é possível arranjar tal $w$. Isto porque verificar se $c^T$ está fora do cone é possívelmente mais fácil. Por exemplo, se as linhas de $A^x$ são linearmente independentes, a equação $A^{xT} y = c^T$ tem no máximo uma solução. É então possível tentar encontrar tal solução. Se ela existir e for $\geq 0$, então $c^T$ está no cone desejado, e $x$ é solução. Caso contrário, não está, e $x$ não é solução.
	
	Isto motiva, então, o enunciado do lema de Farkas:
	
	\begin{conjetura} (Lema de Farkas)
	
	Seja $A$ uma matriz $n \times m$, $v$ um vetor de $\R^n$.
	
	$v \not\in \CC A$ sse existe $w$ tal que $w \cdot v > 0$ e $A^T w \leq 0$.
	\end{conjetura}
	
	Focamo-nos, agora, em provar este lema.
	
	\subsection{O Lema de Farkas (Parte 1)}
	
	Para provar o Lema de Farkas vai-nos ser necessário provar vários resultados intermédios. É, no entanto, feito o esboço da prova, para motivar esses resultados.
	
	O objetivo é, dado um cone $C$ e um ponto $v$ fora deste:
	
	\begin{tikzpicture}
	\fill[->, color=purple!50, very thick] (0,0) -- (3.5, 2) --  (3.5, -1.5) -- (1, -1.5);
	
	\draw[->] (-2, 0) -- (4, 0) node[anchor=south] {x};
	\draw[->] (0, -2) -- (0, 3) node[anchor=west] {y};
	
	\draw[->, color=blue, very thick] (0,0) -- (1,2.5) node[anchor=south west, color=black]{$v$};
	
	\end{tikzpicture}
	
	Encontrar um hiperplano que os separe.
	
	\begin{tikzpicture}
	\fill[->, color=purple!50, very thick] (0,0) -- (3.5, 2) --  (3.5, -1.5) -- (1, -1.5);
	
	\draw[->] (-2, 0) -- (4, 0) node[anchor=south] {x};
	\draw[->] (0, -2) -- (0, 3) node[anchor=west] {y};
	
	\draw[color=green, very thick] (-3.5 / 2, -1) -- (3.5, 2);
	
	\draw[->, color=blue, very thick] (0,0) -- (1,2.5) node[anchor=south west, color=black]{$v$};
	
	\end{tikzpicture}
	
	O que vamos fazer é considerar a projeção ortogonal de $v$, digamos $z$, no cone
	
	\begin{tikzpicture}
	\fill[->, color=purple!50, very thick] (0,0) -- (3.5, 2) --  (3.5, -1.5) -- (1, -1.5);
	
	\draw[->] (-2, 0) -- (4, 0) node[anchor=south] {x};
	\draw[->] (0, -2) -- (0, 3) node[anchor=west] {y};
	
	\draw[color=green, very thick] (-3.5 / 2, -1) -- (3.5, 2);
	
	\draw[->, color=blue, very thick] (0,0) -- (1,2.5) node[anchor=south west, color=black]{$v$};
	\draw[->, color=yellow, very thick] (0,0) -- (1.83077,1.04617) node[anchor=east, color=black]{$z$};
	
	\draw[dashed] (1,2.5) -- (1.83077,1.04617);
	
	\draw (1.83077-0.248072*0.5,1.04617+0.43412*0.5)
		-- ++(0.43412*0.5, 0.248072*0.5)
		-- ++(0.248072*0.5,-0.43412*0.5);
	
	\end{tikzpicture}
	
	E consideramos o plano definido por $w = v-z$.
	
	\begin{tikzpicture}
	\fill[->, color=purple!50, very thick] (0,0) -- (3.5, 2) --  (3.5, -1.5) -- (1, -1.5);
	
	\draw[->] (-2, 0) -- (4, 0) node[anchor=south] {x};
	\draw[->] (0, -2) -- (0, 3) node[anchor=west] {y};
	
	\draw[color=green, very thick] (-3.5 / 2, -1) -- (3.5, 2);
	
	\draw[->, color=blue, very thick] (0,0) -- (1,2.5) node[anchor=south west, color=black]{$v$};
	\draw[->, color=yellow, very thick] (0,0) -- (1.83077,1.04617) node[anchor=east, color=black]{$z$};
	
	\draw[->, color=green, very thick] (1.83077,1.04617) -- (1,2.5) node[anchor=west, color=black, pos=0.5]{$w$};
	
	\draw (1.83077-0.248072*0.5,1.04617+0.43412*0.5)
		-- ++(0.43412*0.5, 0.248072*0.5)
		-- ++(0.248072*0.5,-0.43412*0.5);
	
	\end{tikzpicture}
	
	A partir daqui, há dois grandes passos que é preciso justificar:
	
	\begin{itemize}
	\item O que queremos dizer por projeção ortogonal, e como a encontramos?
	\item Como sabemos que o plano assim definido separa o ponto $v$ do cone $C$?
	\end{itemize}
	
	Dedicamo-nos agora ao primeiro.
	
	\subsection{Cones primitivos}
	
	Em geral, a `projeção ortogonal de $x$ em $Y$' pode ser definida como o $y \in Y$ que minimize $\lvert x - y \rvert$. No entanto, é preciso assegurar que esta função pode ser minimizada.
	
	Felizmente, em certas condições, a existência deste mínimo é-nos garantida.
	
	\begin{prop}
	Se $x \in \R^n$ e $Y$ é um subconjunto fechado e não-vazio de $\R^n$, existe $y \in Y$ tal que $\lvert x - y \rvert \leq \lvert x - y' \rvert$ para todo $y' \in Y$.
	\end{prop}
	
	\begin{proof}
	Primeiro que tudo, considere-se o caso em que $Y$ é limitado. Assim sendo, visto que também é fechado, é compacto. Logo, a função $f(y) = \rvert x - y \lvert$ tem mínimo por Weierstrass, o que mostra a nossa afirmação.
	
	No caso de $Y$ não ser limitado, pegue-se num $y'$ qualquer em $Y$. Considere-se $Y' = Y \cap \{\, y \in \R^n \mid \, \rvert x - y \lvert \leq \rvert x - y' \lvert \,\}$. Este conjunto já é compacto, e então o raciocínio anterior aplica-se, e existe $y$ que minimize $f$ em $Y'$.
	
	Mas se $y$ minimiza $f$ em $Y'$, também minimiza $f$ em $Y$, pois para qualquer $y''$ em $Y \setminus Y'$ tem-se $f(y) \leq f(y') \leq f(y'')$.
	\end{proof}
	
	Assim sendo, dado um ponto $x \in \R^n$ e um conjunto fechado $Y$, sabemos que existe a chamada \emph{projeção ortogonal de $x$ em $Y$}, ou seja, o $y \in Y$ que minimiza $\rvert x - y \lvert$.
	
	Precisamos, agora, de assegurar que os cones são fechados.
	
	Repare-se que qualquer cone em $\R^n$ pode ser expresso como $\{\,Ax \mid x \in \R^m, x \geq 0\,\}$ para alguma matriz $m \times n$. (As suas colunas são os vetores que geram o cone.)
	
	Vamos começar por considerar o caso particular de um cone gerado por uma matriz não-singular $A$. Representamos por $\R^n_{+0}$ o conjunto $\{\,x \in \R^n \mid x \geq 0\,\}$. Repare-se que este conjunto é fechado (deixado como exercício ao leitor.)
	
	Assim sendo, queremos considerar o conjunto $A \R^n_{+0}$, e queremos justificar que este é fechado.
	
	Repare-se que a função linear $x \mapsto Ax$ é contínua. Infelizmente, não é necessáriamente verdade que a imagem sob uma função contínua de um conjunto fechado seja fechado. No entanto, há um resultado que nos pode ajudar:
	
	\begin{prop}
	Se $f : \R^n \rightarrow \R^n$ é uma bijeção de inversa contínua e $X$ é um conjunto fechado, $f(X)$ é fechado.
	\end{prop}
	
	\begin{proof}
	Seja $g$ a inversa de $f$, que sabemos ser contínua. Temos que $f(X) = g^{-1}(X)$. Como $X$ é fechado, $g^{-1}(X)$ é fechado, e então $f(X)$ é fechado.
	\end{proof}
	
	Repare-se que a nossa função $x \mapsto Ax$ está nestas condições, pois tem inversa contínua (em particular, linear) $x \mapsto A^{-1} x$. Assim sendo, a imagem sob $A$ de $\R^n_{+0}$ é fechada, e o nosso cone é fechado.
	
	Este raciocínio pode ser feito, com algumas modificações, na hipótese mais fraca de $A$ ter colunas linearmente indepentes.
	
	\begin{prop}
	Dizemos que um cone $A \R^m_{+0}$ é \emph{primitivo} se as colunas de $A$ são linearmente independentes.
	
	Qualquer cone primitivo é fechado.
	\end{prop}
	
	\begin{proof}
	Considere-se os vetores (colunas de $A$) $a_1, \cdots, a_m$ linearmente independentes, e extendenda-se este conjunto a uma base $a_1, \cdots, a_m, a_{m+1}, \cdots, a_n$.
	
	Se $A'$ é a matriz cujas colunas são $a_1, \cdots, a_n$, o cone $A \R^m_{+0}$ é igual a \allowbreak ${A' (\R^m_{+0} \times \{0\})}$. A função $x \mapsto A'x$ tem inversa contínua, e o conjunto $\R^m_{+0} \times \{0\}$ é fechado, pelo que o cone original é fechado.
	\end{proof}
	
	Infelizmente, não é fácil extender este raciocínio a qualquer cone. É possível, no entanto escrever qualquer cone como uma união de cones primitivos.
	
	A ideia é a seguinte: suponhamos que tenho o cone gerado por $a_1, \cdots, a_m$. Qualquer elemento $x$ deste cone pode ser escrito como $t_1 a_1 + \cdots + t_m a_m$ para $t_i \geq 0$, $i = 1, \cdots, m$. No entanto, da Álgebra Linear, sabemos que podemos escrevê-lo como $t_1 a_{i_1} + \cdots t_k a_{i_k}$ onde $a_{i_1}, \cdots, a_{i_k}$ são linearmente independentes. Se conseguirmos assegurar que há uma forma de escrever isto de modo a que $t_1, \cdots, t_k$ são todos $\geq 0$, obtemos que $x$ pertence ao cone (primitivo) gerado por $a_{i_1}, \cdots, a_{i_k}$.
	
	Feito isto, considere-se o cone gerado por $B = \{a_1, \cdots, a_n\}$. O que isto provaria seria que qualquer $x$ no cone gerado por $B$ pertence a um cone gerado por um $B' \subseteq B$ tal que $B'$ é linearmente independente, e então o cone gerado por $B$ é a união dos cones gerados pelos subconjuntos linearmente independentes de $B$, e então é união (finita) de cones primitivos, sendo então fechado.
	
	Falta apenas, então, mostrar que é sempre possível escrever um elemento do cone como soma não-negativa de elementos linearmente independentes.
	
	\begin{prop}
	Qualquer cone é união finita de cones primitivos, sendo então fechado.
	\end{prop}
	
	\begin{proof}
	Pelo que já vimos antes, basta mostrar que, para $x$ da forma $t_1 a_1 + \cdots t_m a_m$, ($t_1, \cdots, t_m \geq 0$), existem $i_1, \cdots, i_k$ e $r_1, \cdots, r_k \geq 0$ tal que $a_{i_1}, \cdots, a_{i_k}$ são linearmente independentes e $x = r_1 a_{i_1} + \cdots + r_k a_{i_k}$.
	
	Para fazer isto, suponha-se que $x$ pertence ao cone gerado por $a_1, \cdots, a_m$, e considere-se uma forma de escrever $x$ que minimize os índices diferentes de zero. Ou seja, vamos escrever $x$ como $t_1 a_1 + \cdots + t_m a_m$ de uma forma que faça com que o número de $i$ tal que $t_i > 0$ seja mínimo.
	
	Sejam $i_1, \cdots, i_k$ os índices tais que $t_{i_1}, \cdots, t_{i_k} > 0$. Provamos que $a_{i_1}, \cdots, a_{i_k}$ são linearmente independentes.

	Para este objetivo, suponha-se que não são. Mostraremos que afinal esta forma de escrever $x$ não é o mínimo que tinhamos suposto.
	
	Se não são linearmente independentes, existem $s_1, \cdots, s_k$, nem todos iguais a zero (suponha-se spdg que pelo menos um deles é maior que zero), tal que $s_1 a_{i_1} + \cdots + s_k a_{i_k} = 0$
	
	Comparando as duas equações que temos até agora: (E pondo $r_j = t_{i_j}$)
	
	\begin{align*}
	r_1 a_{i_1} + \cdots + r_k a_{i_k} &= x \\
	s_1 a_{i_1} + \cdots + s_k a_{i_k} &= 0
	\end{align*}
	
	A ideia é subtrair a equação de baixo à de cima de modo a deixar todos os termos $\geq 0$, mas cancelar pelo menos um, e então mostrar que existe uma forma de somar os $a$'s a zero que tem menos termos não-nulos.
	
	Para este efeito, considere-se o conjunto $\{\,\frac{r_j}{s_j} \mid s_j > 0\,\}$. Este conjunto é não-vazio por hipótese, e é finito, pelo que tem um mínimo $\frac{r_\ell}{s_\ell}$. Ponha-se então $r'_j = r_j - \frac{r_\ell}{s_\ell} s_j$.
	
	É fácil verificar que $r'_j \geq 0$ para todo $j$ (o caso em que $s_j \leq 0$ é trivial, o caso em que $s_j > 0$ é deixado ao leitor) e que $r'_\ell = 0$. Finalmente, é trivial ver que $r'_1 a_{i_1} + \cdots + r'_k a_{i_k} = x$, pois o que foi feito foi subtrair a equação de baixo à equação de cima $\frac{r_\ell}{s_\ell}$ vezes, pelo que o resultado dá $x + \frac{r_\ell}{s_\ell} 0 = x$.
	
	Isto mostra, então, que a forma de somar a $x$ que minimiza o número de termos não-zero tem todos os termos linearmente independentes, pelo que $x$ é soma de vetores linearmente independentes e então pertence a um dos cones primitivos gerados por subconjuntos linearmente independentes de $B = \{a_1, \cdots, a_m\}$.
	
	Como o número de tais subconjuntos é finito, o cone gerado por $B$ é uma união finita de cones primitivos. Como todos este são fechados, e uniões finitas de conjuntos fechados são fechados, o nosso cone é fechado.
	\end{proof}
	
	Esta proposição é a principal razão pela qual passámos pelos cones primitivos. Visto que agora sabemos que os cones são todos fechados, sabemos que podemos considerar projeções ortogonais, e avançamos para o lema de Farkas.
	
	\subsection{O Lema de Farkas (Parte 2)}
	
\end{document}