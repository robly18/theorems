\documentclass{article}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage[utf8]{inputenc}
\usepackage[portuguese]{babel}
\usepackage{tikz}
\usetikzlibrary{patterns}
\usepackage{listings}
\usepackage{ifxetex}

\ifxetex
%xetex is recommended!
\else
\lstset{
  literate=
  {á}{{\'a}}1
  {à}{{\`a}}1
  {ã}{{\~a}}1
  {é}{{\'e}}1
  {ê}{{\^e}}1
  {í}{{\'i}}1
  {ó}{{\'o}}1
  {õ}{{\~o}}1
  {ú}{{\'u}}1
  {ü}{{\"u}}1
  {ç}{{\c{c}}}1
}
\fi

\addto\captionsportuguese{
	\renewcommand*{\proofname}{Dem}
}

\DeclareMathOperator{\interior}{int}
\DeclareMathOperator{\im}{Im}

\title{IO Done Right}
\author{Duarte Maia}
\date{}

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}

\newtheorem{teorema}{Teorema}
\newtheorem{prop}{Prop}

\theoremstyle{definition}
\newtheorem{definition}{Definição}
\newtheorem*{definition*}{Definição}
\newtheorem*{notacao}{Notação}


\begin{document}
	\maketitle
	
	\tableofcontents
	
	\section{Introdução}
	
	TODO: Escrever palavras motivadoras ou algo assim
	
	\section{Prerequisitos}
	
	Vou aqui escrevendo coisas à medida que elas são necessárias.
	
	Obviamente é preciso saber um pouco de cálculo I e AL.
	
	\section{Problemas de Otimização Linear}
	
	\subsection{Introdução}
	
	Um problema de otimização é um tipo específico de problema em que o objetivo é minimizar ou maximizar uma certa função (função-objetivo), dentro de um certo domínio, usualmente (no contexto desta cadeira) um subconjunto de $\R^n$ parametrizado por um conjunto de (in)equações.
	
	Mais concretamente, o tipo de problemas com que se lida nesta cadeira são problemas de otimização linear. Estes são problemas em que a função-objetivo é uma função linear $\R^n \rightarrow \R$ e todas as condições no domínio são da forma $ax \leq b$, $ax = b$ ou $ax \geq b$, onde $a$ é um vetor-linha e $b$ é um escalar.
	
	\begin{definition}
	Um \emph{problema de otimização linear} (normalmente abreviado a \emph{pol}) é um problema da forma:
	
	\textbf{Objetivo: } maximizar/minimizar (em função de $x \in \R^n$) o valor de ${c_1 x_1 + c_2 x_2 + \ldots + c_n x_n}$
	
	\textbf{Restrições: } $x$ tem de obedecer a todas as seguintes igualdades

\[
\begin{cases}
	a_{11} x_1 + a_{12} x_2 + \ldots + a_{1n} x_n = b_1 \\ 
	a_{21} x_1 + a_{22} x_2 + \ldots + a_{2n} x_n = b_2 \\
	\vdots \\
	a_{p1} x_1 + a_{p2} x_2 + \ldots + a_{pn} x_n = b_p
\end{cases}
\]

E todas as seguintes desigualdades

\[
\begin{cases}
	a'_{11} x_1 + a'_{12} x_2 + \ldots + a'_{1n} x_n \leq b'_1 \\ 
	\vdots \\
	a'_{q1} x_1 + a'_{q2} x_2 + \ldots + a'_{qn} x_n \leq b'_q
\end{cases}
\]
\[
\begin{cases}
	a''_{11} x_1 + a''_{12} x_2 + \ldots + a''_{1n} x_n \geq b''_1 \\ 
	\vdots \\
	a''_{r1} x_1 + a''_{r2} x_2 + \ldots + a''_{rn} x_n \geq b''_r
\end{cases}
\]
	\end{definition}
	
	Infelizmente, isto é algo trabalhoso de escrever. Como tal, normalmente estas condições são escritas de forma mais compacta usando a linguagem da álgebra linear.
	
	\begin{notacao}
Dados dois vetores $x,y \in \R^n$, dizemos $x \leq y$ se $x_i \leq y_i$ para todo $i$.
	\end{notacao}
	\begin{notacao}
	Considere-se o pol escrito acima. Este é normalmente escrito da seguinte forma:
	
	(Supõe tratar-se de um problema de maximização; caso contrário escreva-se $\min$ no lugar de $\max$)
	
	Seja $c$ o vetor linha $[\,c_1\,c_2\, \cdots \,c_n\,]$,

	$A$ a matriz $p \times n$ cujo $i,j$-ésimo elemento é $a_{ij}$, e considerações análogas para $A'$ a $A''$, com $a'_{ij}$ e $a''_{ij}$ respetivamente
	
	$b$ o vetor coluna $(b_1, b_2, \cdots, b_n)$ e análogamente para $b'$ e $b''$.
	
	\[
	\begin{cases}
	\max\limits_x cx \\
	Ax = b \\
	A'x \leq b' \\
	A''x \geq b''
	\end{cases}
	\]
	\end{notacao}
	
	Normalmente, não é útil considerar um tipo de problema tão geral, pelo que discutímos em baixo formas de passar de um tipo de problemas para outros.
	
	\subsection{Tradução}
	
	Considere-se os seguintes problemas de otimização linear:
	
	\[
	\begin{cases}
	\max\limits_x 2x_1 - 3x_2 \\
	5x_1 + x_2 = 6 \\
	2x_1 + 0x_2 \leq 2
	\end{cases}
	\begin{cases}
	\min\limits_x -2x_1 + 3x_2 \\
	5x_1 + x_2 \leq 6 \\
	5x_1 + x_2 \geq 6 \\
	-2x_1 + 0x_2 \geq -2
	\end{cases}
	\]
	
	Apesar de terem uma aparência diferente, alguma inspeção leva à conclusão que estes são, na realidade, exatamente o mesmo problema. Ou seja, o mesmo problema pode ser representado de mais do que uma forma diferente. Isto leva à possibilidade de considerar `formas canónicas' de expressar os problemas, que sejam mais fáceis de estudar. Se, por exemplo, dissermos que todos os problemas da forma XYZ podem ser resolvidos fazendo ABC, e arranjarmos forma de traduzir qualquer pol para esta forma XYZ, estamos numa boa situação.
	
	\begin{notacao}
	Dado um pol $P$, o conjunto de $x \in \R^n$ que satisfazem as suas condições é denominado de \emph{conjunto admissível de $P$}, representado por $X_P$, ou só $X$ se o pol em questão for óbvio de contexto.
	
	Da mesma forma, o \emph{conjunto solução de $P$}, $S_P$, ou $S$ se $P$ é claro de contexto, é o conjunto de $x \in X$ que maximizam, de facto, a função objetivo. Por outras palavras,
	
	\[S_P = \{\,x \in X_p \mid \forall_{y \in X_p} cx \geq cy\,\}\]
	
	Isto no caso de maximização. No caso de minimização, a desigualdade deve estar trocada.
	\end{notacao}
	
	Há várias formas possíveis de transformar um problema noutro.
	
	\subsubsection{Substituição de equivalências}
	
	Por exemplo, a condição de igualdade $a = b$ pode ser expressa como duas desigualdades: $a \leq b$ e $a \geq b$. Assim sendo, sabemos à partida que qualquer pol que nos venha à cabeça pode ser expresso só com condições de desigualdade.
	
	Para mais, a desigualdade $a \geq b$ é equivalente a $-a \leq -b$. Logo, podemos sempre assumir que as condições que restringem um pol são sempre da forma $Ax \leq b$.
	
	Podemos também assumir sem perda de generalidade que um problema de otimização linear é um problema de maximização, pois minimizar $cx$ é o mesmo que maximizar $-cx$.
	
	Logo, dado um pol qualquer, podemos sempre assumir, sem perda de generalidade, que é da forma
	
	\[
	\begin{cases}
	\max\limits_x cx\\
	Ax \leq b
	\end{cases}
	\]
	
	A esta forma vamos-nos referir, no futuro, como `forma semicanónica', na falta de melhor nome. Sugestões são aceites.
	
	O que foi até agora descrito é o tipo mais simples possível de tradução de um problema a outro: trocar as condições por condições equivalentes, e substituir `maximizar $f$' por `minimizar $-f$'.
	
	Este tipo de tradução tem a propriedade que não muda o conjunto admissível e conjunto solução, mas há traduções que não são tão triviais.
	
	\subsubsection{Adição de positividades (Forma canónica)}
	
	É, em muitos casos, útil estudar um problema de otimização em que sabemos que todas as variáveis são não-negativas. Assim sendo, vamos agora ver que podemos sempre, sem perda de generalidade, assumir que o nosso pol é da forma
	
	\[
	\begin{cases}
	\max\limits_x cx\\
	Ax \leq b\\
	x \geq 0
	\end{cases}
	\]
	
	O truque é modificar o conjunto em que estamos.
	
	Dado um número real $x$, define-se a sua parte positiva, $x^+$, como
	
	\[
	x^+ =
	\begin{cases}
	x & \text{se } x \geq 0 \\
	0 & \text{caso contrário}
	\end{cases}
	\]
	
	e a parte negativa, $x^-$, como
	
	\[
	x^- =
	\begin{cases}
	0 & \text{se } x \geq 0 \\
	-x & \text{caso contrário}
	\end{cases}
	\]
	
	Dado um vetor $x \in \R^n$, define-se $x^+$ como o vetor das partes positivas de $x$, e $x^-$ como o vetor das partes negativas. É claro que $x = x^+ - x^-$, e $x^+, x^- \geq 0$. Isto permite-nos escrever qualquer vetor como a diferença de dois vetores não-negativos.
	
	Assim sendo, considere-se a condição $Ax \leq b$. Esta condição é equivalente a $Ax^+ - Ax^- \leq b$, com $x^+, x^- \geq 0$. Para mais, $cx = cx^+ - cx^-$, pelo que isto sugere a consideração de transformar o pol
	
	\[
	P =
	\begin{cases}
	\max\limits_x cx\\
	Ax \leq b
	\end{cases}
	\]
	
	em
	
	\[
	Q =
	\begin{cases}
	\max\limits_{(x_+, x_-)} cx_+ - cx_-\\
	Ax_+ - Ax_- \leq b\\
	x_+, x_- \geq 0
	\end{cases}
	\]
	
	(Aqui, $(x_+,x_-)$ representa apenas um vetor, de dimensão igual ao dobro da de $x$. O $+$ e $-$ em subscrito é suposto ser sugestivo de que um deles é a parte positiva e o outro a parte negativa de $x$.)
	
	Repare-se que este segundo pol não tem a mesma dimensão que o primeiro: se no primeiro, o vetor $x$ pertence a $\R^n$, no segundo, o vetor $(x_+, x_-)$ pertence a $\R^{2n}$. Portanto, não é imediatamente óbvio que estes são `o mesmo' pol, pelo que vamos fazer a seguinte definição:
	
	\begin{definition}
	Dados dois problemas de otimização linear, $P$ e $Q$, dizemos que \emph{$Q$ é uma $f$-tradução de $P$} se se for possível resolver $P$ sabendo a resposta de $Q$, usando $f$.
	
	Em termos mais precisos, se $f$ é uma função sobrejetiva $X_Q \rightarrow X_P$ tal que $q \in X_Q$ tem uma pontuação melhor que $q' \in X_Q$ (menor se $Q$ é de minimzação, maior se maximização) sse $f(q)$ tem melhor pontuação que $f(q')$.
	\end{definition}
	
	Esta definição tem interesse devido à seguinte proposição:
	
	\begin{prop}
	Se $Q$ é uma $f$-tradução de $P$, $S_Q = f(S_P)$.
	\end{prop}
	\begin{proof}
	Basta reparar que, por definição, $x \in S_Q$ sse $x$ tem pontuação melhor ou igual que $y$ para todo $y \in X_Q$, sse $f(x)$ tem pontuação melhor que $f(y)$ para todo $y \in X_Q$. Queremos então mostrar que isto é sse $f(x) \in S_P$.
	
	Como $f$ é sobrejetivo, isto implica que $f(x)$ tem pontuação melhor ou igual que todo $z$ em $X_P$, o que por definição implica $f(x) \in S_P$. Pelo outro lado, se houver $y \in X_Q$ tal que $f(y)$ tem pontuação melhor que $f(x)$, então $f(x)$ claramente não pode ser solução, pois $f(y)$ é admissível de pontuação melhor que ele.
	\end{proof}
	
	Isto mostra que se resolvermos uma tradução de um pol, é fácil recuperar a solução e conjunto admissível do problema original.
	
	O caso anterior, em que $X_P = X_Q$ e $S_P = S_Q$ é o caso trivial em que $f$ é a identidade. Neste caso, em que pretendemos adicionar a condição de positividade, é ligeiramente mais complicado.
	
	Vamos mostrar que, para o $P$ e $Q$ definidos em cima, $Q$ é uma $f$-tradução de $P$, em que $f$ é a função $f(x_+, x_-) = x_+ - x_-$.
	
	\begin{proof}
	Primeiro, mostre-se que se um vetor pertence a $X_Q$, a sua imagem pertence a $X_P$.
	
	Repare-se que, se $(x_+, x_-) \in X_Q$, então a sua imagem é $f(x_+, x_-) = x_+ - x_-$. Por definição, estes satisfazem $A(x_+ - x_-) \leq b$, donde $A f(x) \leq b$, que é a condição para $f(x) \in X_P$.
	
	Para mostrar sobrejetividade, repare-se que se $x \in X_P$, tem-se $(x^+, x^-) \in X_Q$, e $f(x^+, x^-) = x^+ - x^- = x$.
	
	Agora, queremos mostrar que $(x_+, x_-)$ tem pontuação melhor que $(y_+, y_-)$ sse $f(x_+, x_-)$ tem pontuação melhor que $f(y_+, y_-)$. Mas isto é trivial, pois a pontuação de $f(x_+, x_-)$ é a mesma que a de $(x_+, x_-)$, e o objetivo é, tanto em $P$ como em $Q$, maximizar.
	\end{proof}
	
	Assim sendo, podemos sempre assumir, sem perda de generalidade, que qualquer problema que tenhamos está na chamada \emph{forma canónica}, isto é, é da forma
	
	\[
	\begin{cases}
	\max\limits_x cx\\
	Ax \leq b\\
	x \geq 0
	\end{cases}
	\]
	
	\subsubsection{Desigualdades para igualdades (Forma padrão)}
	
	Outra forma útil de expressar um problema de otimização linear é a chamada forma padrão, da forma seguinte:
	
	\[
	\begin{cases}
	\min\limits_x cx\\
	Ax = b\\
	x \geq 0
	\end{cases}
	\]
	
	Já sabemos que podemos supor que os nossos problemas estão na forma canónica, sem perda de generalidade. Portanto, considere-se o pol
	
	
	\[
	P =
	\begin{cases}
	\max\limits_x cx\\
	Ax \leq b\\
	x \geq 0
	\end{cases}
	\]
	
	E vamos reescrevê-lo na forma padrão.
	
	Para fazer a tradução, é preciso representar desigualdades com igualdades. Para isso, repare-se que $a \leq b$ é a mesma coisa que dizer que $a + y = b$, para algum $y$ não-negativo. Assim, adicionamos as chamadas \emph{variáveis de folga}: Considere-se o pol
	
	\[
	Q =
	\begin{cases}
	\min\limits_{(x,y)} -cx\\
	Ax + y = b\\
	x, y \geq 0
	\end{cases}
	\]
	
	Vamos mostrar que $Q$ é uma $f$-tradução de $P$, com $f(x,y) = x$.
	
	\begin{proof}
	Primeiro, verifique-se que se $(x,y) \in X_Q$ se tem $f(x,y) \in X_P$. Repare-se que a condição $f(x,y) \geq 0$ é trivial, pois $x \geq 0$ por hipótese, e a condição $A f(x,y) \leq 0$ é verídica pois $A f(x,y) = Ax \leq Ax + y = b$.
	
	De seguida, é preciso verificar sobrejetividade. Suponhamos que se quer um objeto cuja imagem sob $f$ seja $x \in X_P$. Considere-se, então, o vetor $(x,y)$, com $y = b - Ax$. Repare-se que este vetor pertence a $X_Q$, e a sua imagem é precisamente $x$, como queríamos demonstrar.
	
	Finalmente, provemos que $(x,y)$ tem melhor pontuação que $(x',y')$ em $Q$ sse $x$ tem melhor pontuação que $x'$ em P.
	
	Mas isto é trivial, pois $(x,y)$ tem melhor pontuação que $(x',y')$ sse $-cx < -cx'$ sse $cx > cx'$, como queríamos demonstrar.
	\end{proof}
	
	Isto mostra, então, que qualquer pol tem uma tradução para um pol na forma padrão.
	
	\subsubsection{Padrão para Canónico}
	
	Viu-se agora uma forma para traduzir problemas da forma canónica para a forma padrão. É possível, também, passar da forma padrão para a canónica (provou-se, aliás, que é possível passar de qualquer pol para a forma canónica) usando os procedimentos usados antes.
	
	No entanto, é muito mais económico fazer o processo simples de transformar todas as igualdades em desigualdades do tipo $\leq$. Ou seja, passar de:
	
	\[
	\begin{cases}
	\min\limits_x cx\\
	Ax = b\\
	x \geq 0
	\end{cases}
	\]
	
	Para
	
	\[
	\begin{cases}
	\max\limits_x -cx\\
	Ax \leq b\\
	-Ax \leq -b\\
	x \geq 0
	\end{cases}
	\]
	
	Já vimos antes que estas traduções são válidas.
	
	\section{Noções Topológicas}
	
	\subsection{Prerequisitos}
	
	Para ler esta secção, é recomendado (i.e. necessário) que o leitor saiba os seguintes conceitos topológicos em $\R^n$:
	
	Conceito de conjunto aberto e conjunto fechado; interior, fronteira e extrior de um conjunto; função contínua; teorema de Bolzano e Weierstrass em $\R^n$.
	
	No que se segue vão ser usados alguns factos sobre estes conceitos que não vão ser justificados, visto que caem no âmbito da cadeira de Cálculo II. Estes serão devidamente assinalados, e o leitor é encorajado a prová-los se não estiver familiar com eles.
	
	\subsection{Intuição geométrica}
	
	Esta secção é dedicada a dar intuição geométrica para motivar os teoremas que se seguem. Vamos, a título de exemplo, partir de um pol específico a duas dimensões na forma canónica.
	
	Considere-se o pol
	
	\[
	P =
	\begin{cases}
	\max\limits_{(x,y)} 2x + y\\
	x + 2y \leq 8\\
	x - y \leq 2 \\
	x, y \geq 0
	\end{cases}
	\]
	
	Vamos representar, no plano cartesiano, a região admissível $X_P$ e o vetor $c^T = (2,1)$.
	
	\begin{tikzpicture}
	\draw[->] (-2, 0) -- (6, 0) node[anchor=south] {x};
	\draw[->] (0, -1) -- (0, 5) node[anchor=west] {y};
	\filldraw[color=red, fill=blue!80, very thick] (0,0) node[anchor=north west, color=black]{0} -- (0,4) node[anchor=east, color=black]{(0,4)} -- (4,2) node[anchor=north west, color=black]{\,(4,2)} -- (2,0) node[anchor=north, color=black]{(2,0)} -- (0,0);
	
	\draw[->, color=green, very thick] (4,2) -- (6,3) node[anchor=west, color=black]{$c^T = (2,1)$};
	\draw[very thick, color=green] (3.5,3) -- (4.5,1);
	\end{tikzpicture}
	
	Uma interpretação para o conjunto solução $S_P$ é o conjunto de vetores admissíveis que maximiza o produto escalar com $c^T$. Intuitivamente, os vetores que `estão o mais possível na direção de $c^T$'.
	
	É possível ver (isto em termos intuitivos, claro) que não há nenhum vetor admissível `para lá' da linha verde desenhada, o que indica que o conjunto solução será o conjunto $\{(4,2)\}$.
	
	Neste caso, o conjunto solução tem apenas um elemento, mas é concebível uma situação em que tivesse mais. Por exemplo, se o vetor $c^T$ estivesse perpendicular ao lado $(0,4)-(4,2)$.
	
	Isto é só especulação com base num desenho, mas dita especulação pode ser útil para motivar conclusões.
	
	Por exemplo, com base nestas considerações, seria de esperar que, por exemplo, em geral o conjunto solução estará contido na fronteira do conjunto admissível, o que quer que isso signifique (a área marcada a vermelho na imagem). Também é fácil uma pessoa convencer-se que tem necessariamente de conter pelo menos um dos vértices -- novamente, o que quer que isso signifique.
	
	Seria também de esperar que no caso de o conjunto admissível ser limitado e não vazio, existe, necessariamente, solução. E, já agora, que o conjunto admissível é convexo.
	
	Todos estes factos aleatórios serão provados, e mais, e alguns deles provar-se-ão úteis na procura de formas de resolver problemas de otimização.
	
	\subsection{Factos introdutórios}
	
	Recorde-se o leitor da definição de interior, fronteira e exterior de conjunto, e da definição de conjunto aberto e fechado.
	
	\begin{prop}
		Fixo um pol $P$, $X_P$ e $S_P$ são fechados.
	\end{prop}
	
	\begin{proof}
	Primeiramente, mostre-se que $X_P$ é fechado.
	
	Recorde-se que interseções finitas de fechados são fechadas. De seguida, note-se que, dado um pol $P$ (que podemos supor sem perda de generalidade estar na forma semicanónica. Assim, o conjunto admissível é o conjunto de $x \in \R^n$ que satisfazem as condições
	
	\[a_i x \leq b_i \text{ para $i = 1, 2, \cdots, m$}\]
	
	Onde os $a_i$ são vetores-linha, e os $b_i$ são escalares.
	
	Ou seja, $X_P$ é a interseção
	
	\[\bigcap_{i=1}^m \{\,x \in \R^n \mid a_i x \leq b_i \,\}\]
	
	Repare-se agora que $\{\,x \in \R^n \mid a_i x \leq b_i \,\}$ é a pré-imagem sob uma função linear (e então contínua) de um conjunto fechado ($]-\infty, b_i]$), pelo que usando Cálculo II se conclui que estes conjuntos são todos fechados, pelo que $X_P$ é fechado.
	
	Para mostrar que $S_P$ é fechado, repare-se que há dois casos: ou $S_P$ é vazio, e então fechado; ou contém pelo menos um elemento $s$. Neste último caso, $S_P = X_P \cap \{\,x \mid c^T x \geq c^T s\,\}$ por definição, e isto é a interseção de dois conjuntos fechados, e então fechado.
	\end{proof}
	
	Recorde-se do teorema de Weierstrass, que deverá ter dado em Cálculo II: uma função contínua definida num subconjunto fechado, limitado e não-vazio de $\R^n$ (compacto não-vazio) tem máximo e mínimo. Assim sendo, conseguimos facilmente o seguinte corolário:
	
	\begin{prop}
	Fixo um problema de otimização linear $P$, se $X_P$ é limitado e não-vazio, $S_P$ é não-vazio.
	\end{prop}
	\begin{proof}
	Basta reparar que se $X_P$ é limitado, por ser fechado (ver acima), é compacto. Assim sendo, como a função objetivo é linear e então contínua, tem valor máximo algures em $X_P$. Os valores maximizantes formam o conjunto solução.
	\end{proof}
	
	Mais um detalhe topológico simples, antes de avançarmos para coisas mais específicas: denomine-se o interior de um conjunto $X$ por $\interior X$, e a sua fronteira por $\partial X$.
	
	\begin{prop}
	Se $P$ é um pol de função objetivo não-nula, $S_P \subseteq \partial X_P$.
	\end{prop}
	
	\begin{proof}
	Basta reparar que, se $x \in \interior  X_P$, então há uma vizinhança $\varepsilon$ de $x$ contida em $X_P$. A ideia é que, então, se $c^T$ não é o vetor nulo, podemos `avançar um bocadinho na direção de $c^T$'. Em termos mais precisos, o vetor $y = x + \frac \varepsilon 2 \frac{c^T}{\lvert c^T \rvert}$ pertence a $X_P$, por distar de $x$ menos de $\varepsilon$. Mas este vetor é `melhor' que $x$, pois $cy = cx + c \frac \varepsilon 2 \frac {c^T}{\lvert c^T \rvert} = cx + \frac{\varepsilon |c^T|} 2 > cx$. Logo, nenhum $x \in \interior X_P$ pertence a $S_P$, donde $S_P$ tem que estar todo contido na fronteira.
	\end{proof}
	
	Só um último detalhe, antes de avançarmos para outras pastagens: uma caraterização do interior de $X_P$ para um pol $P$, que se assume spdg estar na forma semicanónica.
	
	\begin{prop}
	Seja $P$ o pol
	
	\[
	P =
	\begin{cases}
	\max\limits_x cx\\
	Ax \leq b
	\end{cases}
	\]
	
	e assuma-se que nenhuma linha da matriz $A$ é composta somente de zeros. (Isto é denotado por \emph{$A$ é uma matriz própria})
	
	Então, $\interior X_P$ é o conjunto de $x > 0$ tal que $Ax < b$. Consequentemente, $\partial X_P$ é o conjunto de $x \geq 0$ tal que $Ax \leq b$ e há pelo menos uma igualdade.
	\end{prop}
	
	\begin{proof}
	Relembre-se da seguinte identidade: $\interior (A \cap B) = \interior A \cap \interior B$. Mais geralmente, o interior de uma interseção finita é a interceção dos interiores.
	
	Como tal, sendo que
	
	\[X_P = \bigcap_{i=1}^m \{\,x \in \R^n \mid a_i x \leq b_i \,\}\]
	
	onde $a_i$ representa a $i$-ésima linha da matriz $A$,
	
	Para determinar $\interior X_P$ basta determinar os interiores destes conjuntos. Afirmamos que o interior do $i$-ésimo conjunto é
	
	\[\interior \{\,x \in \R^n \mid a_i x \leq b_i \,\} = \{\,x \in \R^n \mid a_i x < b_i \,\}\]
	
	Para justificar a inclusão $\supseteq$, note-se que o lado direito é um conjunto aberto contido em $\{\,x \in \R^n \mid a_i x \leq b_i \,\}$, e então contido no seu interior.
	
	Para justificar a inclusão $\subseteq$, suponha-se que $x$ pertence ao lado esquerdo. Mostrar-se-á que pertence ao lado direito.
	
	Por pertencer ao interior daquele conjunto, sabemos que, para $y$ numa vizinhança $\varepsilon$ de $x$, $a_i y \leq b_i$. Considere-se, então, $y = x + \frac \varepsilon 2 \frac{a_i^T}{\lvert a_i^T \rvert}$. Por hipótese, $a_i y \leq b_i$. Mas tem-se também $a_i y = a_i x + a_i \frac \varepsilon 2 \frac{a_i^T}{\lvert a_i^T \rvert} = a_i x + \frac {\varepsilon \lvert a_i^T \rvert} 2 > a_i x$, donde se conclui $a_i x < a_i y \leq b_i$. Logo, $x$ pertence ao lado direito.
	\end{proof}
	
	\section{Álgebra Linear}
	
	\subsection{Intuição geométrica}
	
	Queremos agora investigar a intuição que o conjunto solução contém sempre um vértice, no caso de este ser não-vazio.
	
	Para este estudo, no entanto, é útil considerar, em vez do pol na forma canónica, o pol na forma padrão, visto que é mais fácil caraterizar o significado de `vértice' neste contexto.
	
	Considere-se o seguinte pol:
	
	\[
	P =
	\begin{cases}
	\min\limits_{(x,y,z)} 2x + y + 5z\\
	3x + 6y + 4z = 24\\
	x, y, z \geq 0
	\end{cases}
	\]
	
	O desenho do conjunto admissível é o seguinte:
	
	\resizebox{\columnwidth}{!}{
	\begin{tikzpicture}
	\draw[->] (0, 0) -- (10, 0) node[anchor=south] {x};
	\draw[->] (0, 0) -- (-5, -5) node[anchor=north west] {y};
	\draw[->] (0, 0) -- (0, 7) node[anchor=west] {z};
	\filldraw[color=red, pattern color=blue, very thick, pattern = north east lines] (8,0) node[anchor=north, color=black]{(8,0,0)} -- (-4,-4) node[anchor=east, color=black]{(0,4,0)} -- (0,6) node[anchor=west, color=black]{(0,0,6)} -- (8,0);
	\end{tikzpicture}
	}
	
	Temos agora de definir o conceito de vértices no contexto de otimização linear, mas a figura deve tornar evidente que se tratam dos pontos com o maior número de coordenadas 0.
	
	Vamos primeiro tentar definir o que queremos dizer por `vértice'.
	
	\subsection{Geometria}
	
	Dado o nosso contexto, é possível atribuir significado aos termos `vértice', `aresta', `lado' e assim por diante. No entanto, por simplicidade, vamos apenas definir o significado de vértice.
	
	Relembre-se da definição de plano em Álgebra Linear. Um plano em $\R^n$ é um conjunto da forma $U = a + V$, em que $a \in \R^n$ e $V$ é um subespaço vetorial de $\R^n$. Dizemos que $U$ é um plano de dimensão $m$ se $V$ tiver dimensão $m$.
	
	Um \emph{hiperplano em $\R^n$} é um plano de dimensão $n-1$.
	
	Relembre-se que os planos podem também ser caraterizados como conjuntos-solução de equações lineares $Ax = b$ a $n$ dimensões. A dimensão do plano é, então, a dimensão do núcleo da matriz $A$. Relembre-se também que, se um plano $a + V$ é definido por $Ax = b$, as colunas de $A^T$ geram o espaço $V^\perp$.
	
	Uma particularidade dos hiperplanos é que o seu espaço ortogonal tem dimensão 1. Assim sendo, um hiperplano pode sempre ser expresso da forma $w \cdot x = b$, para algum vetor não-nulo $w$ e escalar $b$. Para mais, quaisquer duas expressões que representem o mesmo hiperplano têm vetores colineares.
	
	Isso permite fazer a seguinte definição:
	
	\begin{definition}
	Dado um plano $H$ definido por $w \cdot x = b$, dizemos que este separa o espaço em dois semiespaços: $H^+$, definido por $w \cdot x > b$ e $H^-$, definido por $w \cdot x < b$.
	
	Devido à observação anterior, esta definição não depende do $w$ escolhido, a menos de sinal. Ou seja, $H^+$ e $H^-$ não dependem de $w$, exceto para distinguir qual é qual.
	
	Diz-se que $H$ \emph{separa o espaço em $H^+$ e $H^-$}.
	\end{definition}
	
	Estamos agora prontos para caraterizar o que queremos dizer com `vértice'.
	
	\begin{definition}
	Dado um conjunto $S \subseteq \R^n$, dizemos que $v \in S$ é um \emph{vértice exterior de $S$} se existem $w \in \R^n$ e $b \in \R$ tal que $w \cdot v = b$ e $w \cdot x < b$ para todo $x \in S \setminus \{v\}$.
	\end{definition}
	
	Esta definição é equivalente à seguinte:
	
	\begin{definition*}
	Nas mesmas condições, dizemos que $v \in S$ é vértice exterior se existe um hiperplano que separa $v$ do resto do conjunto. Ou seja, se existe um hiperplano $H$ que separe o espaço em $H^+$ e $H^-$ tal que $v \in H$ e $S \setminus \{v\} \subseteq H^-$.
	\end{definition*}
	
	Repare-se que é óbvio que esta segunda definição implica a primeira, devido à forma como se definiram os subespaços. No entanto, a outra implicação não é tão óbvia, pois repare-se que não é exigido do vetor $w$ que seja diferente de zero. Este detalhe tem pequena relevância, mas permitir-nos-á simplificar uma prova algures no futuro. Claro que se o vetor $w$ for $0$, o conjunto $S$ contém apenas o ponto $v$, e portanto a segunda definição aplica-se com qualquer hiperplano que passe por $v$. Isto mostra que as duas definições são equivalentes.
	
	Para exemplificar esta definição, considere-se o conjunto:
	
	\[S = \{\, (x,y) \mid x,y \geq 0, y \leq 2+x, y \geq -2 + 2x\,\}\]
	
	Em baixo, está uma representação da afirmação: ``o ponto $(4, 6)$ é vértice de $S$, pois pondo $w = (1,1)$ e $b = 10$, tem-se $w \cdot (4,6) = b$ e para todos os pontos $(x,y)$ em $S \setminus \{(4,6)\}$ tem-se $w \cdot (x,y) < b$''.
	
	\begin{tikzpicture}
	\draw[->] (-2, 0) -- (6, 0) node[anchor=south] {x};
	\draw[->] (0, -1) -- (0, 8) node[anchor=west] {y};
	\filldraw[color=blue, fill=blue!80, very thick] (0,0) -- (0,2) -- (4,6) -- (1,0) -- (0,0);
	
	\draw[->, color=purple, very thick] (4,6) -- (5,7) node[anchor=west, color=black]{$w = (1,1)$};
	
	\draw[color=green, very thick] (2.5, 7.5) -- (5.5, 4.5);
	\fill[pattern=north east lines, pattern color=green] (-1.5, -0.5) -- (-1.5, 7.5) -- (2.5, 7.5) -- (5.5, 4.5) -- (5.5, -0.5);
	\end{tikzpicture}
	
	A reta verde representa o hiperplano (reta) $H$ definido por ${w \cdot (x,y) = 10}$, e a área pintada a verde representa o semiespaço $H^-$ definido por ${w \cdot (x,y) < 10}$. Repare-se que todo o conjunto $S$, exceto o ponto $(4,6)$, está contido neste semiespaço.
	
	\subsection{Vértices de conjuntos admissíveis}
	
	Estamos agora prontos para tentar caraterizar os vértices de conjuntos admissíveis do pol na forma padrão. A seguinte proposição ser-nos-á útil:
	
	\begin{prop} \label{vertexvpmh}
	Seja $S$ um conjunto em $\R^n$. As seguintes quatro afirmações não podem ser todas verdade ao mesmo tempo:
	
	\begin{itemize}
	\item $v$ é vértice de $S$
	\item $v + h \in S$
	\item $v - h \in S$
	\item $h \neq 0$
	\end{itemize}
	\end{prop}
	
	\begin{proof}
	Para mostrar isto, vamos supor que três destes são verdade, e mostramos que o quarto tem que ser falso. Nomeadamente, suponhamos que $v$ é vértice de $S$, $v+h \in S$ e $h \neq 0$.
	
	Assim sendo, visto que $v$ é vértice, existem $w$ e $b$ tal que $w \cdot v = b$ e $w \cdot x < b$ para todo $x \in S$ diferente de $v$. Em particular, $w \cdot (v + h) < b$. Mas então $w \cdot h < 0$, donde $w \cdot (v-h) > b$, e então concluímos que $v-h$ não pode pertencer a $S$.
	\end{proof}
	
	Na caraterização de vértices é-nos útil falar das coordenadas positivas de vetores, e das colunas correspondentes da matriz $A$. (A razão para isto será evidente em breve.) Assim sendo, adotamos as seguintes notações:
	
	\begin{notacao}
	No nosso contexto, considere-se o seguinte pol a $n$ dimensões:
	
	\[
	\begin{cases}
	\min\limits_x cx\\
	Ax = b\\
	x \geq 0
	\end{cases}
	\]
	
	Em que $x$ é um vetor em $\R^n$, $A$ é uma matriz $m \times n$ e $b$ é um vetor em $\R^m$. Por conveniência, seja $N$ o conjunto $\{1,2,\cdots,n\}$.
	
	Fixo um vetor $x$:
	
	Usamos $P_x$ para denotar o conjunto de índices $i$ tal que $x_i > 0$.
	
	Dado um conjunto de índices $B \subseteq N$, $x_B$ representa o vetor em $\R^{\#B}$ obtido de $x$ resultante apenas das coordenadas em $B$. Por exemplo, se $B$ é o conjunto $\{2, 5, 9\}$, $x_B$ seria o vetor em $\R^3$ dado por $(x_2, x_5, x_9)$. Análogamente, $A_B$ denota a submatriz de $A$ considerando apenas as colunas com índices em $B$.
	\end{notacao}
	
	Antes de fazer a prova `a sério', será feito um esboço de prova para suportar a afirmação feita há bocado: ``os vértices tratam-se dos pontos com o maior número de coordenadas 0''. Este esboço de prova vai-nos servir para motivar a caraterização (completa) dos vértices de $X_P$, para $P$ na forma padrão.
	
	Seja $k$ a caraterística da matriz $A$. Recorde-se que a caraterística de uma matriz equivale à dimensão do seu espaço de colunas. Afirmamos que qualquer vértice de $X_P$ tem no máximo $k$ coordenadas maiores que zero.
	
	Para justificar isto, considere-se um $x \in X_P$ tal que $\# P_x > k$. Queremos justificar que este não pode ser um vértice, e vamos fazer isso usando a proposição \ref{vertexvpmh}.
	
	Nomeadamente, vamos arranjar um $h$ diferente de zero tal que $x+h$ e $x-h$ pertencem a $X_P$, o que, devido a esta proposição, implica que $x$ não é vértice de $X_P$. Para arranjar este $h$, repare-se nas condições que ele tem que obedecer:
	
	\begin{itemize}
	\item $h \neq 0$
	\item $Ah = 0$
	\item $x \pm h \geq 0$
	\end{itemize}
	
	Em particular, a última condição diz-nos algo importante: para todo $i$, se $x_i = 0$, $h_i$ tem que ser $0$. Caso contrário, um daqueles dois vetores teria a $i$-ésima casa negativa.
	
	Assim sendo, isso limita as nossas escolhas de $h$. Este tem necessariamente as casas em $N \setminus P_x$ igual a zero, pelo que resta definir $h_{P_x}$.
	
	Repare-se, agora, que, sob estas condições, $Ah = A_{P_x} h_{P_x}$, e visto que $\# P_x$ é maior do que a caraterística de $A$, a matriz $A_{P_x}$ é singular, e então existe um $h$ diferente de zero (chamemos-lhe $\tilde h$) que satisfaz $A \tilde h = 0$ e $\tilde h_i$ é igual a zero para $i \not \in P_x$. Falta apenas assegurar que $x \pm \tilde h \geq 0$, mas isto pode não ser verdade.
	
	Para o passo final, repare-se que estas duas condições ($A \tilde h = 0, h \neq 0$) não deixam de ser verdade se multiplicarmos $\tilde h$ por um escalar $t$ diferente de zero.
	
	A ideia é a seguinte: para todo $i \in N$ tem-se um dos dois casos: ou $x_i = 0$ e então, como $\tilde h_i = 0$, $x_i + t \tilde h_i \geq 0$ independentemente de $t$, ou $x_i > 0$ e então $x_i + t \tilde h_i \geq 0$ para qualquer $t$ de módulo pequeno o suficiente. Considere-se o $m$ mínimo destes módulos, e tem-se que, para $t$ de módulo menor que $m$, $x \pm t \tilde h_i \geq 0$. Pondo $h = t \tilde h_i$, obtemos o resultado desejado: $x \pm h \in X_P$ e $h \neq 0$. Assim sendo, $x$ não pode ser vértice de $X_P$.
	
	Repare-se que neste esboço de prova, a condução que $\#P_x > k$ foi apenas usada para justificar que as colunas de $A_{P_x}$ são linearmente dependentes. Acontece que esta nova condição, mais fraca, não é só suficiente como necessária. Provamos, agora, a
	
	\begin{prop}
	Seja $P$ o seguinte pol na forma padrão:
	
	\[
	\begin{cases}
	\min\limits_x cx\\
	Ax = b\\
	x \geq 0
	\end{cases}
	\]
	
	Então, $x \in X_P$ é vértice de $X_P$ sse $A_{P_x}$ tem as suas colunas todas linearmente independentes.
	\end{prop}
	
	\begin{proof}
	Primeiro, a parte que já está feita: se as colunas de $A_{P_x}$ não forem linearmente independentes, $x$ não é vértice. ($\rightarrow$) Construa-se $\tilde h \neq 0$ tal que $\tilde h_i = 0$ para $i \not \in P_x$, e $\tilde h_{P_x}$ é tal que $A_{P_x} \tilde h_{P_x} = 0$. Isto pode ser feito porque, por hipótese, a solução $A_{P_x} = 0$ tem soluções não-triviais.
	
	Considere-se a função $f(t) = x + t \tilde h$. Pretendemos arranjar $t$ tal que \allowbreak ${f(t), f(-t) \geq 0}$, pois assim ter-se-ia que $x \pm t \tilde h \in X_P$ com $t \tilde h \neq 0$, o que justifica $x$ não ser vértice conforme a prop \ref{vertexvpmh}.
	
	Para este objetivo, decomponha-se esta função em $f_{P_x}(t)$ e $f_{N \setminus P_x}(t)$. A segunda é sempre $\geq 0$ (por ser sempre igual a zero), pelo que basta examinar a primeira.
	
	Mas repare-se que, como $f$ é contínua (soma de uma constante com uma função linear), a sua restrição às coordenadas de $P_x$ também. Para mais, note-se que $f_{P_x}(0) > 0$, pelo que, por continuidade, existe uma vizinhança $\varepsilon$ de zero onde $f_{P_x}(t) > 0$, e então $f(t) \geq 0$. Escolhendo um $t$ diferente de zero nesta vizinhança, por exemplo, $t = \varepsilon/2$, temos $f(t), f(-t) > 0$, como desejado.
	
	A partir daqui, aplicando o raciocínio anterior, usando $h = t \tilde h$, concluimos que $x$ não é vértice de $X_P$.
	
	Agora, a segunda parte ($\leftarrow$): mostrar que se $x \in X_P$ e as colunas de $A_{P_x}$ são linearmente independentes, então $x$ é vértice.
	
	Para fazer isto, considere-se o vetor $w$ definido como
	
	\[w_i = (\text{$0$ se $i \in P_x$, $-1$ caso contrário})\]
	
	E o escalar $0$.
	
	Então, claramente temos $w \cdot x = 0$.
	
	Agora, mostre-se que, para todo $y \in X_P \setminus \{x\}$ se tem $w \cdot y < 0$. Isto é claramente equivalente (dado que $y \in X_P$ e então $y \geq 0$) a mostrar que $y_i \neq 0$ para algum $i \not \in P_x$. Ou seja, que $P_y \not \subseteq P_x$.
	
	Para este efeito, suponha-se que $y \in X_P$ e $P_y \subseteq P_x$. Vamos mostrar que $y = x$.
	
	Se $y \in X_P$, temos $Ay = b$. Mas como $P_y \subseteq P_x$, temos que $Ay = A_{P_x} y_{P_x} = b$. Mas como as colunas de $A_{P_x}$ são linearmente independentes, a solução do sistema $A_{P_x} v = b$, se existir (neste caso existe), é única. Assim sendo, $y_{P_x} = x_{P_x}$, e como as outras coordenadas de $y$ são todas 0, como as de $x$, temos $y = x$, como queriamos demonstrar.
	
	Logo, para $y \in X_P \setminus \{x\}$ tem-se sempre $w \cdot y < 0$, como se pretendia demonstrar, e então $x$ é vértice de $X_P$.
	\end{proof}
	
	Estamos agora em condições de justificar o seguinte:
	
	\begin{prop}
	Seja $P$ o pol
	
	\[
	\begin{cases}
	\min\limits_x cx\\
	Ax = b\\
	x \geq 0
	\end{cases}
	\]
	
	Se $S_P$ é não-vazio, contém pelo menos um vértice de $X_P$.
	\end{prop}
	
	\begin{proof}
	Suponha-se que $S_P$ é não-vazio. A função $x \mapsto \#P_x$ vai de $S_P$ para $\N_0$, pelo que é minimizada nalgum ponto. Chamemos-lhe $z$. Mostraremos que $z$ é vértice de $X_P$.
	
	Para fazer isto, usaremos a caraterização feita há pouco: mostraremos que as colunas de $A_{P_z}$ são linearmente independentes, o que justifica que $z$ é vértice.
	
	A prova faz-se por contrarecíproco. Para mostrar que um $z$ que minimize $\#P_z$ é vértice, supõe-se que não é vértice e mostra-se que não minimiza $\#P_z$.
	
	Suponha-se, então, que $z$ não é vértice, ou seja, que as colunas de $A_{P_z}$ são linearmente dependentes. Vamos arranjar um elemento $\tilde z$ de $S_P$ tal que $\#P_{\tilde z} < \#P_z$
	
	Sabendo que as colunas de $A_{P_z}$ são linearmente dependentes, existe solução não-trivial do sistema $A_{P_z} x = 0$. Seja $\tilde h$ tal que $\tilde h_i = 0$ para $i \not \in P_z$, e $\tilde h_{P_z}$ é solução não-trivial de $A_{P_z} \tilde h_{P_z} = 0$ Suponha-se, sem perda de generalidade, que $\tilde h$ tem pelo menos uma coordenada positiva. Podemos fazer isto, pois, se não for o caso, substitua-se $\tilde h$ por $-\tilde h$.
	
	Repare-se, primeiro, no seguinte detalhe importante: $c \tilde h = 0$. Isto é pois, usando o mesmo raciocínio que na prova anterior, existe $t \neq 0$ tal que $z \pm t \tilde h \in X_P$. Estes dois vetores têm pontuação $cz \pm t c \tilde h$, pelo que, se $c \tilde h$ não fosse zero, um destes teria pontuação melhor do que $z$. Isto contraria a hipótese de $z$ ser solução.
	
	Assim sendo, para todo o $t$ tal que $z - t \tilde h$ é admissível, este é solução.
	
	A ideia é, agora, escolher $t$ de modo a que $z - t \tilde h$ tenha menos coordenadas positivas do que $z$, mas continue admissível. Para fazer isto, efetivamente, o que se faz é `subir o valor de $t$ continuamente até uma das coordenadas ser zero'. (Isto vai ser formalizado mais tarde.) Essa coordenada ficará zero, e nenhuma coordenada que fosse antes zero deixou de o ser, pelo que o número de coordenadas positivas diminuiu... E o vetor continua a ser admissível porque a condição $Ax = b$ nunca deixa de ser verdade, e a condição $x \geq 0$ continua a ser verdade porque não deixamos nenhuma coordenada ir para os negativos.
	
	Em termos formais: considere-se o conjunto $T = \{\,\frac {z_i} {\tilde h_i} \mid i \in P_{\tilde h}\,\}$. Este conjunto é não-vazio (assumimos que $\tilde h$ tinha pelo menos uma coordenada positiva), finito e todos os seus elementos são positivos (pois $P_{\tilde h} \subseteq P_z$). Considere-se, então, o seu mínimo, $\alpha$. Vamos ver que o vetor $\tilde z = z - \alpha \tilde h$ é admissível e tem menos coordenadas positivas do que $z$.
	
	Como já vimos, obedece à condição $A \tilde z = b$, pelo que basta certificarmo-nos que $\tilde z \geq 0$. Fazemos isto coordenada a coordenada.
	
	Examine-se $\tilde z_i = z_i - \alpha \tilde h_i$. Se $i \not \in P_{\tilde h}$, isto é claramente $\geq z_i \geq 0$. Pelo outro lado, se $i \in P_{\tilde h}$, temos que $\alpha \leq z_i/\tilde h_i$, e então $-\alpha \geq -z_i/\tilde h_i$, donde $z_i - \alpha \tilde h_i \geq z_i - \frac {z_i}{\tilde h_i} h_i = 0$. Como se pretendia demonstrar.
	
	Assim sendo, o vetor $\tilde z$ é admissível. Já vimos que é solução. E, finalmente, mostramos que tem menos coordenadas positivas do que $z$.
	
	Repare-se que para todo $i$, se $z_i = 0$ então $\tilde h_i = 0$, pelo que nenhumas coordenadas positivas `novas' aparecem. Pelo outro lado, pelo menos uma coordenada positiva fica nula. Nomeadamente, visto que $\alpha$ é o valor mínimo do conjunto $\{\,\frac {z_i} {\tilde h_i} \mid i \in P_{\tilde h}\,\}$, é da forma $\alpha = z_j / \tilde h_j$ para algum $j$. Vamos examinar a coordenada $j$ de $\tilde z$.
	
	Sabemos que $z_j > 0$. Mas também temos que $\tilde z_j = 0$, pois $\tilde z_j = z_j - \alpha \tilde h_j = z_j - \frac {z_j}{\tilde h_j} \tilde h_j = 0$. Isto mostra, então, que $\tilde z$ tem pelo menos uma coordenada positiva a menos que $z$, o que mostra que $\tilde z$ é o elemento de $S_P$ que pretendiamos arranjar. Isto conclui a nossa prova.
	\end{proof}
	
	\subsection{O método dos vetores básicos}
	
	Estamos perto de ter um método primitivo de resolução de problemas de otimização linear. Dado um pol, podemos sempre pô-lo na forma padrão. Lá estando, podemos examinar os vértices.
	
	A ideia é que, em principio (vamos provar isto) haverá um número finito de vértices. Se o problema tiver solução, o vértice de melhor pontuação será solução. Daí em diante, é preciso verificar que, de facto, $S_P$ é não-vazio. Os detalhes de tal verificação serão feitos mais tarde: por agora, arranjaremos um método de encontrar os vértices de $X_P$ para $P$ na forma padrão.
	
	A ideia é a seguinte: considere-se um conjunto de índices $B \subseteq N$ tal que $A_B$ tem colunas linearmente independentes. Então, a equação $A_B x = b$ tem zero ou uma soluções. Isto dá-nos o primeiro método, mais primitivo possível, de encontrar os vértices de $X_P$:

	\begin{lstlisting}[mathescape=true, keepspaces=true]
Para todo $B \subseteq N$:
  Se as colunas de $A_B$ são linearmente independentes:
    Resolver o sistema $A_B x = b$.
    Se este tiver solução $x$:
      Se $x \geq 0$:
        Definir $v \in \R^n$ de modo a que $v_B = x$ e $v_{N\setminus B} = 0$
        Adicionar $v$ à lista de vértices.
	\end{lstlisting}
	
	Este método funciona, mas tem um enorme problema: se estamos em dimensão $n$, temos $2^n$ sistemas para resolver. Procuramos, então, uma forma mais computacionalmente eficiente de fazer as coisas.
	
	Lembre-se da Álgebra Linear que qualquer conjunto linearmente independente de vetores pode ser extendido a uma base. Isso é refletido na seguinte proposição:
	
	\begin{prop}
	Dada uma matriz $A$ $m \times n$ e um conjunto de índices $B \subseteq N$, dizemos que $B$ é uma \emph{base de índices} se as colunas de $A_B$ formam uma base de $\im A$.
	
	Se $B \subseteq N$ é tal que as colunas de $A_B$ são linearmente independentes, existe uma base de índices $B' \supseteq B$.
	\end{prop}
	
	\begin{proof}
	Esta prova será feita mostrando que, se $B$ é tal que $A_B$ tem colunas linearmente independentes mas $B$ não é base de índices, é possível adicionar um elemento $b$ a $B$ tal que as colunas de $B \cup \{b\}$ continuem a ser linearmente independentes. Feito isto, o algoritmo para extender $B$ a uma base passa a ser repetir este processo até se ter um conjunto de tamanho igual à caraterística de $A$.
	
	Assim sendo, basta mostrar este passo. Suponha-se que $B$ está sob estas condições, mas não é uma base de índices. Então, tem de existir um vetor-coluna de $A$ que não está em $\im A_B$. Isto pois, caso contrário, $\im A = \im A_B$, e $B$ seria uma base de índices. Assim sendo, escolha-se uma tal coluna, e seja $b$ o seu índice. As colunas de $A_{B \cup \{b\}}$ são linearmente independentes, como o leitor poderá facilmente verificar, o que termina a nossa prova.
	\end{proof}
	
	Dizemos que um vetor $x$ é básico (no contexto de um pol $P$ na forma padrão) se $P_x$ está contido numa base de índices $B$.
	
	O que a proposição anterior mostra é que todo vértice é básico (pegue-se em $P_x$ e extenda-se a uma base). Pelo outro lado, é fácil ver que qualquer vetor básico $x$ tem a propriedade que as colunas de $A_{P_x}$ são linearmente independentes. Temos, então, que os vetores admissíveis básicos e os vértices de $X_P$ são os mesmos.
	
	As bases de índices têm uma propriedade notável: no contexto do pol
	
	\[
	P =
	\begin{cases}
	\min\limits_x cx\\
	Ax = b\\
	x \geq 0
	\end{cases}
	\]
	
	Se a equação $Ax = b$ é impossível, este problema trivialmente não tem solução, visto que o conjunto admissível é vazio. Se isto acontece, dizemos que o pol $P$ é \emph{vazio}.
	
	Pelo outro lado, se $b \in \im A$, dada um base de índices $B$, temos que o sistema de equações $A_B x = b$ tem uma e só uma solução.
	
	Outra observação importante é que as bases de índices de $A$ têm todas tamanho igual à dimensão do espaço de colunas de $A$, ou seja, a sua caraterística.
	
	Assim sendo, considere-se o seguinte (novo) algoritmo.
	
	\begin{lstlisting}[mathescape=true, keepspaces=true]
Verificar se o problema é vazio.
Caso afirmativo, a lista de vértices é vazia.
Caso contrário:
  Seja $k$ a caraterística de $A$.
  Para todo $B \subseteq N$ tal que $\#B = k$:
    Se as colunas de $A_B$ são linearmente independentes:
      Resolver o sistema $A_B x = b$. (Solução existe necessariamente)
      Se $x \geq 0$:
        Definir $v \in \R^n$ de modo a que $v_B = x$ e $v_{N\setminus B} = 0$
        Adicionar $v$ à lista de vértices.
	\end{lstlisting}
	
	Este algoritmo é muito mais económico do que o anterior. Ao passo que no outro havia $2^n$ possibilidades para verificar, aqui é preciso apenas verificar $\binom{n}{k}$ conjuntos, onde $k$ é a caraterística de $A$.
	
	
\end{document}