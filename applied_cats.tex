\documentclass[11pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{mathtools}

\usepackage[thmmarks, amsmath]{ntheorem}

\usepackage{graphicx}
\usepackage[a4paper]{geometry}

\usepackage{diffcoeff}
\diffdef{}{op-symbol=\mathrm{d},op-order-sep=0mu}

\usepackage{cancel}

\usepackage{enumitem}

\setlist[enumerate,1]{label=\alph*)}

\usepackage{tikz-cd}
\usepackage{float}

\usepackage{fontspec}
\setmonofont{Consolas}
\usepackage{listings}
\lstset{basicstyle=\ttfamily\footnotesize,keepspaces=true,tabsize=4,breaklines=true}

\usepackage{hyperref}
\usepackage{xurl}

\title{Category Theory and Haskell}
\author{Duarte Maia}
%\date{}

\theorembodyfont{\upshape}
\theoremseparator{.}
\newtheorem{theorem}{Theorem}
\newtheorem{prop}{Prop}
\renewtheorem*{prop*}{Prop}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}

\newtheorem{joke}{Joke}


\theoremstyle{nonumberplain}

\theoremheaderfont{\itshape}
\theorembodyfont{\upshape}
\theoremseparator{:}
\theoremsymbol{\ensuremath{\blacksquare}}
\newtheorem{proof}{Proof}

\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}

\newcommand{\PP}{\mathbb{P}}
\newcommand{\FF}{\mathcal{F}}

\newcommand{\I}{\mathrm{i}}
\newcommand{\e}{\mathrm{e}}



\DeclareMathOperator{\inte}{int}
\DeclareMathOperator{\codim}{codim}
\newcommand{\grad}{\nabla}

\DeclarePairedDelimiter{\norm}{\lvert}{\rvert}
\DeclarePairedDelimiter{\Norm}{\lVert}{\rVert}


\newcommand{\Hask}{\mathrm{Hask}}
\newcommand{\type}[1]{\mathrm{#1}}
\newcommand{\cat}[1]{\mathrm{#1}}

\newcommand{\blank}{{-}}
\newcommand{\op}{\mathrm{op}}
\newcommand{\id}{\mathrm{id}}
\newcommand{\nto}{\Rightarrow}

\DeclareMathOperator{\Hom}{Hom}

\newcommand*\lsin{\lstinline}
\newcommand*\lsmath[1]{\text{\lstinline|#1|}}


\begin{document}
\maketitle
\pagebreak

\tableofcontents
\pagebreak

\section{Introduction}

In the 1930s, around the same time as Alan Turing published his work on what we nowadays call Turing machines, Alonzo Church published his own formalism for computability, which is called \emph{lambda calculus}. Around this time, it was also proven that these two notions of computability are equivalent. Today, Turing Machines serve as the `public face' of computability: we say that a function is computable if there exists a Turing machine which computes it, for example. As far as computational models go, Turing machines are very slow and unwieldy, but they match cleanly to our mental model of what computation is: a sequence of instructions which can be followed to turn inputs into outputs.

Lambda calculus does not match our intuition as cleanly, being based on substitution of expressions in other expressions, but surprisingly turns out to be much more well behaved in practical terms. Starting with ALGOL around 1960, some programming languages have included some features originating from lambda calculus, such as first-class and anonymous functions, which will be explained later. This is especially noticeable in the programming language Haskell, whose first version was published in 1989, which at its core is simply a lambda calculus evaluator.

Lambda calculus comes in several shapes, but the kind relevant for Haskell is the so-called \emph{typed lambda calculus}. It is typed in the sense that functions have well-defined domains and codomains, which are known at compilation time, and it is a syntatic error to compose functions whose domains and codomains do not match. In other words, Haskell is \emph{strongly typed}, in opposition to C, in which a function which recieves a floating-point number may instead accept an integer. As a consequence, functions in Haskell can be thought of as forming a category, with the domain and codomain of every function being known at compile time and composition only being allowed between functions of compatible types. This is part of the reason why there is such a strong relation between Haskell and category theory.

Besides being strongly typed, another important feature of Haskell is \emph{purity}. Essentially, functions in Haskell are like mathematical functions, in the sense that the same function called with the same input will always return the same output, depending on nothing other than its input and doing nothing other than returning its output. This has wide and far-reaching consequences, both in terms of performance (since there is no implicit input or output, the compiler is free to rearrange the order of computations, or even have them done in parallel) and in terms of code maintainability (when changing pre-existing code, the developer does not need to have knowledge of any other part of the system, making it easier for several developers to work in isolation), but also means that a wide array of tools programmers rely on are inviabilized. For example, simulations of physical objects are often done by defining global variables representing the $x$ and $y$ coordinates of the objects at play, their velocities, etc., and then defining a function which advances the global state in time, calling that function on repeat until the desired amount of time has elapsed:
\begin{lstlisting}
x = 0
y = 0
vx = 0
vy = 0
t = 0
while t < 1000:
	vx, vy = force(x,y)
	x = x + vx
	y = y + vy
print("At t = 1000, the object is at " + (x,y))
\end{lstlisting}

This is impossible in Haskell, as not only can global variables never be changed, but even if they could, this could not be done inside of a function, because functions cannot modify anything, only return an output. This requires that the programmer change their perspective: in this case, instead of defining a function which \emph{modifies} a few global variables, one instead defines a function which receives as input the state of the physical system at time $t$ and returns the state at time $t + \dl t$. 

This is just an example of how purity requires a complete rearrangement of the way one thinks about a problem. In this case a solution was quickly found, but how does one receive input from the user? How does one write to a file? How does one implement an algorithm which relies on mutable state, e.g. marking the nodes of a graph as they are traversed? And most importantly, how does one do all of the above without making the code completely unreadable? This is a place where category theory has found surprising application, by providing powerful abstractions which pick up the slack left by the lack of global state and mutability, and at the same time giving Haskell unprecedented amounts of type safety and generality. In the same way that many seemingly unrelated phenomena were found by category theorists to be expressions of a single phenomenon (for example, adjunctions, limits or colimits), computer theorists have found abstractions which encompass many programming strategies which on the surface seem to be completely distinct.

In this essay, I hope to convince the reader that there are very real mathematical concepts underlying many tools used by in the day-to-day of a Haskell programmer. Furthermore, I also want to get across that these concepts are \emph{useful}, allowing for cleaner and more understandable code, at least so long as the programmer is familiar with the underlying math!

\section{A Crash Course in Haskell}

The following examples can be followed along by executing them in GHCi, the Glasgow Haskell Compiler interpreter, which is the \textit{de facto} implementation of Haskell and can be downloaded here: \url{https://www.haskell.org/downloads/}.

As mentioned in the introduction, Haskell is at its core a lambda calculus evaluator, so we begin by introducing the most basic concept to lambda calculus: the lambda expression.

\subsection{Lambda expressions}

A lambda expression is something similar to the notation $a \mapsto f(a)$ for defining mathematical functions without giving them a name. For example, if I am talking about the function which squares a given number, I may write it as $a \mapsto a^2$. A lambda expression is the same, but it is written using a slightly different notation: the square function would be written as $\lambda a. a^2$. The $\lambda$ marks the beginning of the function, then the argument is given a name, and then an expression is written, in which the only free variable is the one in the argument.

A lambda expression can be applied as a function. For example, we could write $(\lambda a. a^2) 2$; in lambda calculus, function application is written without parentheses. Whenever a lambda expression is juxtaposed to the left of another expression, it should be understood as applying the function defined by the lambda to the argument given by the expression. The above example can be written in Haskell as
\begin{lstlisting}
(\a -> a^2) 2
output: 4
\end{lstlisting}

Note the changes in syntax: the lambda has been replaced by a backslash (it's like a lambda with one leg cut off), and the period has been replaced by an arrow. Also, note that the expression \lstinline|(\a -> a^2)| represents a Haskell function, but it has not been given a name. For this reason, lambda expressions in Haskell are also called \emph{anonymous functions}.

The way that this works under the hood is that the Haskell interpreter is doing something called $\beta$-reduction. Formally, given an expression $f x$ where $f$ is a lambda expression of the form $(\lambda a. M)$, the Haskell interpreter rewrites it as $M[a=x]$, i.e. the expression $M$ where every instance of the variable $a$ has been replaced by $x$. In the above example, \lstinline|(\a -> a^2) 2| is $\beta$-reduced to \lstinline|2^2|, which is then evaluated to 4.

The reason why this idea is so powerful is the following: \emph{in Haskell and in lambda calculus, lambda expressions are first class objects}. What this means is that functions can be stored in variables and given as arguments to other functions. For example, the following lambda expression takes as an argument another lambda expression, and returns its `square'. Mathematically, we would usually write it as $f \mapsto f \circ f$.
\begin{lstlisting}
\f -> (\x -> f (f x))
\end{lstlisting}

\subsection{Variables, Syntactic Sugar and Recursion}

As we have seen, functions in Haskell do not need to be named, but that does not mean that they should not. If we want to use a function more than once, we do not want to rewrite its definition every time we do. Therefore, Haskell lets us name expressions so that we may reuse them later. For example,
\begin{lstlisting}
f = \a -> a^2
x = 2
f x
output: 4
\end{lstlisting}

It is inconvenient to write lambda expressions whenever we want to define a function, so Haskell allows us to define functions using more convenient and slightly more standard notation
\begin{lstlisting}
f a = a^2
\end{lstlisting}
though it should be understood that beneath this expression is a definition via a lambda expression. Haskell contains many of these alternative notations, in which a common but cumbersome expression is given an alternative notation, which is unfurled to its underlying meaning before being passed to the Haskell interpreter. These kind of syntactic expansion rules are collectively referred to as \emph{syntactic sugar}.

Another piece of syntactic sugar without which writing any program would be almost impossible is recursion. As an example, consider the following implementation of the factorial function
\begin{lstlisting}
factorial x = if x == 0 then 1 else x * factorial (x-1)
\end{lstlisting}

This is a valid definition in Haskell, but it is not easy to write it as a lambda expression, because the function is being called inside itself. If we were to naïvely write \lstinline|factorial| as an anonymous lambda expression, we would get
\begin{lstlisting}
\x -> if x == 0 then 1 else x * factorial (x-1),
\end{lstlisting}
which would require us to expand the definition of factorial inside the expression (since it is anonymous we cannot refer to it by name), obtaining a more complex expression which itself has a factorial, and so on. This tricky problem (expressing recursion in lambda calculus) has a solution via the so-called $Y$ combinator, which is a particular lambda expression which can be used to define the factorial as above. Fortunately, as a Haskell programmer we don't need to know about it, because recursive expressions are rewritten using the $Y$ combinator under the hood.

\subsection{Types and Morphisms}

As we said in the introduction, Haskell is strongly typed: every function has a domain and a codomain. This makes Haskell a category\footnote{This is not exactly true; see \cite{haskisnotcat}.}, which is usually called $\Hask$, in which the types (examples: \lstinline|Int|, \lstinline|Char|, \lstinline|String|) are the objects and the functions (lambda expressions) are the morphisms.

That said, what is the domain and codomain of the lambda expression \lstinline|\a -> a^2|? We have evaluated it on the number 2, which is an integer (i.e. of type \lstinline|Int|) and gotten an integer back, but we could also have applied it to the floating-point number 2.1 (of type \lstinline|Float|). We'll get back to this ambiguity later, but to assuage our concerns we can add type annotations explicitly using the following syntax
\begin{lstlisting}
f :: Int -> Int
f a = a^2
\end{lstlisting}

This means that $f$ is a morphism with domain \lstinline|Int| and codomain \lstinline|Int|; mathematically, $f \in \Hask(\type{Int}, \type{Int})$. If we try to evaluate it on a non-integer such as 2.1, we get an error:\footnote{Strictly speaking, this isn't the error you'll actually get. You'll get something like \lstinline|No instance for (Fractional Int) arising from the literal '2.1'|, which is Haskell trying a little harder to make 2.1 into an integer.}
\begin{lstlisting}
f 2.1
<interactive>:1:3: error:
    • Couldn't match expected type 'Int' with actual type 'Float'
    • In the first argument of 'f', namely '2.1'
      In the expression: f 2.1
      In an equation for 'it': it = f 2.1
\end{lstlisting}

Haskell has a wide array of so-called primitive types, like \lstinline|Int|, \lstinline|Char| and so on, but it also allows us to make complex types out of simpler ones. We have already seen one: the humble arrow!

The sequence of characters \lstinline|->| has the role usually taken by $\Hom$ or $C$ in categorical texts: we write \lsin|a -> b| to denote what we would usually call $\Hask(a,b)$. This is in itself a Haskell type. In other words, \emph{the $\Hask$ category has exponential objects}. This is the first taste of categorical language in Haskell.

\subsection{Currying and Composition}

So far, every function we have seen takes only one argument. This is also the case in lambda calculus: every lambda has exactly one variable. It also happens in, for example, the category of $\cat{Sets}$: an arrow $A \to B$ takes as argument exactly one element of $A$. Of course, we emulate our functions taking multiple arguments by using cartesian products: a function $f$ which receives two real numbers and returns another is denoted as
\begin{equation}
f \colon \R \times \R \to \R.
\end{equation}

A similar process can be done in Haskell, so that a function that receives, say, two integers, could have type \lsin|(Int,Int) -> Int|. The comma \lsin|(,)| denotes the categorical product in $\Hask$, and is another example of a type constructor. However, functions receiving multiple arguments are usually handled another way.

The type of a pre-existing function \lsin|f| can be found by writing \lsin|:t f| in GHCi. For example, using \lsin|f = (+)|, i.e. the predefined function which adds two numbers, we get\footnote{Again, the actual output isn't exactly the one shown, as addition is defined more generally than on the integers.}
\begin{lstlisting}
:t (+)
output: (+) :: Int -> Int -> Int
\end{lstlisting}

What gives? This expression will be made clearer if we introduce parentheses. Haskell associates the \lsin|->| operator to the right, so the above expression should be read as
\begin{lstlisting}
(+) :: Int -> (Int -> Int)
\end{lstlisting}

Aha! So what's happening here, in categorical terms, is simply that
\begin{equation}
(+) \in \Hask(\type{Int}, \Hask(\type{Int}, \type{Int})).
\end{equation}

Recall that, in sets, we have an adjunction between the functors
\begin{equation}
F(a) = a \times b, \quad G(c) = \Hom(b,c).
\end{equation}

In particular, $F \dashv G$, and the natural isomorphism given by the adjunction is defined as, say,
\begin{equation}
\begin{aligned}
\phi \colon \Hom(a \times b, c) &\to \Hom(a, \Hom(b,c))\\
f &\mapsto (x \mapsto (y \mapsto f(x,y))).
\end{aligned}
\end{equation}

In Haskell, this isomorphism is called \lsin|curry|. We say functions in Haskell are \emph{curried}. The nomenclature is in honor of Haskell Curry, an important logician. We can inspect the type of this function in GHCi:
\begin{lstlisting}
:t curry
output: curry :: ((a, b) -> c) -> a -> b -> c
\end{lstlisting}

This matches our expectation, but remember that parentheses are implicit on the right-hand side. Note the use of the so-called \emph{type variables}: any `type' whose name is a lower-case letter denotes a type variable, and may be substituted by any type. Therefore, \lsin|curry| isn't a function in and of itself: it is a family of functions, parametrized by three objects in $\Hask$: $a$, $b$ and $c$. This is similar to how the usual isomorphism in adjunctions is parametrized by a pair of objects, as in $\varphi_{xy}$. In Haskell, the parametrization is implicit: the compiler keeps the definition as ambiguous as possible at every moment, and deduces what $a$, $b$ and $c$ should be from context when necessary.

Surprisingly, currying (and its inverse, \lsin|uncurry|) isn't used very often. Functions are almost always left curried, because this allows for a technique called partial application. We won't go into details, but for example expressions such as \lsin|(\a -> f 2 a)| are usually shortened to \lsin|(f 2)|.

Finally, let's talk about the most important categorical concept: function composition. It is a very surprising fact that almost no modern languages have a composition operator, but Haskell does. It is denoted with a single dot, as in \lsin|f . g|, as it is the closest a standard keyboard gets to the usual notation of composition, except perhaps for the hideous notation \lsin|f o g|. Unsurprisingly, the composition has type given by
\begin{lstlisting}
:t (.)
output: (.) :: (b -> c) -> (a -> b) -> a -> c
\end{lstlisting}

Again, note the usage of type variables: the types written in lower case are a stand in for any type in $\Hask$.


\section{Elementary Categorical Concepts}

\subsection{A Few Limits and Colimits}\label{limcolim}

We have already seen a few examples of categorical concepts cropping up in Haskell. The types and morphisms form a category called $\Hask$ (kind of, see \cite{haskisnotcat}), the arrow operator \lsin|->| is the categorical exponent, the comma operator \lsin|(,)| is the categorical product, and these two functors (though we haven't realized them as functors yet) are adjoint to each other, with the bijection between hom-sets being given by \lsin|curry|.

Another common categorical construction is the coproduct. In Haskell, this is realized by using the \lsin|Either| type constructor. The coproduct of two types $a$ and $b$ is denoted \lsin|Either a b|, and the coproduct diagram is the following, with the inclusions being denoted by \lsin|Left| and \lsin|Right|.
\begin{equation}
\begin{tikzcd}[column sep = large]
\lsmath{a} \arrow[r, "\lsmath{Left}"] & \lsmath{Either a b} & \lsmath{b} \arrow[l, "\lsmath{Right}"']
\end{tikzcd}
\end{equation}

We also present the product diagram of \lsin|(a,b)|.
\begin{equation}
\begin{tikzcd}[column sep=large]
\lsmath{a} & \lsmath{(a,b)} \arrow[l, "\lsmath{fst}"'] \arrow[r, "\lsmath{snd}"] & \lsmath{b} 
\end{tikzcd}
\end{equation}

As an example, \lsin|Either| is often used for error handling: a \lsin|Right| value represents a successful computation, while \lsin|Left| represents an error. For example, we can define a safe division:
\begin{lstlisting}
safediv :: Rational -> Rational -> Either String Rational
safediv x y = if y == 0 then Left "Error! Division by zero."
              else Right (x/y)
              
safediv 2 3
output: Right (2 % 3)

safediv 2 0
output: Left "Error! Division by zero."
\end{lstlisting}

Note the Haskell notation for fractions: \lsin|2 % 3| means the rational number $2/3$.

We may define a function on \lsin|Either a b| using \emph{pattern matching}. In categorical terms, if $i_1$ and $i_2$ are the inclusions in the coproduct, we define $f \colon a \amalg b \to c$ by defining $f(i_1(x))$ and $f(i_2(x))$.

\begin{lstlisting}
safeadd1 :: Either String Rational -> Either String Rational
safeadd1 (Right y) = Right (y+1)
safeadd1 (Left err) = Left ("Woah there! You have an error: " ++ err)


safeadd1 (safediv 2 3)
output: Right (5 % 3)

safeadd1 (safediv 2 0)
output: Left "Woah there! You have an error: Error! Division by zero."
\end{lstlisting}

Besides binary products and coproducts, Haskell also has empty (co)products, i.e. a final and an initial object.

The final object is the type \lsin|()|, whose only element is the (unique) zero-uple, also denoted \lsin|()|. The only function \lsin|a -> ()| is defined by \lsin|\x -> ()|.

The initial object is the type \lsin|Void|. There is no object of type \lsin|Void|, and the unique function \lsin|Void -> a| is called \lsin|absurd|.

A particularly simple approach to error handling is to use \lsin|()| as the error type, i.e. considering \lsin|Either () a|, where \lsin|Left ()| gives us the information that an error has occurred, while telling us nothing about what it was. Categorically, this corresponds to taking the coproduct with the final object, i.e. `adding one element'. Haskell actually has another type constructor, called \lsin|Maybe|, which has the same semantics.

Let \lsin|a| be a type. Then, we define \lsin|Maybe a| as the type whose elements are either of the form \lsin|Nothing| or \lsin|Just x|, where \lsin|x :: a| (i.e. $x$ has type $a$). In other words, we have the following coproduct diagram.
\begin{equation}
\begin{tikzcd}[column sep = large]
\lsmath{a} \arrow[r, "\lsmath{Just}"] & \lsmath{Maybe a} & \lsmath{()} \arrow[l, "\lsmath{cn}"']
\end{tikzcd}
\end{equation}
where \lsin|cn = \x -> Nothing|. A function defined on \lsin|Maybe a| is defined using the universal property, similarly to the coproduct.
\begin{lstlisting}
safeadd1m :: Maybe Rational -> Maybe Rational
safeadd1m (Just x) = Just (x+1)
safeadd1m Nothing = Nothing
              
safeadd1m (Just 1)
output: Just (2 % 1)

safeadd1m Nothing
output: Nothing
\end{lstlisting}

\subsection{Functors and Typeclasses}

The type constructor \lsin|Maybe| is a particularly simple example, because it takes one type as an argument and returns another. In other words, it is a map from the objects of $\Hask$ to the objects of $\Hask$. This suggests that it could be made into a functor: all we need is to define an adequate map of type \lsin|(a -> b) -> (Maybe a -> Maybe b)|. This map exists, and it is called \lsin|fmap|.
\begin{lstlisting}
fmap :: (a -> b) -> (Maybe a -> Maybe b)
fmap f (Just x) = Just (f x)
fmap f Nothing = Nothing
\end{lstlisting}

Note the use of currying. It looks like we are defining a function of two arguments (one of type \lsin|a -> b| and the other \lsin|Maybe a|), but thanks to currying, this is the same as a function of one argument which returns a function. It is possible to make a definition that maps closer to our intuitive notion of `a function which takes a function as input and returns another function', but it requires introducing additional syntax. In the interest of keeping this essay short, we will forego this introduction, and keep ourselves in a relatively minimal subset of the language.

Let's go back to the definition of \lsin|fmap|. By inspection, it is easy to deduce the functoriality conditions, i.e. \lsin|fmap (f . g) = fmap f . fmap g| and \lsin|fmap id = id| (where \lsin|id| is the identity map). Furthermore, this agrees with the categorical definition of the functor $(1 \amalg \blank)$ given by taking the coproduct with the final object, so this functor in particular is well represented in Haskell. This is not an isolated occurrence, but to explain how functors are represented generally in Haskell we need to talk about \emph{typeclasses}.

In Haskell, a typeclass corresponds to additional structure that a type or type constructor can have. For example, for a given type \lsin|a|, Haskell may or may not know how to check that two elements of \lsin|a| are equal. It certainly knows how to check that two integers \lsin|Int| are the same, but it cannot check that two functions \lsin|Int -> Int| are the same: this is an undecidable problem! Therefore, the binary function \lsin|==| will be defined on \lsin|Int| but not on \lsin|Int -> Int|.

Let us look at the type of \lsin|==|. If we check its type in GHCi using \lsin|:t|, we obtain
\begin{lstlisting}
:t (==)
output: (==) :: Eq a => a -> a -> Bool
\end{lstlisting}

So what's happening here? As the reader might have already suspected, \lsin|==| is a binary function which returns a boolean value: that's what \lsin|a -> a -> Bool| means. However, we have that additional term \lsin|Eq a =>|. What that is saying is that the type signature that follows only applies when \lsin|a| is a type which satisfies the predicate \lsin|Eq|. We say that such types are \emph{in the typeclass \lsin|Eq|}. Most predefined types are in this typeclass, so the equality operator can be used on \lsin|Int|, \lsin|Float|, \lsin|Char|, etc., but not on \lsin|Int -> Int|.

In short, to each typeclass there are a few functions which need to be defined in order for a type to be part of the typeclass: for a type \lsin|mytype| to be in \lsin|Eq|, we need to define \lsin|(==) :: mytype -> mytype -> Bool|. For it to be in \lsin|Ord| (totally ordered types) we need to define \lsin|(<=) :: mytype -> mytype -> Bool|, the `less than or equal operator'. To tell Haskell that \lsin|Maybe Int| is in the \lsin|Eq| typeclass, we would use the notation
\begin{lstlisting}
instance Eq (Maybe Int) where
    Nothing == Nothing = True
    Nothing == Just x = False
    Just x == Nothing = False
    Just x == Just y = x == y
\end{lstlisting}
to define equality. Fortunately, Haskell has built-in mechanisms to deduce that, for example, if \lsin|a| is in the \lsin|Eq| typeclass, so is \lsin|Maybe a|, so we seldom need to explicitly tell it how to define equality for a given type.

What does this have to do with functors? Well, just like how a type can be in a given typeclass, there are also typeclasses for \emph{type constructors}. In particular, given a type constructor of kind \lsin|* -> *| (this means that it requires one type as argument and returns another type), it may or may not be in the \lsin|Functor| typeclass, which is defined as
\begin{lstlisting}
class Functor f where
	fmap :: (a -> b) -> (f a -> f b)
\end{lstlisting}

In other words, for a type constructor (map from objects in $\Hask$ to objects in $\Hask$) to be in the \lsin|Functor| typeclass, we need to define how it acts on morphisms in $\Hask$. It is not (and can not be) enforced by the language that a given implementation of \lsin|fmap| is functorial, but it is generally understood that one should not implement such a pathological version of \lsin|fmap| which does not satisfy them, especially as the compiler might perform optimizations which rely on these laws.

Now that we know about the \lsin|Functor| typeclass, we may list a few of its instances, most of which are familiar from category theory.
\begin{itemize}
\item We've already seen the \lsin|Maybe| functor, which is the coproduct with a one-element set,
\item More generally, for a given type \lsin|a|, the coproduct with \lsin|a| is a functor. In other words, \lsin|Either a| is in the \lsin|Functor| typeclass. Note that this is \emph{not} to say that \lsin|Either| is in the \lsin|Functor| typeclass: \lsin|Either| itself is, categorically speaking, a \emph{bifunctor} in $\Hask$, and therefore in another typeclass called \lsin|Bifunctor|. Of course, technically speaking, this is just a functor $\Hask \times \Hask \to \Hask$, but everything we do is in the $\Hask$ category, which is why bifunctors get a different treatment. This also means that the \lsin|Functor| typeclass could perhaps be more aptly renamed to \lsin|Endofunctor|.

A few more words on \lsin|Either|: since we see \lsin|Either a| as a functor, the instantiation of \lsin|fmap| to \lsin|Either a b| has the type
\begin{lstlisting}
fmap :: (b -> c) -> (Either a b) -> (Either a c)
\end{lstlisting}

In other words, \lsin|fmap f| will apply \lsin|f| to a \lsin|Right| value, and do nothing to a \lsin|Left| value. This is in keeping with the use of \lsin|Either| as an error handling tool: \lsin|fmap f| means `if we have a valid value, apply \lsin|f|. Otherwise, preserve the error message'.

\item Another important functor in category theory is the hom-functor $\Hom(x, \blank)$. In Haskell, this is denoted as \lsin|(->) x|; it should be read as partially applying the type constructor \lsin|(->)|, which takes two arguments, to \lsin|x|, becoming a type constructor that takes a type \lsin|a| and returns the type \lsin|x -> a|. Applied to morphisms, this functor has a very simple definition
\begin{lstlisting}
fmap :: (a -> b) -> (x -> a) -> (x -> b)
fmap f g = f . g
\end{lstlisting}

If one is in a particularly terse mood, they could simply write \lsin|fmap f = f .| or even \lsin|fmap = (.)|. This style of programming, in which one omits the arguments of a function as much as possible, is called \emph{point-free style}.

The hom-functor as we've described has a contravariant sister: the contravariant hom-functor $\Hom(\blank, x)$. This functor is denoted \lsin|Op x|, and is not a member of the \lsin|Functor| typeclass, being instead in the \lsin|Contravariant| typeclass. The function corresponding to \lsin|fmap| is called \lsin|contramap|.

Finally, we can see \lsin|->| as a functor $\Hask^\op \times \Hask \to \Hask$. The nomenclature used in Haskell for such an object is a \lsin|Profunctor|. This does not completely agree with the mathematical nomenclature: in category theory, a profunctor is a functor $D^\op \times C \to \cat{Sets}$. However, the category of types can be approximated by the category of sets (identify a type with the set of its elements), so the nomenclature \lsin|Profunctor| makes sense.

\item Let \lsin|a| be a type. Then, there exists another type, called \lsin|[a]|, whose elements are \emph{lists of \lsin|a|}. In other words, \lsin|[]| is a map that turns a type into another type.

Now, let \lsin|f :: a -> b|. Then, there is an obvious way to turn a list of \lsin|a| into a list of \lsin|b|: simply apply \lsin|f| to every element of the list. This operation is often referred to in programming circles as \emph{mapping $f$}, and can be found in Python with the syntax \lsin|map(f, list)|, in Mathematica with the syntax \lsin|Map[f, list]| and in Haskell with the syntax \lsin|map f list|. It is easy to check that this operation is functorial, so \lsin|map| is actually one of the most widespread examples of functors in programming. In fact, the notion of functor in Haskell actually began as a generalization of the list functor: see \cite{markjones}\footnote{This article is freely available at \url{https://www.cs.tufts.edu/comp/150GIT/archive/mark-jones/fpca93.pdf}.}, where the concept of typeclass is first introduced precisely for this task. The author also uses \lsin|map| to denote application of a functor to a morphism, which was presumably changed to \lsin|fmap| to avoid name collision.

\item Lots of algorithms in computer science rely on some kind of global state in order to keep track of what has already been done, what has already been visited, etc. In Haskell, this can be done by using the product \lsin|(s,a)|, though we will see less cumbersome methods later on.

To be more precise: let \lsin|s| be a type, which represents the global state necessary for our program. For example, where in an imperative language we would have a variable for an integer and another for a list of characters, we might set \lsin|s = (Int, [Char])|. Then, \lsin|(,) s| corresponds to the functor $(s \times \blank)$; \lsin|(,) s a| is a synonym for \lsin|(s,a)|. An element of type \lsin|(s,a)| can be seen as an element of \lsin|a| together with some `background state' of type \lsin|s|. Mapping a morphism \lsin|f :: a -> b| corresponds to modifying the element of type \lsin|a| without modifying the state, yielding a function \lsin|fmap f :: (s,a) -> (s,b)|.
\end{itemize}

There are many more examples of functors in the Haskell ecosystem. In particular, most types of containers (lists, trees, sets, databases, etc) are functors, as well as type constructors which represent computations, such as \lsin|(->) x|, as well as others we will see shortly, such as \lsin|IO| and random number generation.

\subsection{Natural Transformations and Free Theorems}

Let \lsin|f| and \lsin|g| be two \lsin|Functor|s. Then, a natural transformation between \lsin|f| and \lsin|g| would be a function of type
\begin{lstlisting}
eta :: f a -> g a
\end{lstlisting}
where the only free variable is \lsin|a|. We mentioned before the concept of type variables, where in the signature of a function we write lower-case letters instead of types, and the compiler will fill those letters in for whatever type makes sense at compile time. Another way to think of it is that our function is actually a function-making machine, which recieves as input a type (in this case \lsin|a|) and returns a function of appropriate type. This matches up with the usual notation for natural transformations $\eta_x$, where the subscript represents an object of the category, in this case a type. Sometimes, this type argument is written explicitly using the notation\footnote{Don't be confused with the name \lsin|forall|. Superficially it can be understood as the universal quantifier, but from the perspective of the compiler it reads as `\lsin|eta| recieves a type \lsin|a| and returns a function of type \lsin|f a -> g a|'. The reason behind the nomenclature \lsin|forall| has to do with an important theorem in logic called the Curry-Howard isomorphism, which is far beyond the scope of this essay.}
\begin{lstlisting}
eta :: forall a. f a -> g a
\end{lstlisting}

Let us look at an example. Consider the following natural transformation from \lsin|Maybe| to \lsin|[]|
\begin{lstlisting}
maybeToList :: Maybe a -> [a]
maybeToList Nothing = []
maybeToList (Just x) = [x]
\end{lstlisting}

It is easy to verify that this is indeed natural, i.e. that
\begin{lstlisting}
maybeToList . (fmap f) = (fmap f) . maybeToList
\end{lstlisting}
Note that in this expression \lsin|fmap| is used for two different functors. Now, it would be reasonable to ask whether it would be possible to define functions \lsin|f a -> g a| which are not natural transformations, and the very surprising answer turns out to be \emph{no}. This is in contrast to, for example, \lsin|fmap|, for which it is not very difficult to define instances of the \lsin|Functor| typeclass which do not satisfy the functor laws.

This has to do with something called \emph{parametric polymorphism}. There is a very influential paper titled `Theorems for free!' \cite{theoremsforfree} by Philip Wadler, in which a method is explained for obtaining theorems about a function in typed lambda calculus from nothing but its type signature. The intuitive reason behind this very unexpected result is that in lambda calculus and in Haskell a polymorphic function (i.e. one which has more than one type at once) must be defined with the same expression for all its instantiations. For example, consider a function \lsin|f :: a -> [a]|. The expression must for \lsin|f| must be the same whether we are considering \lsin|f :: Int -> [Int]| or \lsin|f :: () -> [()]|. As a consequence, \lsin|f| cannot do anything to its argument, because it has no guarantees that anything it can do to it is valid. Therefore, there is a very limited number of functions \lsin|f| may be: it may be the constant returning the empty list, it may be the function \lsin|\x -> [x]|, it may be \lsin|\x -> [x,x]|, etc., but no more. This is an example of how the type signature of a polymorphic function can tell us a lot about the function itself, yielding the so-called theorems for free. In the case of functions \lsin|eta :: f a -> g a|, with \lsin|f| and \lsin|g| functors, the free theorem obtained from the type signature is precisely the naturality of \lsin|eta|. \cite{parametric}

\subsection{\texorpdfstring{$\Hask$}{Hask} is not Finitely Complete (Probably)}

In \ref{limcolim} we discussed products and coproducts in Haskell. We showed that $\Hask$ has initial and final objects and is closed under binary products and coproducts. Consequently, $\Hask$ has all finite products and coproducts, so it makes sense to ask whether it has (co)equalizers, as if it did it would have all finite (co)limits. I have not found any definite source proving that the answer is negative, but this Stack Overflow answer strongly suggests that the answer is no: \url{https://stackoverflow.com/a/15113919/2997964}. In any case, the fact that after over 30 years of categorical language and methods being applied to the language there is still no mechanism, built in or otherwise, for constructing (co)equalizers strongly suggests that such a problem is, if solvable, very difficult.

\section{A Kleisli Category: The Writer Monad}

The reader might recognize the names `monad' and `Kleisli category' from the mathematical terminology, and indeed the concepts we are about to discuss coincide with these mathematical notions. However, since they have not yet been explained, the reader should see these as mere names; an explanation will have to wait until section \ref{sec:monads}.

\subsection{The Motivation}

Suppose that one is implementing a complicated algorithm and wishes to keep a log of what the algorithm is doing for debugging purposes. Then, classically, one keeps a global `log' variable, to which one writes as the algorithm proceeds. In pseudocode:
\begin{lstlisting}
log = ""
def factorial(n):
	let output = 1
	log.append("We start with 1. ")
	for i = 1 ... n:
		log.append("We multiply by " ++ i ++ ". ")
		output = output * i
		log.append("We get " ++ output ++ ". ")
	log.append("This is the final result.")
	return output

factorial(3)
output: 6
log: We start with 1. We multiply by 1. We get 1. We multiply by 2. We get 2. We multiply by 3. We get 6. This is the final result.
\end{lstlisting}

In Haskell, this could be dealt with by having our \lsin|factorial| function return a pair \lsin|(String, Int)|, but this solution quickly gets unwieldy when considering complex systems. For example, function composition can no longer be done via the \lsin|(.)| operator. Indeed, composition of functions with an attached log becomes a complicated endeavor: if the function \lsin|f| does some computation and returns a log of what it did, and the function \lsin|g| wants to call \lsin|f| and compose its own logs with the ones from \lsin|f|, one needs to do ugly constructions to separate the logs of \lsin|f| from the output, do the computations, and return the logs mixed with the result. This is doable but cumbersome, as the following example shows.
\begin{lstlisting}
logAdd1 :: Int -> (String, Int)
logAdd1 x = ("Added 1", x+1)

logMul2 :: Int -> (String, Int)
logMul2 x = ("Multiplied by 2", 2*x)

log2nplus1 :: Int -> (String, Int)
log2nplus1 x = let (logmul2, xmul2) = logMul2 x in
               let (logadd1, result) = logAdd1 xmul2 in
               (logmul2 ++ logadd1, result)
\end{lstlisting}

Kleisli categories offer a solution to this issue.

\subsection{The Solution}

Let us begin by recapping the problem. We have functions which, besides computing some value, return a log of the computation. In other words, they are of type
\begin{lstlisting}
f :: a -> (String, b)
g :: b -> (String, c)
\end{lstlisting}

We want to compose \lsin|f| and \lsin|g| and obtain a composite log, but clearly the expression \lsin|g . f| does not make sense. In order to deal with logging functions in a reasonable way we need to change the notion of composition, and hence the category at hand.

\begin{definition}\label{def:kleisli}
Let $W$ be the category such that:
\begin{itemize}
\item The objects of $W$ coincide with the objects of $\Hask$,
\item Let $f \colon a \twoheadrightarrow b$ mean that $f$ is a morphism in $W$. We say $f \colon a \twoheadrightarrow b$ if $f$ is a map (in $\Hask$) of type $f \colon a \to (\type{String}, b)$. In other words,
\begin{equation}
W(a,b) := \Hask(a, (\type{String},b)).
\end{equation}
\item The composition of two maps $f \colon a \twoheadrightarrow b$ and $g \colon b \twoheadrightarrow c$ is given by
\begin{lstlisting}
h :: a -> (String, c)
h x = let (logg, y) = g x in
      let (logf, z) = f y in
      (logg ++ logf, z)
\end{lstlisting}
\end{itemize}
\end{definition}

It is clear that $W$ is a category: composition is associative because string concatenation is associative, and the identities on $W$ are simply given by
\begin{lstlisting}
idW :: a -> (String, a)
idW x = ("", x)
\end{lstlisting}

In fact, a little thought will show that the construction of $W$ can be generalized, by replacing \lsin|String| with any type which has an associative binary operation and a null element. In other words, we need a \emph{monoid}.

Monoids in Haskell are represented by the \lsin|Monoid| typeclass. For a type \lsin|w| to be in the \lsin|Monoid| typeclass, there must exist a constant
\begin{lstlisting}
mempty :: w
\end{lstlisting}
and a binary function
\begin{lstlisting}
(<>) :: w -> w -> w
\end{lstlisting}
satisfying the axioms for a monoid (associativity of \lsin|<>|, etc). Like in the case for functors, the language does not enforce the monoid axioms: it is the responsibility of the programmer, when defining a new monoid, to ensure that they hold.

That said, we can now define the so-called \emph{Kleisli category for the \lsin|Writer| monad}:

\begin{definition}
Let \lsin|w| be a type in the \lsin|Monoid| typeclass. We define \emph{the Kleisli category for the monad \lsin|Writer w|}, denoted $W$, as follows.
\begin{itemize}
\item The objects of $W$ coincide with the objects of $\Hask$,
\item A morphism $f \colon a \to b$, with $a$ and $b$ types, is a map (in $\Hask$ of type $f \colon a \to (w, b)$. In other words,
\begin{equation}
W(a,b) = \Hask(a, (w,b)).
\end{equation}

Again using $f \colon a \twoheadrightarrow b$ to mean $f \in W(a,b)$,
\item The composition of two maps $f \colon a \twoheadrightarrow b$ and $g \colon b \twoheadrightarrow c$ is given by
\begin{lstlisting}
h :: a -> (w, c)
h x = let (logg, y) = g x in
      let (logf, z) = f y in
      (logg <> logf, z)
\end{lstlisting}
\end{itemize}
\end{definition}

In Haskell, the writer monad is already present in the \lsin|Control.Monad.Writer| package. Instead of \lsin|(w,a)|, for historical reasons the notation \lsin|Writer w a| is used, so a Kleisli arrow would actually be of type
\begin{lstlisting}
f :: a -> Writer w b
\end{lstlisting} 
and composition in the Kleisli category is given by the so-called `fish operator'
\begin{lstlisting}
(>=>) :: (b -> Writer w c) -> (a -> Writer w b) -> a -> Writer w c
\end{lstlisting}
where \lsin|f >=> g| is implemented as the \lsin|h| above.

\subsection{The Tools of the Trade}

So far, we've defined the \lsin|Writer| monad (for a fixed monoid \lsin|w|) and described the corresponding Kleisli category, so that we are able to compose functions of type \lsin|a -> Writer w b|. However, composition alone does not lend itself to very readable programs, so Haskell gives us an array of tools that let us structure our programs better, to make them more readable and easier to write.

Before we begin, it is important to mention the function \lsin|writer|. We said earlier that we use the notation \lsin|Writer w a| instead of \lsin|(w,a)| as in the examples. The different notation corresponds to a different type, but this type is isomorphic to \lsin|(w,a)|, and \lsin|writer| is the isomorphism.

Well, actually, for historical reasons, the argument of \lsin|writer| actually has the product swapped, i.e. it has type \lsin|(a,w) -> Writer w a|. Obviously this is of no real consequence, as \lsin|(a,w)| is isomorphic to \lsin|(w,a)|. The following diagram shows the isomorphisms between the three types \lsin|(w,a)|, \lsin|(a,w)|, and \lsin|Writer w a|.

\begin{equation}
\begin{tikzcd}[column sep = large]
\lsmath{(w,a)} \arrow[r, bend left, "\lsmath{swap}"] & \arrow[l, bend left, "\lsmath{swap}"] \lsmath{(a,w)} \arrow[r, bend left, "\lsmath{writer}"] & \lsmath{Writer w a} \arrow[l, bend left, "\lsmath{runWriter}"]
\end{tikzcd}
\end{equation}

An essential observation is that logging and computing can be separated. If we wish to do some computation and log some result, we can define a function which does the relevant logging and computes nothing, another which 

\section{The Monad Family}\label{sec:monads}

We preface this section with a joke.

\begin{joke}
\leavevmode
\begin{description}[labelindent=\parindent]
\item[Novice learning Haskell:] What is a Monad?

\item[Haskell expert:] Nothing special: it's just a monoid in the category of endofunctors!
\end{description}
\end{joke}

We will spend the rest of this section explaining why this joke is funny.

\subsection{Categorical Definition}

We assume that the reader is already familiar with the concept of a monad, but in order to establish notation we will nevertheless give a brief introduction to the concept.

\begin{definition}
A monad in a category $X$ is an endofunctor $T \colon X \to X$, together with two natural transformations
\begin{equation}
\eta \colon \id_X \nto T, \quad \mu \colon T^2 \nto T,
\end{equation}
satisfying the equations
\begin{gather}
\mu \cdot T\mu = \mu \cdot \mu T,\\
\mu \cdot \eta T = \mu \cdot T \eta = \id_T.
\end{gather}
\end{definition}

Monads often, but not always, arise from adjunctions.

\begin{prop}
Let $F \colon X \to Y$ and $U \colon Y \to X$ be adjoint functors, with $F \dashv U$. Then, $T = UF$ is a monad, with $\eta$ the unit of the adjunction and $\mu = F \varepsilon U$, where $\varepsilon$ is the counit of the adjunction.
\end{prop}

Let us look at one particular example, which will be useful in the sequence. Let $F \colon \cat{Sets} \to \cat{Mon}$ be the free-monoid-generated-by functor, and $U \colon \cat{Mon} \to \cat{Sets}$ be the forgetful functor. In this case, $T = UF$ is the functor which, given a set $A$, returns the monad freely generated by $A$, without the monadic structure. In other words, we can see $T(A)$ as the collection of finite sequences of elements of $A$.

In this case, $\eta \colon A \to T(A)$ is the function which, given an element $a$ of $A$, returns the one-element sequence `$a$'.

The other natural transformation, $\mu$, is trickier. It is a function from $T(T(A))$ to $T(A)$. In other words, it receives sequences of sequences of elements of $A$, and returns a sequence of elements of $A$.\footnote{Strictly speaking, this is not necessarily true. The result is heavily dependent on how we represent the free monad generated by a set. Here we will be assuming that the free monad generated by $A$ is represented by the finite sequences of elements of $A$, with the operation given by concatenation.} It is not obvious, but can be deduced by expanding the definition of $\mu$ and $\eta$, that $\mu$ consists of sequence concatenation. In other words, suppose that $s = s_1 \dots s_n$ is an element of $T(A)$, with
\begin{equation}
s_i = a_{i1} \dots a_{i m_i}, \quad i = 1, \dots, n.
\end{equation}

Then, $\mu(s)$ is given by
\begin{equation}
\mu(s) = a_{11} \dots a_{1 m_1} a_{21} \dots a_{2 m_2} \dots a_{n1} \dots a_{n m_n}.
\end{equation}

[[write kleisli category first]]

\bibliographystyle{plain}
\bibliography{bibliography}

\end{document}