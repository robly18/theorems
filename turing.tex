\documentclass{article}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{mathtools}

\title{Establishing with rigour and certainty the existance of Turing Machines that compute certain functions}
\author{Duarte Maia}
\date{}

\newtheorem{theorem}{Theorem}

\newcommand{\B}{\mathbf{B}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Ha}{\mathbf{H}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\Nc}{\mathcal{N}}
\newcommand{\Q}{\mathcal{Q}}
\newcommand{\prarrow}[1]{\overset{#1}{\rightarrow}}
\newcommand{\plarrow}[1]{\overset{#1}{\leftarrow}}
\DeclareMathOperator{\suc}{\mathbf{succ}}
\DeclareMathOperator{\run}{\mathbf{run}}
\DeclareMathOperator{\dom}{\mathrm{dom}}
\DeclareMathOperator{\runtime}{\mathbf{runtime}}
\DeclareMathOperator{\out}{\mathbf{out}}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclareMathOperator{\Nifat}{\mathbf{if\,at}}
\DeclareMathOperator{\Nthen}{\mathbf{then}}
\DeclareMathOperator{\Nelse}{\mathbf{else}}
\DeclareMathOperator{\Nendif}{\mathbf{endif}}
\DeclareMathOperator{\Nwhileon}{\mathbf{while\,on}}
\DeclareMathOperator{\Ndo}{\mathbf{do}}
\DeclareMathOperator{\Nendwhile}{\mathbf{endwhile}}


\begin{document}
	\maketitle

	\section{Introduction}

	It is to my dismay that books on computability, even those that boast of care and rigour, often skimp on showing the computability of functions. That is, to show that a certain function is computable, a Turing Machine is given (or the sketch of how such would be written) and an appeal to intuition is made so that the reader will `clearly see' that the given machine will halt in finite time and output the desired result.
	
	In this document, we will establish with rigour the computability of a certain class of functions (namely, $\mu$-recursive functions), and we will show that any $\mu$-recursive function can be computed by a Turing Machine.
	
	\section{Roadmap}
	
	Before we begin, a note on what lies ahead of us.
	
	Turing Machines are hard to tame beasts. The rules they obey are seemingly simple, but a lot of complexity is hidden by them. They are, in general, wild and unpredictable, given that branching can and will occur at almost any moment. We will not try to tame these wild beasts, but will instead make tools to allow us to build structured programs whose termination and correction are more reasonably provable. Of course, the Halting Problem guarantees us that we won't be able to find a recipe for proving these things, but that doesn't stop us from doing it on a case-per-case basis.
	
	We will, on top of Turing Machines, build two languages, one on top of the other. %give them names?
	The reason for this indirection is to make it easier to compile these languages to a Turing Machine, and to show that they have the semantics we wish them to have.
	
	Our goal is to make proving things easier, and so our final language will have as few `moving parts' as possible. For example, we will abolish the pointer position, giving us less state to keep track of.
	
	%how structured will this be?

	\section{Turing Machines}

	We start by giving a formal definition of Turing Machine. Ours will be a definition of a Turing Machine with a tape infinite only to the right. These definitions are an adaptation of the definition found in \cite{bridges}.
	
	We will consider fixed an alphabet $Y$, the \emph{tape alphabet}, containing at least one symbol, the `blank symbol', which we will denote by $\B$. We will consider another alphabet, $X \subseteq Y$, which is called the \emph{input alphabet}, that does not contain $\B$. These definitions will be as general as possible, but we will always assume in the following $X$ has at least two symbols: 0 and 1. That is, $\{0,1\} \subseteq X$.
	
	Given an alphabet $A$, the set of finite strings in $A$ (including the empty one, which we will denote by $\Lambda$) is denoted $A^*$. Given a string $s$, we will denote its length by $\lvert s \rvert$. The set of (countably infinite) sequences in $A$ will be denoted $A^\infty$:  these are, formally, functions from $\N^+$ to $A$.
	
	We will allow concatenation of finite strings $u, v$ by juxtaposition: $uv$. We will allow the usual abbreviations for building languages (sets of strings): if $L$ and $M$ are two languages, $LM$ represents the language containing strings of the form $\ell m$ where $\ell \in L, m \in M$. Union, in this context, is denoted by a +. When convenient, we will omit curly braces, for example using $1^*$ to represent all strings containing only 1, or $01^*0 + 0$ to represent all strings that start and end in zeros, with only ones in the middle.
	
	Given a string $s$ or language $A$, it will be useful to denote by $s \B^\infty$ or $A \B^\infty$ the sequence (or set of sequences) obtained by concatenating the string $s$ or language $A$ to the beginning of the sequence $\B\B\B\cdots$. A sequence of the form $s \B^\infty$ is said to be \emph{finitary}.
	
	Given a string or sequence $s$, we will use $s[i]$ to refer to the $i$-th character in $s$, and notations such as $s[i,j]$ or $s]i,j[$ or anything inbetween to refer to particular intervals. The question of whether the edges are included or not should be obvious by analogy with the notation for intervals. In the particular case of sequences, we also allow the notation $s[i, \infty[$.
	
	A \emph{Turing Machine} is a 4-uple $\M = (\Q, \delta, q_0, q_f)$, where:
	
	\begin{itemize}
	\item $\Q$ is a set, the so-called \emph{set of states}
	
	\item $\delta$ is a partial function $\Q \times Y \rightharpoonup \Q \times Y \times \{L, \Lambda, R\}$
	
	\item $q_0, q_f \in \Q$ are the so-called start and end states, respectively.
	
	\item $\delta(q_f, y)$ is undefined for all $y \in Y$
	\end{itemize}
	
	A \emph{tape state on $\M = (\Q, \delta, q_0, q_f)$} is a triple $T = (q, t, p)$ such that $q \in \Q$, $t \in Y^\infty$, $p \in \N^+$.
	
	Given a Turing Machine $\M = (\Q, \delta, q_0, q_f)$ and a tape state $T = (q, t, p)$, we will say $T' = (q', t', p')$ is the \emph{successor state of $T$} if:
	
	Let $\delta(q, t[p]) = (q', c, d)$ in
	
	\begin{itemize}
	\item $t' = t[1, p[ \, c \, t]p, \infty[$
	
	\item If $d$ is $L, \Lambda, R$, then $p = p' + 1, p = p', p + 1 = p'$, respectively.
	\end{itemize}
	
	The reader should have no trouble seeing that this state is unique (justifying calling it `the' successor rather than `a' successor), and it is not hard to see that it is not always defined. For example, if $p$ is $1$ and $d$ is $L$, the equation $p = p' + 1$ has no solution $p'$ in $\N^+$. Furthermore, this is obviously not defined if $\delta(q, t[p])$ is undefined, which it might very well be.
	
	We define the partial \emph{successor} function $\suc T = T'$.
	
	We say that the Turing Machine $\M = (\Q, \delta, q_0, q_f)$ \emph{halts on input $s$ with output $t$}, where $s, t \in X^*$, if there exists $n$ such that
	
	\[\suc^n (q_0, s \B^\infty, 1) = (q_f, t \B^\infty, 1)\]
	
	It is clear that given some input, the output on which $\M$ halts, if it exists, is unique, as this last state has no successor as $\delta(q_f, (t \B^\infty)[1])$ is undefined by definition.
	
	This allows us to define, for any Turing Machine $\M$, the partial function $\out_\M : X^* \rightharpoonup X^*$ that, given $s$, if $\M$ halts on input $s$ with some output $t$, returns $t$. It is otherwise undefined.
	
	In what follows, we will be interested in a more general meaning of `run' and `halt'. Namely, we will be interested in the behavior of machines that stop execution, even if they don't park on the left end of the tape, or with a tape of the form $X^* \B^\infty$, or if they don't necessarily start in these conditions.
	
	Suppose given a sequence $s$ and a position $p$, as well as a Turing Machine $\M$ with starting state $q_0$ and final state $q_f$. On the occasion that there exists $n$ such that $\suc^n (q_0, s, p) = (q_f, s', p')$ -- It is clear that this $n$ is unique due to the restriction enforced on the state change function -- we define the partial function $\runtime_\M (s,p) := n$, as well as $\run_\M (s, p) = (s', p')$. If these exist, we say $\M$ \emph{halts on the tape $(s,p)$ with the tape $(s', p')$ after $n$ steps}. Otherwise, we say $\M$ does not halt. Notice that this includes the case of undefined behavior, such as an undefined result for $\delta$, or trying to move $\L$ on position 0.
	
	\section{Computability}
	
	We have laid out the basic groundwork for defining what it means for a function to be Turing Computable:
	
	Given a partial function $f : X^* \supseteq A \rightharpoonup B \subseteq X^*$, we say $\M$ \emph{represents $f$ on $A$} if:
	
	\begin{itemize}
	
	\item For all $s \in \dom f$, $\out_\M s$ is defined and equals $f(s)$
	
	\item For all $s \in A \setminus \dom f$ either $\out_\M s$ is undefined, or it is not in $B$.
	
	\end{itemize}
	
	We wish to establish the computability or noncomputability of some functions. For example, we would like to show that the sum of natural numbers is computable. Of course, the sum is a function from $\N_0 \times \N_0$ to $\N_0$, neither of which are (unless you have a very odd notion of natural number and cartesian product) subsets of $X^*$, so we will have to adjust our definitions accordingly.
	
	As such, we will encode $n$-uples of natural numbers in a standard form: any natural number $n$ will be encoded in unary. The symbol $\ceil{n}$ will be used to denote the string composed of $n$ ones. That is:
	
	\[\ceil{n} = \underbrace{111\cdots1}_{n \text{ times}}\]
	
	Defining this inductively: $\ceil{0} = \Lambda$; $\ceil{n+1} = \ceil{n}1$.
	
	It is easy to see by induction that $\ceil{n+m} = \ceil{n}\ceil{m}$.
	
	We now move on to encoding $n$-uples of natural numbers. For our purposes, zero will serve as a comma. As such, the $n$-uple $(x_1, x_2, \cdots, x_n)$ is encoded as

	\[ \ceil{(x_1, x_2, \cdots, x_n)} = \ceil{x_1}0\ceil{x_2}0\cdots0\ceil{x_n} \]
	
	For our purposes, when convenient, we will identify $\N_0$ with the set of strings of ones, i.e. we will pretend $\N_0 = 1^*$, and likewise, we will identify $\N_0^n$ with the strings of the above form. That is, we identify:
	
	\[\N_0^n = \underbrace{1^*\,0\,1^*\,0\,1^*\,0 \cdots 0\,1^*}_{n \text{ zeroes}}\]
	
	This leaves us with only one caveat: the zero-uple. I have yet to find a satisfying way to define these things in a satisfactory way.
	
	Our definition of computable function will stay the same, but our focus will be mostly on functions from $\N_0^n$ to $\N_0$, where $n$ is greater than zero.\footnote{We could focus on functions from $\N_0^n$ to $\N_0^m$, but any such function can just be represented by $m$ functions from $\N_0^n$ to $\N_0$, and so, we can restrict our study to this slightly easier class of functions without loss of generality.} It is a shame to be excluding the zeroary functions, but fortunately, these are just constants, and thus of little interest, as a function to compute these would simply erase the tape and write its own output, making all such functions trivially computable.
	
	So, we wish to investigate the set of Turing computable functions. Examples of functions we would love to show computable would be, for example, the function $+ : \N_0^2 \rightarrow \N_0$, or $\times$, or $- : \N_0^2 \rightharpoonup \N_0$. However, while writing out a program for each of these is not of extreme difficulty, showing formally that they terminate and that the output is what we expect it to be is tedious. So much so, in fact, that most books on the subject simply forego this verification for the most part.
	
	The main reason for this is that, on Turing Machines, there is a lot of global state to keep track of, and relatively few tools to keep track of it. Not only do we have both the tape and pointer position to keep track of, it is also very hard in general to modify arbitrary data, as, unless we are modifying something at the rightmost end of the written data, we will have to shuffle things over to make room, which, without being very careful about how we organize the data, might very well be an enormous headache. Add to it the fact that there are no limitations on control flow (you can jump from any state to any other state without restrictions) and it is extremely hard to show the correctness of a given program.
	
	As such, in this study, we will develop a sort of `compiled language', which will sit on Turing machines and which we will show can be compiled to one, with the semantics we will later establish, and about which it will be much easier to reason about, using techniques such as (a modified version of) Hoare calculus.
	
	The first step in this path is to develop a set of instructions (composed of possibly more than one state) whose behavior we can reason about in abstract. But before we do that, we will need a way to compose them.
	
	\section{Composition}
	Our first special program is going to be composition.
	
	Given two Turing Machines, $\M_1$ and $\M_2$, their \emph{composition}, denoted $\M_1;\M_2$, is the machine $\M$ described as follows:
	
	Let 
	
	\[\M_1 = (\Q_1, \delta_1, q_{10}, q_{1f})\]
	\[\M_2 = (\Q_2, \delta_2, q_{20}, q_{2f})\]
	
	Define
	
	\[ \M = \M_1 ; \M_2 \]
	
	As $\M = (\Q, \delta, (q_{10}, 1), (q_{2f}, 2))$ such that:
	
	\begin{itemize}
	\item $\Q = \{\, (q, i) \mid q \in \Q_i, i = 1,2 \,\}$
	
	\item $\delta((q, i), y) = ((q', i), c, d)$ where $\delta_i(q,y) = (q', c, d)$ and $q \neq q_{if}$
	
	\item $\delta((q_{1f}, 1), y) = ((q_{20}, 2), y, \Lambda)$
	\end{itemize}
	
	Predictably, we prove that this machine has the expected behavior: it's as though we ran $\M_1$ until it halts, followed by running $\M_2$ on the result.
	
	\begin{theorem}
	Fix the Turing Machines:
	
	\[\M_1 = (\Q_1, \delta_1, q_{10}, q_{1f})\]
	\[\M_2 = (\Q_2, \delta_2, q_{20}, q_{2f})\]
	
	And some sequence $s$ and position $p$. Let $\M = (\Q, \delta, q_0, q_f) = \M_1 ; \M_2$. Then, we have
	
	\[\run_\M (s,p) = \run_{\M_2} \run_{\M_1} (s,p)\]
	\[\runtime_\M (s,p) = \runtime_{\M_1} (s,p) + 1 + \runtime_{\M_2} \run_{\M_1} (s,p)\]
	
	(This includes the case of non-halting, as, in that case, both sides will be undefinex.
	\end{theorem}
	
	\begin{proof}
	To simplify the exposition, we introduce the following notation:
	
	Given two triples $(q, c, d)$ and $(q', c', d')$, we say $(q, c, d) \prarrow{i} (q', c', d')$ if $c = c', d = d'$ and $q' = (q, i)$.
	
	Let $n_1 = \runtime_{\M_1} (s,p)$ and $n_2 = \runtime_{\M_2} \run_{\M_1} (s,p)$. We suppose for now that both of these exist.
	
	Now, we show by induction that for $i = 0, \cdots, n_1$ we have that
	
	\[\suc^{i}_{\M_1} (q_{10}, s, p) \prarrow{1} \suc^{i}_{\M_1;\M_2} ((q_{10}, 1), s, p)\]
	
	
	For $i = 0$ this is obvious. Suppose this is true for some $i < n_1$.
	
	That is, suppose 
	
	\[\suc^{i}_{\M_1} (q_{10}, s, p) = (q', s', p')\]
	\[\suc^{i}_{\M_1;\M_2} ((q_{10}, 1), s, p) = ((q', 1), s', p')\]
	
	Thus,
	
	\[\suc^{i+1}_{\M_1} (q_{10}, s, p) = \suc_{\M_1} (q', s', p')\]
	\[\suc^{i+1}_{\M_1;\M_2} (q_{10}, s, p) = \suc_{\M_1;\M_2} ((q',1), s', p')\]
	
	Supposing $\delta_1(q', s'[p']) = (q'', c, d)$, we have that $\delta((q', 1), s'[p']) = ((q'', 1), c, d)$. The reader can now easily examine the three cases of the definition of successor to see that, indeed, if the former is $(q'', s'', p'')$, the latter will be $((q'', 1), s'', p'')$.
	
	Now, notice that if
	
	\[\suc^{n_1}_{\M_1} (q_{10}, s, p) = (q_{1f}, t, p_1)\]
	
	Then, by what we just showed,
	
	\[\suc^{n_1}_{\M_1;\M_2} ((q_{10}, 1), s, p) = ((q_{1f}, 1), t, p_1)\]
	
	Taking the successor of the latter, we know that, for $i = 0$,
	
	\[\suc^{i}_{\M_2} (q_{20}, t, p_1) = (q_{20}, t, p_1)\]
	\[\suc^{n_1+1+i}_{\M_1;\M_2} ((q_{10}, 1), s, p) = ((q_{20}, 2), t, p_1)\]
	
	We will show that for $i = 0, \cdots, n_2$ we have that, if
	
	\[\suc^{i}_{\M_2} (q_{20}, t, p_1) \prarrow{2} \suc^{n_1+1+i}_{\M_1;\M_2} ((q_{10}, 1), s, p)\]
	
	We already saw it true for $i=0$, so we show it true for the rest by induction.
	
	Suppose $i < n_2$. Then assuming it true for $i$, we have,
	
	\[\suc^{i+1}_{\M_2} (q_{20}, t, p_1) = \suc_{\M_2} (q', t', p')\]
	
	\[\suc^{n_1+1+(i+1)}_{\M_1;\M_2} ((q_{10}, 1), s, p) = \suc_{\M_1;\M_2} ((q', 2), t', p')\]
	
	Repeating the same argument as before, we see that if $\delta_2(q', t'[p']) = (q'', c, d)$, then $\delta((q',2), t'[p']) = ((q'', 2), c, d)$ as $q' \neq q_{2f}$, and so, checking against the definition of successor, if the former is $(q'', t'', p'')$ then the latter will be $((q'', 2), t'', p'')$.
	
	This culminates in $i = n_2$, where we have
	
	\[\suc^{n_2}_{\M_2} (q_{20}, t, p_1) = (q_{2f}, r, p_2)\]
	
	And so,
	
	\[\suc^{n_1 + 1 + n_2}_{\M_1;\M_2} ((q_{10},1), s, p) = ((q_{2f},2), r, p_2)\]
	
	As we wished.
	
	Now, we need only show that if either machine does not halt, then neither does their composition. If one of the machines does not halt, then either there is undefined behavior, or it never reaches the final state. Reproducing the steps in the proof above, one can see that if any of these happens, it also happens for the composed machine. %maybe fix this later
	\end{proof}
	
	This last proposition tells us something very important, that will form the basis of most of what we will do from now on.
	
	Let $\phi, \psi, \omega$ be propositions of two free variables: a sequence $s$ and a positive integer $p$. Given a machine $\M$, the symbol $\phi\M\psi$ represents the proposition `if $\phi(s,p)$ is true and $\run_\M(s, p) = (s', p')$, then $\psi(s', p')$ is true.'
	
	Sometimes the juxtaposition notation will be inadequate. If this is the case, we also allow using curly braces to group propositions, as such: $\{\phi\}\M\{\psi\}$.
	
	
	\begin{theorem}
	If $\phi \M_1 \psi$ and $\psi \M_2 \omega$, then $\phi \M_1;\M_2 \omega$
	\end{theorem}
	
	\begin{proof}
	%deal with non-haltings!!
	TODO fix
	
	If $\phi(s,p)$ is true, then, by hypothesis, if $\M_1$ halts on the tape $(q_{10}, s, p)$ with $(q_{1f}, s', p')$, we have $\psi(s', p')$. We then run $\M_2$ on $(q_{20}, s', p')$ to get $(q_{2f}, s'', p'')$, and thus, by hypothesis $\omega(s'', p'')$.
	
	Since, by the previous theorem, $\M_1;\M_2$ halts on the tape $((q_{10}, 1),s,p)$ with output $((q_{2f}, 2), s'', p'')$, it is clear that $\phi \M_1;\M_2 \omega$.
	
	%NOTATION!!!
	\end{proof}
	
	We move on to our second special program: conditional branching.
	
	Fix two Turing Machines, $\M_1$ and $\M_2$ such that
	
	\[\M_1 = (\Q_1, \delta_1, q_{10}, q_{1f})\]
	\[\M_2 = (\Q_2, \delta_2, q_{20}, q_{2f})\]
	
	And fixed some element $y_0$ of $Y$, we define the following Turing Machine:
	
	\[ \M = \Nifat y_0 \Nthen \M_1 \Nelse \M_2 \Nendif \]
	
	As $\M = (\Q, \delta, q_0, q_f)$ such that:
	
	\begin{itemize}
	\item $\Q = \{\,(q, i) \mid q \in \Q_i, i = 1,2\,\} \cup \{q_0, q_f\}$
	
	\item $q_0$ and $q_f$ are distinct symbols %this needs to be done better
	
	\item $\delta(q_0, y_0) = ((q_{10}, 1), y_0, \Lambda)$
	
	\item $\delta(q_0, y) = ((q_{20}, 2), y, \Lambda)$ for all $y \neq y_0$
	
	\item $\delta((q, i), y) = ((q', i), c, d)$ if $\delta_i(q, y) = (q', c, d)$
	
	\item $\delta((q_{if}, i), y) = (q_f, y, \Lambda)$
	\end{itemize}
	
	This machine's behavior is what it says on the tin, as the next proposition shows.
	
	\begin{theorem}
	Let $s$ be a sequence, $p$ a position, $y_0 \in Y$, and the following Turing Machines:
	
	
	\[\M_1 = (\Q_1, \delta_1, q_{10}, q_{1f})\]
	\[\M_2 = (\Q_2, \delta_2, q_{20}, q_{2f})\]
	\[\M = (\Q, \delta, q_0, q_f) = \Nifat y_0 \Nthen \M_1 \Nelse \M_2 \Nendif \]
	
	If $s[p] = y_0$ and $\suc^n_{\M_1} (q_{10}, s, p) = (q_{1f}, s', p')$, then $\suc^{n+2}_\M (q_0, s, p) = (q_f, s', p')$
	
	Likewise, if $s[p] \neq y_0$ and $\suc^n_{\M_2} (q_{20}, s, p) = (q_{2f}, s', p')$, then $\suc^{n+2}_\M (q_0, s, p) = (q_f, s', p')$
	\end{theorem}
	
	\begin{proof}
	We will only do the proof for the first case, as the proof of the second is completely analogous.
	
	Suppose, then $s[p] = y_0$. We have $\suc_\M (q_0, s, p) = ((q_{10}, 1), s, p)$, as the reader may easily verify.
	
	From now on, we show by induction that, for $i = 0, \cdots, n$, if
	
	\[\suc^i_{\M_1} (q_{10}, s, p) = (q, s', p')\]
	
	Then
	
	\[\suc^{1+i}_\M (q_0, s, p) = ((q, 1), s', p')\]
	
	For $i = 0$ this has already been shown. Suppose, now, this is true for some $i < n$.
	
	Applying the definition of $\delta$ vs $\delta_1$, knowing that $q \neq q_{1f}$, one can easily see that if the state change for the former is given by $(q', c, d)$, the state change for the latter is given by $((q', 1), c, d)$, and so the successors will also obey this relation.
	
	Finally, now that we know that if $\suc^n_{\M_1} (q_{10}, s, p) = (q_{1f}, s', p')$ then $\suc^{1+n}_\M (q_0, s, p) = ((q_{1f}, 1), s', p')$, applying the successor function to the latter once more, we get $\suc^{n+2}_\M (q_0, s, p) = (q_f, s', p')$, as we wished to show.
	
	This finishes the proof of the first case. As said before, the proof of the second case is completely analogous.
	\end{proof}
	
	This gives us the following corollary:
	
	\begin{theorem}
	Suppose that $\{\phi \land s[p] = y_0\} \M_1 \{\psi\}$ and $\{\phi \land s[p] \neq y_0\} \M_1 \{\psi\}$. Then,
	
	\[\{\phi\} \Nifat y_0 \Nthen \M_1 \Nelse \M_2 \Nendif \{\psi\}\]
	\end{theorem}
	
	\begin{proof}
	TODO. Do this when you fix your damn notation.
	\end{proof}
	
	We finish this section with the last kind of composition we will need for sanely making Turing Machines.

	Fixed some subset $S$ of $Y$ and a Turing Machine $\M_1 = (\Q_1, \delta_1, q_{10}, q_{1f})$, we define the following Turing Machine:
	
	\[ \M = \Nwhileon S \Ndo \M_1 \Nendwhile \]
	
	As $\M = (\Q, \delta, q_0, q_f)$ such that:
	
	\begin{itemize}
	\item $\Q = \Q_1 \cup \{q_0, q_f\}$
	
	\item $q_0$ and $q_f$ are symbols not in $\Q$
	
	\item $\delta(q_0, y) = (q_{10}, y, \Lambda)$ for all $y \in S$
	
	\item $\delta(q_0, y) = (q_f, y, \Lambda)$ for all $y \not\in S$
	
	\item $\delta(q, y) = \delta_1(q, y)$ for all $q \in \Q_1$ if $q \neq q_{1f}$
	
	\item $\delta(q_{1f}, y) = (q_0, y, \Lambda)$
	\end{itemize}
	
	
	
\begin{thebibliography}{9}
\bibitem{bridges} 
Bridges, Douglas S.
\textit{Computability - A Mathematical Sketchbook}. 
Springer, 1994. %i don't know how to do this right
\end{thebibliography}	

\end{document}