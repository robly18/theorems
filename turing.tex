\documentclass{article}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{mathtools}

\title{Establishing with rigour and certainty the existance of Turing Machines that compute certain functions}
\author{Duarte Maia}
\date{}

\newtheorem{theorem}{Theorem}

\newcommand{\B}{\mathbf{B}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Ha}{\mathbf{H}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\Nc}{\mathcal{N}}
\newcommand{\Q}{\mathcal{Q}}
\DeclareMathOperator{\suc}{\mathbf{succ}}
\DeclareMathOperator{\run}{\mathbf{run}}
\DeclareMathOperator{\dom}{\mathrm{dom}}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclareMathOperator{\Nifat}{\mathbf{if\,at}}
\DeclareMathOperator{\Nthen}{\mathbf{then}}
\DeclareMathOperator{\Nelse}{\mathbf{else}}
\DeclareMathOperator{\Nendif}{\mathbf{endif}}


\begin{document}
	\maketitle

	\section{Introduction}

	It is to my dismay that books on computability, even those that boast of care and rigour, often skimp on showing the computability of functions. That is, to show that a certain function is computable, a Turing Machine is given (or the sketch of how such would be written) and an appeal to intuition is made so that the reader will `clearly see' that the given machine will halt in finite time and output the desired result.
	
	In this document, we will establish with rigour the computability of a certain class of functions (namely, $\mu$-recursive functions), and we will show that any $\mu$-recursive function can be computed by a Turing Machine.
	
	\section{Roadmap}
	
	Before we begin, a note on what lies ahead of us.
	
	Turing Machines are hard to tame beasts. The rules they obey are seemingly simple, but a lot of complexity is hidden by them. They are, in general, wild and unpredictable, given that branching can and will occur at almost any moment. We will not try to tame these wild beasts, but will instead make tools to allow us to build structured programs whose termination and correction are more reasonably provable. Of course, the Halting Problem guarantees us that we won't be able to find a recipe for proving these things, but that doesn't stop us from doing it on a case-per-case basis.
	
	We will, on top of Turing Machines, build two languages, one on top of the other. %give them names?
	The reason for this indirection is to make it easier to compile these languages to a Turing Machine, and to show that they have the semantics we wish them to have.
	
	Our goal is to make proving things easier, and so our final language will have as few `moving parts' as possible. For example, we will abolish the pointer position, giving us less state to keep track of.
	
	%how structured will this be?

	\section{Turing Machines}

	We start by giving a formal definition of Turing Machine. Ours will be a definition of a Turing Machine with a tape infinite only to the right. These definitions are an adaptation of the definition found in \cite{bridges}.
	
	We will consider fixed an alphabet $Y$, the \emph{tape alphabet}, containing at least one symbol, the `blank symbol', which we will denote by $\B$. We will consider another alphabet, $X \subseteq Y$, which is called the \emph{input alphabet}, that does not contain $\B$. These definitions will be as general as possible, but we will always assume in the following $X$ has at least two symbols: 0 and 1. That is, $\{0,1\} \subseteq X$.
	
	Given an alphabet $A$, the set of finite strings in $A$ (including the empty one, which we will denote by $\Lambda$) is denoted $A^*$. Given a string $s$, we will denote its length by $\lvert s \rvert$. The set of (countably infinite) sequences in $A$ will be denoted $A^\infty$:  these are, formally, functions from $\N^+$ to $A$.
	
	We will allow concatenation of finite strings $u, v$ by juxtaposition: $uv$. We will allow the usual abbreviations for building languages (sets of strings): if $L$ and $M$ are two languages, $LM$ represents the language containing strings of the form $\ell m$ where $\ell \in L, m \in M$. Union, in this context, is denoted by a +. When convenient, we will omit curly braces, for example using $1^*$ to represent all strings containing only 1, or $01^*0 + 0$ to represent all strings that start and end in zeros, with only ones in the middle.
	
	Given a string $s$ or language $A$, it will be useful to denote by $s \B^\infty$ or $A \B^\infty$ the sequence (or set of sequences) obtained by concatenating the string $s$ or language $A$ to the beginning of the sequence $\B\B\B\cdots$. A sequence of the form $s \B^\infty$ is said to be \emph{finitary}.
	
	Given a string or sequence $s$, we will use $s[i]$ to refer to the $i$-th character in $s$, and notations such as $s[i,j]$ or $s]i,j[$ or anything inbetween to refer to particular intervals. The question of whether the edges are included or not should be obvious by analogy with the notation for intervals. In the particular case of sequences, we also allow the notation $s[i, \infty[$.
	
	A \emph{Turing Machine} is a 4-uple $\M = (\Q, \delta, q_0, q_f)$, where:
	
	\begin{itemize}
	\item $\Q$ is a set, the so-called \emph{set of states}
	
	\item $\delta$ is a partial function $\Q \times Y \rightharpoonup \Q \times Y \times \{L, \Lambda, R\}$
	
	\item $q_0, q_f \in \Q$ are the so-called start and end states, respectively.
	
	\item $\delta(q_f, y)$ is undefined for all $y \in Y$
	\end{itemize}
	
	A \emph{tape state on $\M = (\Q, \delta, q_0, q_f)$} is a triple $T = (q, t, p)$ such that $q \in \Q$, $t \in Y^\infty$, $p \in \N^+$.
	
	Given a Turing Machine $\M = (\Q, \delta, q_0, q_f)$ and a tape state $T = (q, t, p)$, we will say $T' = (q', t', p')$ is the \emph{successor state of $T$} if:
	
	Let $\delta(q, t[p]) = (q', c, d)$ in
	
	\begin{itemize}
	\item $t' = t[1, p[ \, c \, t]p, \infty[$
	
	\item If $d$ is $L, \Lambda, R$, then $p = p' + 1, p = p', p + 1 = p'$, respectively.
	\end{itemize}
	
	The reader should have no trouble seeing that this state is unique (justifying calling it `the' successor rather than `a' successor), and it is not hard to see that it is not always defined. For example, if $p$ is $1$ and $d$ is $L$, the equation $p = p' + 1$ has no solution $p'$ in $\N^+$. Furthermore, this is obviously not defined if $\delta(q, t[p])$ is undefined, which it might very well be.
	
	We define the partial \emph{successor} function $\suc T = T'$.
	
	We say that the Turing Machine $\M = (\Q, \delta, q_0, q_f)$ \emph{halts on input $s$ with output $t$}, where $s, t \in X^*$, if there exists $n$ such that
	
	\[\suc^n (q_0, s \B^\infty, 1) = (q_f, t \B^\infty, 1)\]
	
	It is clear that given some input, the output on which $\M$ halts, if it exists, is unique, as this last state has no successor as $\delta(q_f, (t \B^\infty)[1])$ is undefined by definition.
	
	This allows us to define, for any Turing Machine $\M$, the partial function $\run_\M : X^* \rightharpoonup X^*$ that, given $s$, if $\M$ halts on input $s$ with some output $t$, returns $t$. It is otherwise undefined.
	
	\section{Computability}
	
	We have laid out the basic groundwork for defining what it means for a function to be Turing Computable:
	
	Given a partial function $f : X^* \supseteq A \rightharpoonup B \subseteq X^*$, we say $\M$ \emph{represents $f$ on $A$} if:
	
	\begin{itemize}
	
	\item For all $s \in \dom f$, $\run_\M s$ is defined and equals $f(s)$
	
	\item For all $s \in A \setminus \dom f$ either $\run_\M s$ is undefined, or it is not in $B$.
	
	\end{itemize}
	
	We wish to establish the computability or noncomputability of some functions. For example, we would like to show that the sum of natural numbers is computable. Of course, the sum is a function from $\N_0 \times \N_0$ to $\N_0$, neither of which are (unless you have a very odd notion of natural number and cartesian product) subsets of $X^*$, so we will have to adjust our definitions accordingly.
	
	As such, we will encode $n$-uples of natural numbers in a standard form: any natural number $n$ will be encoded in unary. The symbol $\ceil{n}$ will be used to denote the string composed of $n$ ones. That is:
	
	\[\ceil{n} = \underbrace{111\cdots1}_{n \text{ times}}\]
	
	Defining this inductively: $\ceil{0} = \Lambda$; $\ceil{n+1} = \ceil{n}1$.
	
	It is easy to see by induction that $\ceil{n+m} = \ceil{n}\ceil{m}$.
	
	We now move on to encoding $n$-uples of natural numbers. For our purposes, zero will serve as a comma. As such, the $n$-uple $(x_1, x_2, \cdots, x_n)$ is encoded as

	\[ \ceil{(x_1, x_2, \cdots, x_n)} = \ceil{x_1}0\ceil{x_2}0\cdots0\ceil{x_n} \]
	
	For our purposes, when convenient, we will identify $\N_0$ with the set of strings of ones, i.e. we will pretend $\N_0 = 1^*$, and likewise, we will identify $\N_0^n$ with the strings of the above form. That is, we identify:
	
	\[\N_0^n = \underbrace{1^*\,0\,1^*\,0\,1^*\,0 \cdots 0\,1^*}_{n \text{ zeroes}}\]
	
	This leaves us with only one caveat: the zero-uple. I have yet to find a satisfying way to define these things in a satisfactory way.
	
	Our definition of computable function will stay the same, but our focus will be mostly on functions from $\N_0^n$ to $\N_0$, where $n$ is greater than zero.\footnote{We could focus on functions from $\N_0^n$ to $\N_0^m$, but any such function can just be represented by $m$ functions from $\N_0^n$ to $\N_0$, and so, we can restrict our study to this slightly easier class of functions without loss of generality.} It is a shame to be excluding the zeroary functions, but fortunately, these are just constants, and thus of little interest, as a function to compute these would simply erase the tape and write its own output, making all such functions trivially computable.
	
	So, we wish to investigate the set of Turing computable functions. Examples of functions we would love to show computable would be, for example, the function $+ : \N_0^2 \rightarrow \N_0$, or $\times$, or $- : \N_0^2 \rightharpoonup \N_0$. However, while writing out a program for each of these is not of extreme difficulty, showing formally that they terminate and that the output is what we expect it to be is tedious. So much so, in fact, that most books on the subject simply forego this verification for the most part.
	
	The main reason for this is that, on Turing Machines, there is a lot of global state to keep track of, and relatively few tools to keep track of it. Not only do we have both the tape and pointer position to keep track of, it is also very hard in general to modify arbitrary data, as, unless we are modifying something at the rightmost end of the written data, we will have to shuffle things over to make room, which, without being very careful about how we organize the data, might very well be an enormous headache. Add to it the fact that there are no limitations on control flow (you can jump from any state to any other state without restrictions) and it is extremely hard to show the correctness of a given program.
	
	As such, in this study, we will develop a sort of `compiled language', which will sit on Turing machines and which we will show can be compiled to one, with the semantics we will later establish, and about which it will be much easier to reason about, using techniques such as (a modified version of) Hoare calculus.
	
	The first step in this path is to develop a set of instructions (composed of possibly more than one state) whose behavior we can reason about in abstract. But before we do that, we will need a way to compose them.
	
	\section{Composition}
	
	Consider a set of Turing Machines, $\{\M_i\}$ indexed by some finite set $I$. Suppose defined a partial function $\Delta : I \times Y \rightharpoonup I$, and pick a starting state $i_0 \in I$ and a halting state $i_f \in I$. Suppose also that $\Delta(i_f, y)$ is undefined for all $i \in Y$. The 5-uple $\Nc = (I, \{\M_i\}, \Delta, i_0, i_f)$ is said to be a \emph{meta-Turing Machine}.
	
	The analogy between this and a Turing machine should be clear, which is why we make the following definition:
	
	Given a meta-Turing Machine $\Nc = (I, \{\M_i\}, \Delta, i_0, i_f)$, we say $\Nc$ \emph{compiles to} $\M = (\Q, \delta, q_0, q_f)$ if $\M$ is the Turing Machine such that:
	
	\begin{itemize}
	\item Suppose $\M_i = (\Q_i, \delta_i, q_{i0}, q_{if})$
	
	\item $\Q = \{\,(q, i) \mid i \in I, q \in \Q_i\,\}$
	
	\item $\delta((q, i), y) = ((q', i), c, d)$ if $q \neq q_{if}$ and $\delta_i(q, y) = (q', c, d)$
	
	\item $\delta((q_{i_f}, i), y) = \text{ let $\Delta(i, y) = j$ in } (q_{j0}, y, \Lambda)$
	
	\item $q_0 = (q_{i_0 0}, i_0)$
	
	\item $q_f = (q_{i_f f}, i_f)$
	\end{itemize}
	
	The reader can verify that $\M$ does indeed satisfy the definition of a Turing Machine.
	
	It might seem unclear how throwing this mess of a definition on top of the already-complicated Turing Machines would make anything easier. But the trick is, we're only compiling special programs.
	
	Our first special program is going to be composition.
	
	Given two Turing Machines, $\M_1$ and $\M_2$, their \emph{composition}, denoted $\M_1;\M_2$, is the machine obtained by compilation of:
	
	\[(\{1,2\}, \{\M_1, \M_2\}, \Delta, 1, 2)\]
	
	Where $\Delta(1, y) = 2$.
	
	Predictably, we prove that this machine has the expected behavior: it's as though we ran $\M_1$ until it halts, followed by running $\M_2$ on the result.
	
	\begin{theorem}
	Fix the Turing Machines:
	
	\[\M_1 = (\Q_1, \delta_1, q_{10}, q_{1f})\]
	\[\M_2 = (\Q_2, \delta_2, q_{20}, q_{2f})\]
	
	And some sequence $s$ and position $p$.
	
	Suppose $n_1$ is such that
	
	\[\suc^{n_1}_{\M_1} (q_{10}, s, p) = (q_{1f}, t, p_1)\]
	
	And $n_2$ is such that
	
	\[\suc^{n_2}_{\M_2} (q_{20}, t, p_1) = (q_{1f}, r, p_2)\]
	
	Then, $\suc^{n_1 + n_2 + 1}_{\M_1;\M_2} ((q_{10},1), s, p) = ((q_{1f},2), r, p_2)$.
	\end{theorem}
	
	\begin{proof}
	%todo, make this proof less disgusting. maybe introduce new notation so this won't suck
	
	First, we show by induction that for $i = 0, \cdots, n_1$ we have that if
	
	\[\suc^{i}_{\M_1} (q_{10}, s, p) = (q', s', p')\]
	
	And
	
	\[\suc^{i}_{\M_1;\M_2} ((q_{10}, 1), s, p) = (q'', s'', p'')\]
	
	Then $s' = s''$, $p' = p''$ and $q'' = (q', 1)$.
	
	For $i = 0$ this is obvious. Suppose this is true for some $i < n_1$.
	
	That is, suppose 
	
	\[\suc^{i}_{\M_1} (q_{10}, s, p) = (q', s', p')\]
	\[\suc^{i}_{\M_1;\M_2} ((q_{10}, 1), s, p) = ((q', 1), s', p')\]
	
	Thus,
	
	\[\suc^{i+1}_{\M_1} (q_{10}, s, p) = \suc_{\M_1} (q', s', p')\]
	\[\suc^{i+1}_{\M_1;\M_2} (q_{10}, s, p) = \suc_{\M_1;\M_2} ((q',1), s', p')\]
	
	Supposing $\delta_1(q', s'[p']) = (q'', c, d)$, we have that, if $\delta$ is the state change function of $\M_1;\M_2$, $\delta((q', 1), s'[p']) = ((q'', 1), c, d)$. We know this because, since $i < n_1$, $q'$ can't be equal to $q_{0f}$. Comparing this with the definition of successor, the reader will easily get to the conclusion that the induction step checks out. %todo fix this trash
	
	Now, notice that if
	
	\[\suc^{n_1}_{\M_1} (q_{10}, s, p) = (q_{1f}, t, p_1)\]
	
	Then, by what we just showed,
	
	\[\suc^{n_1}_{\M_1;\M_2} ((q_{10}, 1), s, p) = ((q_{1f}, 1), t, p_1)\]
	
	Taking the successor of the latter, we know that, for $i = 0$,
	
	\[\suc^{i}_{\M_2} (q_{20}, t, p_1) = (q_{20}, t, p_1)\]
	\[\suc^{n_1+1+i}_{\M_1;\M_2} ((q_{10}, 1), s, p) = ((q_{20}, 1), t, p_1)\]
	
	We will show that for $i = 0, \cdots, n_2$ we have that, if
	
	\[\suc^{i}_{\M_2} (q_{20}, t, p_1) = (q', t', p')\]
	
	Then
	
	\[\suc^{n_1+1+i}_{\M_1;\M_2} ((q_{10}, 1), s, p) = ((q', 1), t', p')\]
	
	We already saw it true for $i=0$, so we show it true for the rest by induction.
	
	Suppose $i < n_2$. Then,
	
	\[\suc^{i+1}_{\M_2} (q_{20}, t, p_1) = \suc_{\M_2} (q', t', p')\]
	
	\[\suc^{n_1+1+(i+1)}_{\M_1;\M_2} ((q_{10}, 1), s, p) = \suc_{\M_1;\M_2} ((q', 2), t', p')\]
	
	Repeating the same argument as before, we see that if $\delta_2(q', t'[p']) = (q'', c, d)$, then $\delta((q',2), t'[p']) = ((q'', 2), c, d)$ as $q' \neq q_{2f}$, and so, checking against the definition of successor, the induction step holds true.
	
	This culminates in $i = n_2$, where we have
	
	\[\suc^{n_2}_{\M_2} (q_{20}, t, p_1) = (q_{1f}, r, p_2)\]
	
	And so,
	
	\[\suc^{n_1 + 1 + n_2}_{\M_1;\M_2} ((q_{10},1), s, p) = ((q_{1f},2), r, p_2)\]
	
	As we wished.
	\end{proof}
	
	This last proposition tells us something very important, that will form the basis of most of what we will do from now on.
	
	Let $\phi, \psi, \omega$ be propositions of two free variables: a sequence $s$ and a positive integer $p$. Given a machine $\M$, the symbol $\phi\M\psi$ represents the proposition `if $\phi(s,p)$ is true and $\M$ halts when given the tape $s, p$, then after running $\M$ on the tape $(q_0, s, p)$ until it halts to become $(q_f, s', p')$, $\psi(s', p')$ is true.'
	
	%i need better notation!!
	
	\begin{theorem}
	If $\phi \M_1 \psi$ and $\psi \M_2 \omega$, then $\phi \M_1;\M_2 \omega$
	\end{theorem}
	
	\begin{proof}
	%deal with non-haltings!!
	
	If $\phi(s,p)$ is true, then, by hypothesis, if $\M_1$ halts on the tape $(q_{10}, s, p)$ with $(q_{1f}, s', p')$, we have $\psi(s', p')$. We then run $\M_2$ on $(q_{20}, s', p')$ to get $(q_{2f}, s'', p'')$, and thus, by hypothesis $\omega(s'', p'')$.
	
	Since, by the previous theorem, $\M_1;\M_2$ halts on the tape $((q_{10}, 1),s,p)$ with output $((q_{2f}, 2), s'', p'')$, it is clear that $\phi \M_1;\M_2 \omega$.
	
	%NOTATION!!!
	\end{proof}
	
	We move on to our second special program: conditional branching.
	
	Fix two Turing Machines, $\M_1$ and $\M_2$. We define the following Turing Machine:
	
	\[ \Nifat y \Nthen \M_1 \Nelse \M_2 \Nendif \]
	
	As $\M = (\Q, \delta, q_0, q_f)$ such that:
	
	\begin{itemize}
	
	\end{itemize}

	
\begin{thebibliography}{9}
\bibitem{bridges} 
Bridges, Douglas S.
\textit{Computability - A Mathematical Sketchbook}. 
Springer, 1994. %i don't know how to do this right
\end{thebibliography}	

\end{document}