\documentclass{article}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\let\mathbbalt\mathbb
\usepackage{unicode-math}
\let\mathbbu\mathbb
\let\mathbb\mathbbalt

\usepackage{eucal}


\usepackage{tikz}
\usepackage{tikz-cd}

\title{A different formalism for conditional expectation}
\author{}
\date{}

\newcommand{\M}{\mathcal{M}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\E}{\mathcal{E}}

\newcommand{\R}{\mathbb{R}}

\newcommand{\dd}{\,\mathrm{d}}
\newcommand{\id}{\mathrm{id}}

\newcommand{\amp}{\mathbin{\&}}

\newcommand{\ind}{\mathbbu{1}}

\newtheorem{prop}{Prop}

\begin{document}
	\maketitle
	
	\section{Intuition}
	
	Let $(\Omega, \M, P)$ be a sample space. Suppose there is a process occurring in this space, over time, and we have already collected data giving us part of the information about the process.
	
	Let $X$ be a real random variable we are trying to predict, and let $O$ be a random variable denoting the data we have already collected. This last variable is not necessarily real; let it take values in a measure space $(V, \E)$.
	
	We would like to find some way to predict $X$ from the known data $O$. We will, then, define a measurable function $\tilde X : V \to \R$ such that, in some way, $\tilde X(O)\approx X$.
	
	Before we elaborate on what we want this `approximately equal' sign to mean, let us first look at the specific case where $X$ is an indicator variable of some event $E$. If this is the case, $\tilde X(O)$ would, intuitively, represent the probability that $E$ happens given that we know the value of $O$. Symbolically, ignoring for the sake of argument problems of division by zero, $\tilde X(o) = \frac{P(E \cap O = o)}{P(O = o)}$. Hence, $\tilde X(o) P(O = o) = P(E \cap O = o)$, which, integrating over some possible range of values of $O$, would yield
	
	\begin{equation}\label{def1}
	\int_{O \in B} \tilde X(O) \dd P = P(E \amp (O \in B)).
	\end{equation}
	
	There is no division by zero problem with this equation, and so we take it as the definition of $\tilde X$, with one minor observation before we fully commit to it.
	
	Note that $P(E \amp (O \in B)) = \int_{O \in B} X \dd P$, which leads to a more general version of equation \eqref{def1}:
	
	\begin{equation}\label{def2}
	\int_{O \in B} \tilde X(O) \dd P = \int_{O \in B} X \dd P.
	\end{equation}
	
	This last equation suffers no restriction that $X$ is an indicator variable, and so we seek to find $\tilde X$ in these conditions.
	
	\section{Definition}
	
	We now proceed to find $\tilde X$ satisfying \eqref{def2}.
	
	Define a measure $P_i$ on $(V, \E)$, induced by $O$. That is,
	
	\[P_i(B) = P(O \in B).\]
	
	Now, define a measure on $(V, \E)$ as
	
	\[\nu(B) = \int_{O \in B} X \dd P.\]
	
	Clearly, $\nu \ll P_i$, as if $P_i(B) = 0$ then
	
	\begin{align*}
	\nu(B) &= \int_{O \in B} X \dd P\\
	&= \int X \cdot \ind_{O \in B} \dd P
	\end{align*}
	
	And this last function is zero almost everywhere, which means the integral equals zero.
	
	Hence, by Radon-Nikodym, \textbf{under the assumption that $\nu$ is finite}, that is, $X$ has finite first moment, there exists some function $f : V \to \R$ such that, for all $B \in \E$,
	
	\[\int_B f \dd P_i = \int_{O \in B} X \dd P.\]
	
	Finally, we make the observation that, for any measurable function $g$,
	
	\[\int g \dd P_i = \int g(O) \dd P,\]
	and therefore
	
	\[\int_B f \dd P_i = \int_{O \in B} f(O) \dd P\]
	and we then conclude, finally, that for all $B \in \E$
	
	\[\int_{O \in B} f(O) \dd P = \int_{O \in B} X \dd P.\]
	
	Comparing with \eqref{def2}, we conclude $f$ is precisely the $\tilde X$ we were looking for, completing the proof of existence.
	
	We proceed to prove uniqueness.
	
	Let $\tilde X$ and $\hat X$ be two functions satisfying \eqref{def2}. We will show they are equal a.e.
	
	We know $\int_B \tilde X \dd P_i = \int_B \hat X \dd P_i$ for all $B \in \E$, which, by standard arguments, means $P_i(\tilde X \neq \hat X) = 0$. That is, $\tilde X = \hat X$ a.e. on $(V, \E, P_i)$. Furthermore, $P(\tilde X(O) \neq \hat X(O)) = 0$, that is, the estimate given by $\tilde X$ will almost always be the same as given by $\hat X$. 
	
	\section{Estimating Estimates (Or: The Tower Law)}
	
	We have already shown that, under fixed observed data $O$, there is one and only one estimate for a random variable $X$, modulo a null set. We will now investigate what happens when there is more than one data point being observed.
	
	Let $O'$ be another random variable, also representing observed data, taking values in the measure space $(V', \E')$. We will say $O'$ is \emph{recoverable from $O$} if there is a measurable function $F$ such that $O' = F(O)$ almost surely. In other words, $O$ contains enough info to recover $O'$. This should imply that estimates built from $O$ are, in a sense, stronger than those built from $O'$.
	
	Consider the following diagram.
	
	\begin{center}
	\begin{tikzcd}[column sep = 4em, row sep = 4em]
\Omega \arrow[r, "O"] \arrow[rr, "O'", bend left] \arrow[rd, "X" description] & V \arrow[r, "F"] \arrow[d, "Y" description] & V' \arrow[ld, "Y'" description] \\
                                                                  & \R                                      &                        
	\end{tikzcd}
	\end{center}
	
	In this diagram, $Y(O)$ represents the estimate of $X$ using data $O$, and $Y'(O')$ the estimate using data $O'$.
	
	We assert the following: consider the r.v. $Z = Y(O)$. It is the result of estimating $X$. This estimate can be itself estimated by using the information $O'$, to yield a r.v $\tilde Z(O')$. That is, we are using weaker information to estimate what we would estimate if we had stronger information.
	
	This turns out to yield the same result as estimating $X$ directly. That is:
	
	\[\tilde Z(O') = Y'(O') \text{ a.s.}\]
	
	Intuitively, to estimate an estimate of $X$ is to estimate $X$.
	
	The proof is not particularly difficult. Indeed, all we need to show is that $\tilde Z$ satisfies the condition defining $Y'$.
	
	Fix some $B' \in \E'$. Let us calculate $\int_{O' \in B'} \tilde Z(O') \dd P$.
	
	\begin{align*}
	\int_{O' \in B'} \tilde Z(O') \dd P &= \int_{O' \in B'} Z \dd P\\
	&= \int_{F(O) \in B'} Y(O) \dd P\\
	&= \int_{O \in F^{-1}(B')} Y(O) \dd P\\
	&= \int_{O \in F^{-1}(B')} X \dd P\\
	&= \int_{O' \in B'} X \dd P
	\end{align*}
	
	This concludes the proof.
	
	An active ingredient in this proof is that which we refer to as the conditional expectation. Consider an r.v. $X$ and some observed data $O$. The function $\tilde X$ allows us to estimate $X$ from $O$, but for theoretical purposes it is useful for us to inspect the r.v. $\tilde X(O)$; that is, the r.v. `the result of estimating $X$'. This r.v. corresponds to the usual definition of conditioning over a variable, and we denote it $E[X \mid O]$. What we have just shown is a part of the tower law: if $O'$ is recoverable from $O$, then $E[ E[X \mid O] \mid O'] = E[X \mid O']$.
	
	We note that the other rest of the tower law is a triviality: Indeed, suppose we wish to estimate $E[X \mid O']$ using the data $O$. Well, if we know data $O$, then we can also recover $O'$, and so find exactly $E[X \mid O']$.
	
	Formally, let $Y'$ be the estimate of $X$ using data $O'$. Note that the function $Y : V \to \R$ defined as $Y(v) = Y'(F(v))$ satisfies the requirement \eqref{def2} to be an estimate of $E[X \mid O']$, as fixed any $B \in \E$
	
	\begin{align*}
	\int_{O \in B} Y(O) &= \int_{O \in B} Y'(F(O))\\
	&= \int_{O \in B} Y'(O')\\
	&= \int_{O \in B} E[X \mid O']
	\end{align*}
	
	This shows that $E[E[X \mid O'] \mid O] = E[X \mid O']$. Note that the only thing we used about $E[X \mid O']$ was that it is a function of $O$. Hence, we conclude a version of the tower law:
	
	\begin{prop}
	Suppose $O'$ is recoverable from $O$.
	
	\begin{itemize}
	\item $X$ is a function of \footnote{We are sweeping a lot of `almost everywhere's under the rug. This is supposed to mean that $X$ is equal a.e. to something of the form $T(O)$.} $O$ iff $E[X \mid O] = X$.
	
	\item $E[E[X \mid O'] \mid O] = E[X \mid O'] = E[E[X \mid O] \mid O']$.
	\end{itemize}
	
	\end{prop}
	
	We point out that the condition `$X$ is a function of $O$' turns out to be equivalent to `$X$ is measurable in the $\sigma$-algebra generated by $O$'.
	
	\begin{proof}
	Clearly, if $X = T(O)$ then $\{X \in B\} = \{O \in T^{-1}(B)\}$, which shows the right-implication.
	
	The left implication is slightly tricker, but doable through usual truncation methods. Indeed, split $X$ into its positive and negative parts, truncate it as in the construction of the Lebesgue integral, and take the limit. The only thing we need to show is that these truncations are a function of $O$. But if $X_n$ is such a truncation, $X_n = \sum v_i \ind_{v_i \leq X < v_{i+1}}$, where $v_i$ ranges over the values $X_n$ may take. However, $\{X \in \left[v_i, v_{i+1}\right[\}$ is, by hypothesis, of the form $\{O \in B_i\}$, and so $X_n = \sum v_i \ind_{B_i}(O)$, which is a function of $O$. Taking the limit in $n$, we get $X$ is a function of $O$, as desired.
	\end{proof}
	
	We conclude this section with a trivial but useful remark.
	
	Suppose $O$ and $O'$ are two pieces of partial information, each deducible from the other. That is, they both hold the same information. Then, it stands to reason that estimating $X$ from one is the same as estimating it from the other. This is an easy consequence of the tower law, as
	
	\[ E[X \mid O] = E[E[X \mid O] \mid O'] = E[X \mid O'].\]
	
	\section{Full generality}
	
	The r.v. $E[X \mid O]$ actually turns out to be a particular case of the definition given in \eqref{def2}. Fixed $O$, consider the $\sigma$-algebra on $\Omega$ generated by $O$, that is, sets of the form $\{O \in B\}$ for $B \in \E$. Call it $\N$. Since $O$ is measurable, $\N \subseteq \M$.
	
	Consider the identity function $\id$ from the measure space $(\Omega, \M)$ to $(\Omega, \N)$. It is certainly measurable, and so we can apply our procedure to get an estimate for $X$ from $\id$. This nets us a random variable $\hat X$ that is $\N$-measurable (as is $E[X \mid O]$) and satisfies, for all $B \in \N$,
	
	\[\int_B \hat X \dd P = \int_B X \dd P.\]
	
	Note that $E[X \mid O]$ does as well, as
	
	\begin{align*}
	\int_B E[X \mid O] \dd P &= \int_{O \in B'} \tilde X(O) \dd P\\
	&= \int_{O \in B'} X \dd P\\
	&= \int_B X \dd P,
	\end{align*}
	where $B'$ is the set such that $B = \{O \in B'\}$ (exists by definition of $\N$) and $\tilde X$ is the estimate for $X$ obtained from $O$.
	
	Hence, by uniqueness of $\hat X$, it must be equal to $E[X \mid O]$ a.e.
	
	\smallskip
	
	This proof can be adjusted to show that, if $\N$ is any $\sigma$-algebra contained in $\M$, the r.v. $E[X \mid \N]$ (in the usual sense) is obtained from our definition by using the identity function $\id : (\Omega, \M) \to (\Omega, \N)$.

\end{document}